{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed------1 \n",
      "\n",
      "completed------2 \n",
      "\n",
      "completed------3 \n",
      "\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_288), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_193/lstm_cell_385/kernel:0' shape=(4, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_193/lstm_cell_385/recurrent_kernel:0' shape=(50, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_193/lstm_cell_385/bias:0' shape=(200,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_289), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_96/forward_lstm_194/lstm_cell_387/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_96/forward_lstm_194/lstm_cell_387/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_96/forward_lstm_194/lstm_cell_387/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_290), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_96/backward_lstm_194/lstm_cell_388/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_96/backward_lstm_194/lstm_cell_388/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_96/backward_lstm_194/lstm_cell_388/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_291), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_195/lstm_cell_389/kernel:0' shape=(2, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_195/lstm_cell_389/recurrent_kernel:0' shape=(50, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_195/lstm_cell_389/bias:0' shape=(200,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_292), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_97/forward_lstm_196/lstm_cell_391/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_97/forward_lstm_196/lstm_cell_391/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_97/forward_lstm_196/lstm_cell_391/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_293), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_97/backward_lstm_196/lstm_cell_392/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_97/backward_lstm_196/lstm_cell_392/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_97/backward_lstm_196/lstm_cell_392/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_294), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_197/lstm_cell_393/kernel:0' shape=(2, 24) dtype=float32>\n",
      "  <tf.Variable 'lstm_197/lstm_cell_393/recurrent_kernel:0' shape=(6, 24) dtype=float32>\n",
      "  <tf.Variable 'lstm_197/lstm_cell_393/bias:0' shape=(24,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_295), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_98/forward_lstm_198/lstm_cell_395/kernel:0' shape=(6, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_98/forward_lstm_198/lstm_cell_395/recurrent_kernel:0' shape=(10, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_98/forward_lstm_198/lstm_cell_395/bias:0' shape=(40,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_296), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_98/backward_lstm_198/lstm_cell_396/kernel:0' shape=(6, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_98/backward_lstm_198/lstm_cell_396/recurrent_kernel:0' shape=(10, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_98/backward_lstm_198/lstm_cell_396/bias:0' shape=(40,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_297), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_199/lstm_cell_397/kernel:0' shape=(2, 12) dtype=float32>\n",
      "  <tf.Variable 'lstm_199/lstm_cell_397/recurrent_kernel:0' shape=(3, 12) dtype=float32>\n",
      "  <tf.Variable 'lstm_199/lstm_cell_397/bias:0' shape=(12,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_298), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_99/forward_lstm_200/lstm_cell_399/kernel:0' shape=(3, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_99/forward_lstm_200/lstm_cell_399/recurrent_kernel:0' shape=(5, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_99/forward_lstm_200/lstm_cell_399/bias:0' shape=(20,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_299), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_99/backward_lstm_200/lstm_cell_400/kernel:0' shape=(3, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_99/backward_lstm_200/lstm_cell_400/recurrent_kernel:0' shape=(5, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_99/backward_lstm_200/lstm_cell_400/bias:0' shape=(20,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "completed------4 \n",
      "\n",
      "completed------5 \n",
      "\n",
      "[[7022   40]\n",
      " [1805   24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88      7062\n",
      "           1       0.38      0.01      0.03      1829\n",
      "\n",
      "    accuracy                           0.79      8891\n",
      "   macro avg       0.59      0.50      0.45      8891\n",
      "weighted avg       0.71      0.79      0.71      8891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def add3LIndicator_2D(change_p):\n",
    "    if change_p >= 0.04:\n",
    "        return 'V'\n",
    "    elif change_p <= -0.04:\n",
    "        return 'V'\n",
    "    else:\n",
    "        return 'NV'\n",
    "    \n",
    "def addLabel(value):\n",
    "    if value == 'V':\n",
    "        a = 1\n",
    "    elif value == 'NV':\n",
    "        a = 0\n",
    "        \n",
    "    return a\n",
    "\n",
    "def add3LIndicator_3D(change_p):\n",
    "    if change_p >= 0.04:\n",
    "        return 'U'\n",
    "    elif change_p <= -0.04:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'NV'\n",
    "    \n",
    "def addLabel_3D(value):\n",
    "    if value == 'U':\n",
    "        a = 1\n",
    "    elif value == 'NV':\n",
    "        a = 0\n",
    "    else:\n",
    "        a = 2\n",
    "        \n",
    "    return a\n",
    "\n",
    "\n",
    "# %%\n",
    "def label_(x, new_df, y):\n",
    "    max_ = new_df.iloc[x]['max_ch']\n",
    "    min_ = new_df.iloc[x]['min_ch']\n",
    "    \n",
    "#     if (max_ >= 0.12) & (min_ >= -0.06):\n",
    "#         return 1\n",
    "#     elif (max_ <= 0.06) & (min_ <= -0.12):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "    if (max_ >= y) :\n",
    "        return 1\n",
    "    elif (min_ <= -y):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def label_d(x, new_df, y):\n",
    "    max_ = new_df.iloc[x]['max_ch']\n",
    "    min_ = new_df.iloc[x]['min_ch']\n",
    "    \n",
    "#     if (max_ >= 0.12) & (min_ >= -0.06):\n",
    "#         return 1\n",
    "#     elif (max_ <= 0.06) & (min_ <= -0.12):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "    if (max_ >= y) :\n",
    "        return 1\n",
    "    elif (min_ <= -y):\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def label_dd(x, new_df, y):\n",
    "    max_ = new_df.iloc[x]['max_ch']\n",
    "    min_ = new_df.iloc[x]['min_ch']\n",
    "    \n",
    "#     if (max_ >= 0.12) & (min_ >= -0.06):\n",
    "#         return 1\n",
    "#     elif (max_ <= 0.06) & (min_ <= -0.12):\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "    if (min_ <= -y) :\n",
    "        return 0\n",
    "    elif (max_ >= y):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def calcEpsTTM(array):\n",
    "    array = array.replace('[', '')\n",
    "    array = array.replace(']', '')\n",
    "    eps = [float(s) for s in array.split(', ')]\n",
    "        \n",
    "    return (eps[0]+eps[1]+eps[2]+eps[3])\n",
    "    \n",
    "def fundamental(symbol, tech_df):\n",
    "    \n",
    "    cal_df = pd.read_csv('fundamental/'+ symbol)\n",
    "    cal_df = cal_df.set_index(['Unnamed: 0'])\n",
    "\n",
    "    eps_list = []\n",
    "    eps_act_list = []\n",
    "    pre_eps_vec_list = []\n",
    "    post_eps_vec_list = []\n",
    "\n",
    "    for index, row in tech_df.iterrows():\n",
    "        tech_df.loc[index, 'event'] = cal_df.loc[row['Date'], 'event']\n",
    "        tech_df.loc[index, 'YoY'] = cal_df.loc[row['Date'], 'YoY']\n",
    "        tech_df.loc[index, 'YoYRev'] = cal_df.loc[row['Date'], 'YoYRev']\n",
    "        tech_df.loc[index, 'surprisePercent'] = cal_df.loc[row['Date'], 'surprisePercent']\n",
    "        tech_df.loc[index, 'operatingMargin'] = cal_df.loc[row['Date'], 'operatingMargin']\n",
    "        eps_list.append(cal_df.loc[row['Date'], 'eps_seq'])\n",
    "        eps_act_list.append(cal_df.loc[row['Date'], 'eps_actual'])\n",
    "        pre_eps_vec_list.append(cal_df.loc[row['Date'], 'pre_eps_vec'])\n",
    "        post_eps_vec_list.append(cal_df.loc[row['Date'], 'post_eps_vec'])\n",
    "\n",
    "    tech_df['eps_seq'] = eps_list\n",
    "    tech_df['eps_actual'] = eps_act_list\n",
    "    tech_df['pre_eps_vec'] = pre_eps_vec_list\n",
    "    tech_df['post_eps_vec'] = post_eps_vec_list\n",
    "\n",
    "\n",
    "    return tech_df\n",
    "\n",
    "\n",
    "# %%\n",
    "def calcEpsTTM(array):\n",
    "    array = array.replace('[', '')\n",
    "    array = array.replace(']', '')\n",
    "    eps = [float(s) for s in array.split(', ')]\n",
    "        \n",
    "    return (eps[0]+eps[1]+eps[2]+eps[3])\n",
    "\n",
    "def direction(x):\n",
    "    if x >= 0.04:\n",
    "        return 1\n",
    "    elif x <= -0.04:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def vola(x):\n",
    "    if x >= 0.04:\n",
    "        return 1\n",
    "    elif x <= -0.04:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# %%\n",
    "# t = pd.DataFrame({'tickers' :tickers_})\n",
    "# t.to_csv('chosen_stocls_direction_v2_70.csv',index = False)\n",
    "\n",
    "\n",
    "# %%\n",
    "# tickers = pd.read_csv('chosen_stocls_direction_v2_70.csv')['tickers'].unique().tolist()\n",
    "\n",
    "\n",
    "# %%\n",
    "# pd.read_csv('chosen_stocls_direction_v2_70.csv')['tickers'].apply(lambda x: x.split('.')[0]).unique().tolist()\n",
    "\n",
    "\n",
    "# %%\n",
    "# pd.read_csv('prod_up_tickers.csv')['tickers'].apply(lambda x: x.split('.')[0]).unique().tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "def direction(x):\n",
    "    if x >= 0.04:\n",
    "        return 1\n",
    "    elif x <= -0.04:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def vola(x):\n",
    "    if x >= 0.03:\n",
    "        return 1\n",
    "    elif x <= -0.03:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, concatenate\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, concatenate, BatchNormalization\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dropout, Bidirectional\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import keras.backend as K\n",
    "import sys\n",
    "\n",
    "def convToNumpyArr(s, n):\n",
    "    str_arrays = s.split('], ')\n",
    "    #arr = np.empty(shape=[0, n])\n",
    "    arr = []\n",
    "    \n",
    "    for array in str_arrays:\n",
    "        array = array.replace('[[', '')\n",
    "        array = array.replace('[', '')\n",
    "        array = array.replace(']', '')\n",
    "        array = array.replace(']]', '')\n",
    "        py_list = [float(s) for s in array.split(',')]\n",
    "        arr.append(py_list)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    y_true= tf.cast(y_true, dtype='float64')\n",
    "    y_pred= tf.cast(y_pred, dtype='float64')\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    y_true= tf.cast(y_true, dtype='float64')\n",
    "    y_pred= tf.cast(y_pred, dtype='float64')\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true= tf.cast(y_true, dtype='float64')\n",
    "    y_pred= tf.cast(y_pred, dtype='float64')\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # calculating squared difference between target and predicted values \n",
    "    y_true= tf.cast(y_true, dtype='float64')\n",
    "    y_pred= tf.cast(y_pred, dtype='float64')\n",
    "    loss = K.square(((y_pred - y_true)* [1.0, 3.0]))  # (batch_size, 2)\n",
    "                \n",
    "    # summing both loss values along batch dimension \n",
    "    loss = K.sum(loss, axis=1)        # (batch_size,)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def add3LIndicator_2D(change_p):\n",
    "    if change_p >= 0.04:\n",
    "        return 'V'\n",
    "    elif change_p <= -0.04:\n",
    "        return 'V'\n",
    "    else:\n",
    "        return 'NV'\n",
    "    \n",
    "def addLabel(value):\n",
    "    if value == 'V':\n",
    "        a = 1\n",
    "    elif value == 'NV':\n",
    "        a = 0\n",
    "        \n",
    "    return a\n",
    "\n",
    "def add3LIndicator_3D(change_p):\n",
    "    if change_p >= 0.04:\n",
    "        return 'U'\n",
    "    elif change_p <= -0.04:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'NV'\n",
    "    \n",
    "def addLabel_3D(value):\n",
    "    if value == 'U':\n",
    "        a = 1\n",
    "    elif value == 'NV':\n",
    "        a = 0\n",
    "    else:\n",
    "        a = 2\n",
    "        \n",
    "    return a\n",
    "\n",
    "def convToNumpyArrEPS(s, n):\n",
    "    str_arrays = s.split('], ')\n",
    "    #arr = np.empty(shape=[0, n])\n",
    "    arr = []\n",
    "    \n",
    "    for array in str_arrays:\n",
    "        array = array.replace('[[', '')\n",
    "        array = array.replace('[', '')\n",
    "        array = array.replace(']', '')\n",
    "        array = array.replace(']]', '')\n",
    "        py_list = [float(s) for s in array.split(', ')]\n",
    "        arr.append(py_list)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "\n",
    "def convToNumpyArrEPSVEC(array):\n",
    "    array = array.replace('[', '')\n",
    "    array = array.replace(']', '')\n",
    "    py_list = [float(s) for s in array.split(', ')]\n",
    "    \n",
    "    return py_list\n",
    "\n",
    "\n",
    "# %%\n",
    "a = 'aapl_df, amzn_df, bb_df, clf_df, clne_df, fb_df, ibm_df, msft_df, nok_df, nvda_df,shop_df, sndl_df, tlry_df, tsla_df, wsbc_df, jpm_df, jnj_df, unh_df, v_df, pg_df, hd_df, pypl_df, dis_df, bac_df, ma_df,cmcsa_df, pfe_df, crm_df, nflx_df, csco_df, xom_df, vz_df, abt_df, tmo_df, ko_df,tmus_df, unp_df, hon_df, ups_df, ms_df, amgn_df, pm_df, c_df, bmy_df,lin_df, low_df, ba_df, sbux_df, chtr_df, intu_df, now_df, schw_df, blk_df, amd_df,t_df, mmm_df, bkng_df, gs_df, rtx_df, amat_df, amt_df, de_df, zts_df, isrg_df, adi_df, pld_df, chtr_df, tgt_df, ge_df, antm_df, axp_df, cvs_df, spgi_df, cat_df,syk_df, mo_df, gild_df, lrcx_df, cop_df, lmt_df, tjx_df, adp_df, mdlz_df, mu_df, pnc_df, mmc_df, usb_df, cci_df, cb_df, eqix_df, gm_df, duk_df, fis_df'\n",
    "\n",
    "\n",
    "# %%\n",
    "tickers = []\n",
    "for i in a.split(','):\n",
    "    tickers.append(i.split('_')[0].upper().replace(' ','') + '.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# import helper\n",
    "import os\n",
    "pd.options.mode.chained_assignment = None\n",
    "# tickers = tickers + ['SPY.csv']\n",
    "# tick = []\n",
    "\n",
    "# # av = ['OGN.csv', 'BF.csv', 'BRK.csv', 'CARR.csv', 'OTIS.csv','BRK-A.csv','MRNA.csv','TSLA.csv','NVDA.csv']\n",
    "# av = []\n",
    "# li = []\n",
    "# for i in os.listdir('/Volumes/AARTHI/sematic_data/lstm_training_data'):\n",
    "# #     if (i != '.ipynb_checkpoints') and (i!= 'VOO.csv') and (i not in av):\n",
    "#     if (i != '.ipynb_checkpoints') and (i not in av) :\n",
    "#         li.append(i)\n",
    "\n",
    "# # # # av = ['UWMC.csv','ANAC.csv','CRSR.csv','ARVL.csv','PLTR.csv','AGC.csv','LMAO.csv','OG.csv','U.csv','ACKIT.csv','CCIV.csv','RSI.csv','.DS_Store','ACND.csv','BREZ.csv','ACEVW.csv','BDTX.csv','PSFE.csv','RBLX.csv','ASO.csv','RKT.csv']\n",
    "# # for i in os.listdir('fundamental'):\n",
    "# # #     if (i != '.ipynb_checkpoints') and (i!= 'VOO.csv') and (i not in av):\n",
    "# #     if (i != '.ipynb_checkpoints') and (i not in av) :\n",
    "# #         tick.append(i)\n",
    "\n",
    "# df_rate = pd.read_csv('Fed_rate.csv')\n",
    "# spx = pd.read_csv('spx.csv')\n",
    "# spx['Date'] = spx['Date'].astype(str)\n",
    "# spx = spx[['Date','Adjusted_close']]\n",
    "# spx.columns = ['Date','SNP_Close']\n",
    "# cc = 0\n",
    "# df_list = []\n",
    "# avoid = []\n",
    "# for i in tickers:\n",
    "#     if (i in li) :\n",
    "#         cc += 1\n",
    "#         print(cc)\n",
    "#         aa = i.split('.')[0]\n",
    "\n",
    "#         temp = pd.read_csv('/Volumes/AARTHI/sematic_data/lstm_training_data/' + i)\n",
    "#         temp['ticker'] = i\n",
    "#         # df_list.append(temp)\n",
    "#         if i != 'SPY.csv':\n",
    "\n",
    "#             try:\n",
    "#                 temp = helper.fundamental(aa, temp)\n",
    "#                 if (temp['YoYRev'].isnull().sum() == 0) & (temp['operatingMargin'].isnull().sum() == 0):\n",
    "#                     temp[\"EpsTTM\"] = temp[\"eps_actual\"].apply(calcEpsTTM)\n",
    "#                     temp[\"EPRatio\"]= temp[\"EpsTTM\"] / temp[\"Adjusted_close\"]\n",
    "\n",
    "#                     fund_df = temp.merge(df_rate, on= ['Date']).drop(columns = ['Unnamed: 0'])\n",
    "#                     fund_df = fund_df.merge(spx, on = 'Date')\n",
    "\n",
    "#                     #Start with fundamental\n",
    "#                     fund_df['Price_OED'] = None\n",
    "#                     fund_df['SNP_Price_OED'] = None\n",
    "\n",
    "#                     price_oed = None #Adjusted closing price on Earnings Day\n",
    "#                     snp_price_oed = None #Adjusted closing price of S&P500 on Earnings Day\n",
    "#                     prev_eps_seq = fund_df.iloc[0].eps_seq\n",
    "\n",
    "#                     for i in range(fund_df.shape[0]):\n",
    "#                         if (fund_df.iloc[i].event == 1.0):\n",
    "#                             price_oed = fund_df.iloc[i].Adjusted_close\n",
    "#                             snp_price_oed = fund_df.iloc[i].SNP_Close\n",
    "#                             prev_eps_seq = fund_df.iloc[i].eps_seq\n",
    "#                         elif (fund_df.iloc[i].eps_seq != prev_eps_seq):\n",
    "#                             price_oed = fund_df.iloc[i].Adjusted_close\n",
    "#                             snp_price_oed = fund_df.iloc[i].SNP_Close\n",
    "#                             prev_eps_seq = fund_df.iloc[i].eps_seq\n",
    "                            \n",
    "#                         fund_df.iloc[i, fund_df.columns.get_loc('Price_OED')] = price_oed\n",
    "#                         fund_df.iloc[i, fund_df.columns.get_loc('SNP_Price_OED')] = snp_price_oed\n",
    "\n",
    "#                     fund_df[\"EpsTTM\"] = fund_df[\"eps_actual\"].apply(calcEpsTTM)\n",
    "#                     fund_df[\"EPRatio\"]= fund_df[\"EpsTTM\"] / fund_df[\"Adjusted_close\"]\n",
    "\n",
    "#                     fund_df['Price_Ratio'] = fund_df['Adjusted_close'] / fund_df['Price_OED']\n",
    "#                     fund_df['Price_Surprise_Ratio'] = fund_df['Price_Ratio'] / (1 + fund_df['surprisePercent'])\n",
    "\n",
    "#                     fund_df['Adj_YoY'] = fund_df['YoY'] * abs(fund_df['YoY'])\n",
    "#                     fund_df['Adj_surprisePercent'] = (1/3000) * fund_df['surprisePercent'] * abs(fund_df['surprisePercent'])\n",
    "\n",
    "#                     fund_df['alt_surprise'] = (fund_df['epsDifference'] / fund_df['Adjusted_close']) * 100\n",
    "\n",
    "#                     fund_df['alt_Price_Surprise_Ratio'] = fund_df['Price_Ratio'] / (1 + (fund_df['alt_surprise']/100))\n",
    "#                     fund_df['SNP_Price_Ratio'] = fund_df['SNP_Close'] / fund_df['SNP_Price_OED']\n",
    "\n",
    "#                     print('yes')\n",
    "\n",
    "#                     df_list.append(fund_df)\n",
    "#                 else:\n",
    "#                     avoid.append(i)\n",
    "#                     print(i)\n",
    "#             except:\n",
    "#                 print('Failed')\n",
    "#                 avoid.append(i)\n",
    "#                 print(i)\n",
    "#         else:\n",
    "#             spy = temp.copy()\n",
    "#     else:\n",
    "#         print('No Fundamentals - {}'.format(i))\n",
    "#         avoid.append(i)\n",
    "\n",
    "# print('Completed>>>>>>1')    \n",
    "        \n",
    "# df_test = []\n",
    "\n",
    "# for i in tickers:\n",
    "#     aa = i.split('.')[0]\n",
    "#     if (i != '.DS_Store') & (i not in avoid) & (i in li) :\n",
    "\n",
    "#         try:\n",
    "#             temp = pd.read_csv('/Volumes/AARTHI/sematic_data/lstm_testing_data/' + i)\n",
    "#             temp['ticker'] = i\n",
    "#             # df_test.append(temp)\n",
    "\n",
    "#             if i != 'SPY.csv':\n",
    "\n",
    "#                 temp = helper.fundamental(aa, temp)\n",
    "#                 if (temp['YoYRev'].isnull().sum() == 0) & (temp['operatingMargin'].isnull().sum() == 0):\n",
    "#                     temp[\"EpsTTM\"] = temp[\"eps_actual\"].apply(calcEpsTTM)\n",
    "#                     temp[\"EPRatio\"]= temp[\"EpsTTM\"] / temp[\"Adjusted_close\"]\n",
    "\n",
    "#                     fund_df = temp.merge(df_rate, on= ['Date']).drop(columns = ['Unnamed: 0'])\n",
    "#                     fund_df = fund_df.merge(spx, on = 'Date')\n",
    "\n",
    "#                     #Start with fundamental\n",
    "#                     fund_df['Price_OED'] = None\n",
    "#                     fund_df['SNP_Price_OED'] = None\n",
    "\n",
    "#                     price_oed = None #Adjusted closing price on Earnings Day\n",
    "#                     snp_price_oed = None #Adjusted closing price of S&P500 on Earnings Day\n",
    "#                     prev_eps_seq = fund_df.iloc[0].eps_seq\n",
    "\n",
    "#                     for i in range(fund_df.shape[0]):\n",
    "#                         if (fund_df.iloc[i].event == 1.0):\n",
    "#                             price_oed = fund_df.iloc[i].Adjusted_close\n",
    "#                             snp_price_oed = fund_df.iloc[i].SNP_Close\n",
    "#                             prev_eps_seq = fund_df.iloc[i].eps_seq\n",
    "#                         elif (fund_df.iloc[i].eps_seq != prev_eps_seq):\n",
    "#                             price_oed = fund_df.iloc[i].Adjusted_close\n",
    "#                             snp_price_oed = fund_df.iloc[i].SNP_Close\n",
    "#                             prev_eps_seq = fund_df.iloc[i].eps_seq\n",
    "                            \n",
    "#                         fund_df.iloc[i, fund_df.columns.get_loc('Price_OED')] = price_oed\n",
    "#                         fund_df.iloc[i, fund_df.columns.get_loc('SNP_Price_OED')] = snp_price_oed\n",
    "\n",
    "#                     fund_df[\"EpsTTM\"] = fund_df[\"eps_actual\"].apply(calcEpsTTM)\n",
    "#                     fund_df[\"EPRatio\"]= fund_df[\"EpsTTM\"] / fund_df[\"Adjusted_close\"]\n",
    "\n",
    "#                     fund_df['Price_Ratio'] = fund_df['Adjusted_close'] / fund_df['Price_OED']\n",
    "#                     fund_df['Price_Surprise_Ratio'] = fund_df['Price_Ratio'] / (1 + fund_df['surprisePercent'])\n",
    "\n",
    "#                     fund_df['Adj_YoY'] = fund_df['YoY'] * abs(fund_df['YoY'])\n",
    "#                     fund_df['Adj_surprisePercent'] = (1/3000) * fund_df['surprisePercent'] * abs(fund_df['surprisePercent'])\n",
    "\n",
    "#                     fund_df['alt_surprise'] = (fund_df['epsDifference'] / fund_df['Adjusted_close']) * 100\n",
    "\n",
    "#                     fund_df['alt_Price_Surprise_Ratio'] = fund_df['Price_Ratio'] / (1 + (fund_df['alt_surprise']/100))\n",
    "#                     fund_df['SNP_Price_Ratio'] = fund_df['SNP_Close'] / fund_df['SNP_Price_OED']\n",
    "\n",
    "#                     print('yes')\n",
    "\n",
    "#                     df_test.append(fund_df)\n",
    "#                 else:\n",
    "#                     avoid.append(i)\n",
    "#                     print(i)\n",
    "#             else:\n",
    "#                 spy_test = temp.copy()\n",
    "\n",
    "#         except:\n",
    "#             avoid.append(i)\n",
    "#             print(i)\n",
    "\n",
    "#         # temp[\"EpsTTM\"] = temp[\"eps_actual\"].apply(calcEpsTTM)\n",
    "#         # temp[\"EPRatio\"]= temp[\"EpsTTM\"] / temp[\"Adjusted_close\"]\n",
    "#         # df_test.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print('Completed>>>>>>2')    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# new_df = pd.concat(df_list, ignore_index=True, sort=True)\n",
    "\n",
    "# new_df = new_df.reset_index(drop = True)\n",
    "\n",
    "# new_df = new_df.drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "\n",
    "# test_df = pd.concat(df_test, ignore_index=True, sort=True)\n",
    "# test_df = test_df.reset_index(drop = True)\n",
    "\n",
    "# print(len(new_df[['eps_seq',\"pre_eps_vec\",\"post_eps_vec\",'epsDifference','YoY']]))\n",
    "\n",
    "# new_df.to_csv('/Volumes/AARTHI/sematic_data/lstm_train.csv',index = False)\n",
    "# test_df.to_csv('/Volumes/AARTHI/sematic_data/lstm_test.csv',index = False)\n",
    "\n",
    "# spy.to_csv('/Volumes/AARTHI/sematic_data/spy_train.csv',index = False)\n",
    "# spy_test.to_csv('/Volumes/AARTHI/sematic_data/spy_test.csv',index = False)\n",
    "\n",
    "# new_df= pd.read_csv('/Volumes/AARTHI/sematic_data/lstm_train.csv')\n",
    "test_df = pd.read_csv('/Volumes/AARTHI/sematic_data/lstm_test.csv')\n",
    "\n",
    "spy = pd.read_csv('/Volumes/AARTHI/sematic_data/spy_train.csv')\n",
    "spy_test = pd.read_csv('/Volumes/AARTHI/sematic_data/spy_test.csv')\n",
    "\n",
    "test_df = test_df.drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "# new_df['vol'] = new_df['future_ch_p'].apply(vola)\n",
    "\n",
    "# test_df['vol'] = test_df['future_ch_p'].apply(vola)\n",
    "\n",
    "\n",
    "\n",
    "new_df = new_df.dropna().reset_index(drop = True)\n",
    "test_df = test_df.dropna().reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "# new_df = new_df[(new_df['Date'] <= '2020-02-01') | ((new_df['Date'] >= '2020-08-01') & (new_df['Date'] < '2021-07-01')) ].reset_index(drop = True)\n",
    "new_df = new_df[(new_df['Date'] <= '2019-12-01') ].reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "# len(test_df)\n",
    "# test_df = test_df[((test_df['Date'] >= '2021-07-01') &(test_df['Date'] <= '2021-12-01')) | ((test_df['Date'] >= '2019-09-01') &(test_df['Date'] <= '2019-12-31'))].reset_index(drop = True)\n",
    "test_df = test_df[((test_df['Date'] >= '2021-07-01') & (test_df['Date'] < '2021-10-01')  )| ((test_df['Date'] < '2020-02-01') & (test_df['Date'] > '2019-12-01'))].reset_index(drop = True)\n",
    "# len(test_df)\n",
    "\n",
    "def snpSeq(snp_df, date):\n",
    "    try:\n",
    "\n",
    "        return snp_df.loc[snp_df['Date'] == date].Change_Seq.tolist()[0]\n",
    "    except:\n",
    "\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "def snpFut5Price(snp_df, date, column_name):\n",
    "    try:\n",
    "\n",
    "        return float(snp_df.loc[snp_df['Date'] == date].future_ch_p.tolist()[0])\n",
    "    except:\n",
    "        \n",
    "\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "new_df['SNP_P_Ch_Seq'] = [snpSeq(spy, date) for date in new_df['Date']]\n",
    "test_df['SNP_P_Ch_Seq'] = [snpSeq(spy_test, date) for date in test_df['Date']]\n",
    "\n",
    "new_df[\"SNP_5d_ch_p\"] = [snpFut5Price(spy, date, \"future_ch_p\") for date in new_df['Date']]\n",
    "new_df = new_df.dropna(subset = [\"SNP_5d_ch_p\"])\n",
    "new_df = new_df.dropna(subset = [\"SNP_P_Ch_Seq\"])\n",
    "new_df[\"fut_snp_5d_ch_p\"] = new_df[\"future_ch_p\"] - new_df[\"SNP_5d_ch_p\"]\n",
    "\n",
    "test_df[\"SNP_5d_ch_p\"] = [snpFut5Price(spy_test, date, \"future_ch_p\") for date in test_df['Date']]\n",
    "test_df = test_df.dropna(subset = [\"SNP_5d_ch_p\"])\n",
    "test_df = test_df.dropna(subset = [\"SNP_P_Ch_Seq\"])\n",
    "test_df[\"fut_snp_5d_ch_p\"] = test_df[\"future_ch_p\"] - test_df[\"SNP_5d_ch_p\"]\n",
    "\n",
    "new_df['vol'] = new_df[\"fut_snp_5d_ch_p\"].apply(vola)\n",
    "test_df['vol'] = test_df[\"fut_snp_5d_ch_p\"].apply(vola)\n",
    "\n",
    "new_df = new_df.sample(frac = 1)\n",
    "\n",
    "new_df = new_df.dropna().reset_index(drop = True)\n",
    "\n",
    "\n",
    "train_len = len(new_df)\n",
    "\n",
    "new_df = new_df.append(test_df)\n",
    "\n",
    "\n",
    "\n",
    "new_df = new_df.reset_index(drop = True)\n",
    "\n",
    "# price_df = new_df[\"Price_Seq\"]\n",
    "snp_price_df = new_df[\"SNP_P_Ch_Seq\"]\n",
    "volume_df = new_df[\"Volume_Seq\"]\n",
    "change_df = new_df['Change_Seq']\n",
    "eps_df = new_df[\"eps_seq\"]\n",
    "pre_eps_vec_df = new_df[\"pre_eps_vec\"]\n",
    "post_eps_vec_df = new_df[\"post_eps_vec\"]\n",
    "\n",
    "# price_lstm_ = []\n",
    "snp_price_lstm_ = []\n",
    "volume_lstm_ = []\n",
    "pattern_lstm_ = []\n",
    "eps_lstm_ = []\n",
    "pre_eps_vec_lstm_ = []\n",
    "post_eps_vec_lstm_ = []\n",
    "\n",
    "print('completed------1 \\n')\n",
    "\n",
    "for i in range(change_df.shape[0]):\n",
    "    # price_lstm_.append(convToNumpyArr(price_df.iloc[i], 4))\n",
    "    snp_price_lstm_.append(convToNumpyArr(snp_price_df.iloc[i], 4))\n",
    "    volume_lstm_.append(convToNumpyArr(volume_df.iloc[i], 2))\n",
    "    pattern_lstm_.append(convToNumpyArr(change_df.iloc[i],4))\n",
    "    eps_lstm_.append(convToNumpyArrEPS(eps_df.iloc[i], 2))\n",
    "    pre_eps_vec_lstm_.append(convToNumpyArrEPSVEC(pre_eps_vec_df.iloc[i]))\n",
    "    post_eps_vec_lstm_.append(convToNumpyArrEPSVEC(post_eps_vec_df.iloc[i]))\n",
    "    \n",
    "# price_lstm_np = np.array(price_lstm_)\n",
    "# snp_price_lstm_np = np.array(snp_price_lstm_)\n",
    "volume_lstm_np = np.array(volume_lstm_)\n",
    "pattern_lstm_np = np.array(pattern_lstm_)\n",
    "snp_price_lstm_np = np.array(snp_price_lstm_)\n",
    "eps_lstm_np = np.array(eps_lstm_)\n",
    "pre_eps_vec_lstm_np = np.array(pre_eps_vec_lstm_)\n",
    "post_eps_vec_lstm_np = np.array(post_eps_vec_lstm_)\n",
    "\n",
    "pattern_lstm_np = pattern_lstm_np - snp_price_lstm_np\n",
    "\n",
    "new_df['Adj_YoY'] = new_df['YoY'] * new_df['YoY']\n",
    "new_df['Adj_surprise'] = abs(new_df['epsDifference'] / new_df['Adjusted_close']) * 100\n",
    "new_df['Adj_YoYRev'] = abs(new_df['YoYRev']) * 10\n",
    "\n",
    "eps_vec_np = np.stack((pre_eps_vec_lstm_np, post_eps_vec_lstm_np), axis=2)\n",
    "\n",
    "# test_price_lstm_np = price_lstm_np[train_len:]\n",
    "# price_lstm_np = price_lstm_np[:train_len]\n",
    "\n",
    "test_volume_lstm_np = volume_lstm_np[train_len:]\n",
    "volume_lstm_np = volume_lstm_np[:train_len]\n",
    "\n",
    "test_pattern_lstm_np = pattern_lstm_np[train_len:]\n",
    "pattern_lstm_np = pattern_lstm_np[:train_len]\n",
    "\n",
    "test_snp_price_lstm_np = snp_price_lstm_np[train_len:]\n",
    "volume_lstm_np = volume_lstm_np[:train_len]\n",
    "\n",
    "test_eps_lstm_np = eps_lstm_np[train_len:]\n",
    "eps_lstm_np = eps_lstm_np[:train_len]\n",
    "\n",
    "test_eps_vec_np = eps_vec_np[train_len:]\n",
    "eps_vec_np = eps_vec_np[:train_len]\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "label_df = new_df[\"vol\"][:train_len]\n",
    "test_label_df = new_df[\"vol\"][train_len:]\n",
    "label_np = label_df.to_numpy()\n",
    "test_label_np = test_label_df.to_numpy()\n",
    "label_np_oh = np_utils.to_categorical(label_np)\n",
    "test_np_oh = np_utils.to_categorical(test_label_np)\n",
    "\n",
    "print('completed------2 \\n')\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('weigts_vol.h5', save_best_only=True, monitor='val_accuracy', mode='min')\n",
    "\n",
    "X_train0_df = new_df[['Adj_YoY', 'Adj_surprise', 'Adj_YoYRev']]\n",
    "X_train1_df =1 -  (new_df[['RSI9', 'RSI14', 'RSI20']] - 50)/50\n",
    "X_train2_df = new_df[['VWMA9_ch_p', 'VWMA14_ch_p', 'VWMA20_ch_p']] * 10\n",
    "X_train3_df = new_df[['1d_ch_p', '2d_ch_p', '3d_ch_p', '7d_ch_p']] * 10\n",
    "#     X_train3_df = inference_df[['1d_ch_p', '2d_ch_p', '3d_ch_p', '7d_ch_p',\n",
    "#                          '1d_adx9_ch_p', '2d_adx9_ch_p', '3d_adx9_ch_p', '7d_adx9_ch_p',\n",
    "#                           '1d_adx14_ch_p', '2d_adx14_ch_p', '3d_adx14_ch_p', '7d_adx14_ch_p',\n",
    "#                          '1d_RSI14_ch_p', '2d_RSI14_ch_p', '3d_RSI14_ch_p', '7d_RSI14_ch_p']] * 10\n",
    "#     X_train4_df = (inference_df[['ADX5', 'ADX9', 'ADX14', 'ADX20']] - 50)/50\n",
    "\n",
    "#     X_train_df = pd.concat([X_train1_df, X_train2_df, X_train3_df,X_train4_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# X_train1_df = (new_df[['RSI9', 'RSI14', 'RSI20']] - 50)/50\n",
    "# X_train2_df = new_df[['VWMA9_ch_p', 'VWMA14_ch_p', 'VWMA20_ch_p']] * 10\n",
    "# X_train3_df = new_df[['1d_ch_p', '2d_ch_p', '3d_ch_p', '7d_ch_p']] * 10\n",
    "# X_train3_df = new_df[['VWMA9_ch_p', 'VWMA14_ch_p', 'VWMA20_ch_p','1d_ch_p', '2d_ch_p', '3d_ch_p', '7d_ch_p'\n",
    "#                      ,\"VWMA2_ch_p\",\"VWMA5_ch_p\",\"VWMA30_ch_p\",'1d_ch_v','7d_ch_v','12d_ch_v','21d_ch_v',\n",
    "#        'vola5','vola9','vola21','macd259','signal259','hist259','macd91121','signal91121','hist91121','alt_surprise', '2d_ch_p','VWMA20_ch_p', 'EPRatio','SNP_Price_Ratio','FR_1M_ch_p', 'operatingMargin','stoch_d','FR_3M_ch_p','bop']]\n",
    "\n",
    "X_train_df = pd.concat([X_train0_df[:train_len], X_train1_df[:train_len],  X_train2_df[:train_len], X_train3_df[:train_len]], axis=1)\n",
    "X_test_df = pd.concat([X_train0_df[train_len:], X_train1_df[train_len:],  X_train2_df[train_len:], X_train3_df[train_len:]], axis=1)\n",
    "\n",
    "X_train_np = np.array(X_train_df)\n",
    "X_test_np = np.array(X_test_df)\n",
    "\n",
    "# define two sets of inputs\n",
    "\n",
    "lstm_seq_size = 40\n",
    "\n",
    "\n",
    "# price = Input(shape=(lstm_seq_size,4))\n",
    "volume = Input(shape=(lstm_seq_size,2))\n",
    "pattern = Input(shape=(lstm_seq_size,4))\n",
    "technicals = Input(shape=(13,))\n",
    "eps = Input(shape=(5,2))\n",
    "eps_vec = Input(shape=(21,2))\n",
    "\n",
    "print('completed------3 \\n')\n",
    "\n",
    "# the first branch operates on the first input - price\n",
    "x = LSTM(50,input_shape = (lstm_seq_size,4),return_sequences = True, activation = 'relu')(pattern)\n",
    "x = Bidirectional(LSTM(60,return_sequences = False, activation = 'relu'))(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Model(inputs=pattern, outputs=x)\n",
    "# the second branch opreates on the second input - volume\n",
    "y = LSTM(50,input_shape = (lstm_seq_size,2),return_sequences = True, activation = 'relu')(volume)\n",
    "y = Bidirectional(LSTM(60,return_sequences = False, activation = 'relu'))(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Model(inputs=volume, outputs=y)\n",
    "# the third branch opreates on the third input - technicals\n",
    "z = Dense(60, activation='relu', kernel_initializer='he_normal', input_shape=(13,))(technicals)\n",
    "z = Dense(30, activation='relu')(z)\n",
    "z = Dropout(0.1)(z)\n",
    "z = Model(inputs=technicals, outputs=z)\n",
    "# the fourth branch opreates on the second input - volume\n",
    "l = LSTM(6,input_shape = (5,2),return_sequences = True, activation = 'relu')(eps)\n",
    "l = Bidirectional(LSTM(10,return_sequences = False, activation = 'relu'))(l)\n",
    "l = Dropout(0.1)(l)\n",
    "l = Model(inputs=eps, outputs=l)\n",
    "# the fifth branch opreates on the second input - volume\n",
    "m = LSTM(3,input_shape = (21,2),return_sequences = True, activation = 'relu')(eps_vec)\n",
    "m = Bidirectional(LSTM(5,return_sequences = False, activation = 'relu'))(m)\n",
    "m = Model(inputs=eps_vec, outputs=m)\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x.output, y.output, z.output, l.output, m.output])\n",
    "#combined = concatenate([x.output, y.output, z.output])\n",
    "combined = Dropout(0.1)(combined)\n",
    "#combined = Dense(40, activation='relu')(combined)\n",
    "#combined = Dropout(0.1)(combined)\n",
    "#combined = Multiply()([x.output, y.output])\n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "out = Dense(2, activation='softmax')(combined) #switched from 3\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[x.input, y.input, z.input, l.input, m.input], outputs=out)\n",
    "\n",
    "\n",
    "\n",
    "print('completed------4 \\n')\n",
    "\n",
    "\n",
    "# #encoder - used for our dimention reduction\n",
    "# encoder = Model(inputs=[ y.input, z.input, p.input], outputs=combined)\n",
    "# # encoder = Model(inputs=[ y.input, z.input, p.input], outputs=combined)\n",
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# compile the model\n",
    "# model.compile(optimizer = tf.optimizers.Adam(learning_rate = 0.01), loss = loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# compile the model\n",
    "# model.compile(optimizer = tf.optimizers.Adam(learning_rate = 0.003), loss = custom_loss, metrics=[f1_score, 'accuracy', precision_m])\n",
    "# model.compile(optimizer = tf.optimizers.Adam(learning_rate = 0.0005), loss = custom_loss, metrics=[precision_m, f1_score, 'accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "\n",
    "# # fit the model\n",
    "# # model.fit([pattern_lstm_np, volume_lstm_np, X_train_np ], label_np, epochs= 3, batch_size=256, validation_split = 0.2, callbacks=[earlyStopping, mcp_save], verbose=1)\n",
    "# model.fit([pattern_lstm_np, volume_lstm_np, X_train_np, eps_lstm_np, eps_vec_np], label_np_oh, epochs=5, batch_size=64, validation_split = 0.15, callbacks=[earlyStopping, mcp_save], verbose=1)\n",
    "# # print(model.summary())\n",
    "# # model.evaluate([test_pattern_lstm_np,test_volume_lstm_np, X_test_np,test_eps_lstm_np, test_eps_vec_np ], test_label_np)\n",
    "model.load_weights('/Users/kukeshayanth/Downloads/vol-100comp-snp-closs.h5')\n",
    "\n",
    "p = model.predict([ test_pattern_lstm_np,test_volume_lstm_np, X_test_np,test_eps_lstm_np, test_eps_vec_np])\n",
    "\n",
    "print('completed------5 \\n')\n",
    "\n",
    "o = []\n",
    "for i in p:\n",
    "    o.append(i.argmax())\n",
    "    \n",
    "print(confusion_matrix(test_label_np.tolist(), o))\n",
    "\n",
    "print(classification_report(test_label_np.tolist(), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed------1 \n",
      "\n",
      "completed------2 \n",
      "\n",
      "completed------3 \n",
      "\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_360), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_241/lstm_cell_481/kernel:0' shape=(4, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_241/lstm_cell_481/recurrent_kernel:0' shape=(50, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_241/lstm_cell_481/bias:0' shape=(200,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_361), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_120/forward_lstm_242/lstm_cell_483/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_120/forward_lstm_242/lstm_cell_483/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_120/forward_lstm_242/lstm_cell_483/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_362), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_120/backward_lstm_242/lstm_cell_484/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_120/backward_lstm_242/lstm_cell_484/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_120/backward_lstm_242/lstm_cell_484/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_363), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_243/lstm_cell_485/kernel:0' shape=(2, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_243/lstm_cell_485/recurrent_kernel:0' shape=(50, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_243/lstm_cell_485/bias:0' shape=(200,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_364), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_121/forward_lstm_244/lstm_cell_487/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_121/forward_lstm_244/lstm_cell_487/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_121/forward_lstm_244/lstm_cell_487/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_365), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_121/backward_lstm_244/lstm_cell_488/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_121/backward_lstm_244/lstm_cell_488/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_121/backward_lstm_244/lstm_cell_488/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_366), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_245/lstm_cell_489/kernel:0' shape=(2, 24) dtype=float32>\n",
      "  <tf.Variable 'lstm_245/lstm_cell_489/recurrent_kernel:0' shape=(6, 24) dtype=float32>\n",
      "  <tf.Variable 'lstm_245/lstm_cell_489/bias:0' shape=(24,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_367), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_122/forward_lstm_246/lstm_cell_491/kernel:0' shape=(6, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_122/forward_lstm_246/lstm_cell_491/recurrent_kernel:0' shape=(10, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_122/forward_lstm_246/lstm_cell_491/bias:0' shape=(40,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_368), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_122/backward_lstm_246/lstm_cell_492/kernel:0' shape=(6, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_122/backward_lstm_246/lstm_cell_492/recurrent_kernel:0' shape=(10, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_122/backward_lstm_246/lstm_cell_492/bias:0' shape=(40,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_369), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_247/lstm_cell_493/kernel:0' shape=(2, 12) dtype=float32>\n",
      "  <tf.Variable 'lstm_247/lstm_cell_493/recurrent_kernel:0' shape=(3, 12) dtype=float32>\n",
      "  <tf.Variable 'lstm_247/lstm_cell_493/bias:0' shape=(12,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_370), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_123/forward_lstm_248/lstm_cell_495/kernel:0' shape=(3, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_123/forward_lstm_248/lstm_cell_495/recurrent_kernel:0' shape=(5, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_123/forward_lstm_248/lstm_cell_495/bias:0' shape=(20,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_371), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_123/backward_lstm_248/lstm_cell_496/kernel:0' shape=(3, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_123/backward_lstm_248/lstm_cell_496/recurrent_kernel:0' shape=(5, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_123/backward_lstm_248/lstm_cell_496/bias:0' shape=(20,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "completed------4 \n",
      "\n",
      "completed------5 \n",
      "\n",
      "[[6652  410]\n",
      " [1612  217]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.94      0.87      7062\n",
      "           1       0.35      0.12      0.18      1829\n",
      "\n",
      "    accuracy                           0.77      8891\n",
      "   macro avg       0.58      0.53      0.52      8891\n",
      "weighted avg       0.71      0.77      0.73      8891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df_copy = pd.read_csv('/Volumes/AARTHI/sematic_data/lstm_test.csv')\n",
    "\n",
    "test_df =test_df_copy.copy()\n",
    "\n",
    "spy_test = pd.read_csv('/Volumes/AARTHI/sematic_data/spy_test.csv')\n",
    "\n",
    "test_df = test_df.drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "test_df = test_df.dropna().reset_index(drop = True)\n",
    "\n",
    "test_df = test_df[((test_df['Date'] >= '2021-07-01') & (test_df['Date'] < '2021-10-01')  )| ((test_df['Date'] < '2020-02-01') & (test_df['Date'] > '2019-12-01'))].reset_index(drop = True)\n",
    "\n",
    "def snpSeq(snp_df, date):\n",
    "    try:\n",
    "\n",
    "        return snp_df.loc[snp_df['Date'] == date].Change_Seq.tolist()[0]\n",
    "    except:\n",
    "\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "def snpFut5Price(snp_df, date, column_name):\n",
    "    try:\n",
    "\n",
    "        return float(snp_df.loc[snp_df['Date'] == date].future_ch_p.tolist()[0])\n",
    "    except:\n",
    "        \n",
    "\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "test_df['SNP_P_Ch_Seq'] = [snpSeq(spy_test, date) for date in test_df['Date']]\n",
    "\n",
    "\n",
    "test_df[\"SNP_5d_ch_p\"] = [snpFut5Price(spy_test, date, \"future_ch_p\") for date in test_df['Date']]\n",
    "test_df = test_df.dropna(subset = [\"SNP_5d_ch_p\"])\n",
    "test_df = test_df.dropna(subset = [\"SNP_P_Ch_Seq\"])\n",
    "test_df[\"fut_snp_5d_ch_p\"] = test_df[\"future_ch_p\"] - test_df[\"SNP_5d_ch_p\"]\n",
    "\n",
    "test_df['vol'] = test_df[\"fut_snp_5d_ch_p\"].apply(vola)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_df = test_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "new_df = new_df.reset_index(drop = True)\n",
    "\n",
    "# price_df = new_df[\"Price_Seq\"]\n",
    "snp_price_df = new_df[\"SNP_P_Ch_Seq\"]\n",
    "volume_df = new_df[\"Volume_Seq\"]\n",
    "change_df = new_df['Change_Seq']\n",
    "eps_df = new_df[\"eps_seq\"]\n",
    "pre_eps_vec_df = new_df[\"pre_eps_vec\"]\n",
    "post_eps_vec_df = new_df[\"post_eps_vec\"]\n",
    "\n",
    "# price_lstm_ = []\n",
    "snp_price_lstm_ = []\n",
    "volume_lstm_ = []\n",
    "pattern_lstm_ = []\n",
    "eps_lstm_ = []\n",
    "pre_eps_vec_lstm_ = []\n",
    "post_eps_vec_lstm_ = []\n",
    "\n",
    "print('completed------1 \\n')\n",
    "\n",
    "for i in range(change_df.shape[0]):\n",
    "    # price_lstm_.append(convToNumpyArr(price_df.iloc[i], 4))\n",
    "    snp_price_lstm_.append(convToNumpyArr(snp_price_df.iloc[i], 4))\n",
    "    volume_lstm_.append(convToNumpyArr(volume_df.iloc[i], 2))\n",
    "    pattern_lstm_.append(convToNumpyArr(change_df.iloc[i],4))\n",
    "    eps_lstm_.append(convToNumpyArrEPS(eps_df.iloc[i], 2))\n",
    "    pre_eps_vec_lstm_.append(convToNumpyArrEPSVEC(pre_eps_vec_df.iloc[i]))\n",
    "    post_eps_vec_lstm_.append(convToNumpyArrEPSVEC(post_eps_vec_df.iloc[i]))\n",
    "    \n",
    "# price_lstm_np = np.array(price_lstm_)\n",
    "# snp_price_lstm_np = np.array(snp_price_lstm_)\n",
    "volume_lstm_np = np.array(volume_lstm_)\n",
    "pattern_lstm_np = np.array(pattern_lstm_)\n",
    "snp_price_lstm_np = np.array(snp_price_lstm_)\n",
    "eps_lstm_np = np.array(eps_lstm_)\n",
    "pre_eps_vec_lstm_np = np.array(pre_eps_vec_lstm_)\n",
    "post_eps_vec_lstm_np = np.array(post_eps_vec_lstm_)\n",
    "\n",
    "pattern_lstm_np = pattern_lstm_np - snp_price_lstm_np\n",
    "\n",
    "new_df['Adj_YoY'] = new_df['YoY'] * new_df['YoY']\n",
    "new_df['Adj_surprise'] = abs(new_df['epsDifference'] / new_df['Adjusted_close']) * 100\n",
    "new_df['Adj_YoYRev'] = abs(new_df['YoYRev']) * 10\n",
    "\n",
    "eps_vec_np = np.stack((pre_eps_vec_lstm_np, post_eps_vec_lstm_np), axis=2)\n",
    "\n",
    "# test_price_lstm_np = price_lstm_np[train_len:]\n",
    "# price_lstm_np = price_lstm_np[:train_len]\n",
    "\n",
    "test_volume_lstm_np = volume_lstm_np\n",
    "\n",
    "\n",
    "test_pattern_lstm_np = pattern_lstm_np\n",
    "\n",
    "test_snp_price_lstm_np = snp_price_lstm_np\n",
    "\n",
    "test_eps_lstm_np = eps_lstm_np\n",
    "\n",
    "test_eps_vec_np = eps_vec_np\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "test_label_df = new_df[\"vol\"]\n",
    "\n",
    "test_label_np = test_label_df.to_numpy()\n",
    "\n",
    "test_np_oh = np_utils.to_categorical(test_label_np)\n",
    "\n",
    "print('completed------2 \\n')\n",
    "\n",
    "\n",
    "\n",
    "X_train0_df = new_df[['Adj_YoY', 'Adj_surprise', 'Adj_YoYRev']]\n",
    "X_train1_df =1 -  (new_df[['RSI9', 'RSI14', 'RSI20']] - 50)/50\n",
    "X_train2_df = new_df[['VWMA9_ch_p', 'VWMA14_ch_p', 'VWMA20_ch_p']] * 10\n",
    "X_train3_df = new_df[['1d_ch_p', '2d_ch_p', '3d_ch_p', '7d_ch_p']] * 10\n",
    "\n",
    "X_test_df = pd.concat([X_train0_df, X_train1_df,  X_train2_df, X_train3_df], axis=1)\n",
    "\n",
    "\n",
    "X_test_np = np.array(X_test_df)\n",
    "\n",
    "# define two sets of inputs\n",
    "\n",
    "lstm_seq_size = 40\n",
    "\n",
    "\n",
    "# price = Input(shape=(lstm_seq_size,4))\n",
    "volume = Input(shape=(lstm_seq_size,2))\n",
    "pattern = Input(shape=(lstm_seq_size,4))\n",
    "technicals = Input(shape=(13,))\n",
    "eps = Input(shape=(5,2))\n",
    "eps_vec = Input(shape=(21,2))\n",
    "\n",
    "print('completed------3 \\n')\n",
    "\n",
    "# the first branch operates on the first input - price\n",
    "x = LSTM(50,input_shape = (lstm_seq_size,4),return_sequences = True, activation = 'relu')(pattern)\n",
    "x = Bidirectional(LSTM(60,return_sequences = False, activation = 'relu'))(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Model(inputs=pattern, outputs=x)\n",
    "# the second branch opreates on the second input - volume\n",
    "y = LSTM(50,input_shape = (lstm_seq_size,2),return_sequences = True, activation = 'relu')(volume)\n",
    "y = Bidirectional(LSTM(60,return_sequences = False, activation = 'relu'))(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Model(inputs=volume, outputs=y)\n",
    "# the third branch opreates on the third input - technicals\n",
    "z = Dense(60, activation='relu', kernel_initializer='he_normal', input_shape=(13,))(technicals)\n",
    "z = Dense(30, activation='relu')(z)\n",
    "z = Dropout(0.1)(z)\n",
    "z = Model(inputs=technicals, outputs=z)\n",
    "# the fourth branch opreates on the second input - volume\n",
    "l = LSTM(6,input_shape = (5,2),return_sequences = True, activation = 'relu')(eps)\n",
    "l = Bidirectional(LSTM(10,return_sequences = False, activation = 'relu'))(l)\n",
    "l = Dropout(0.1)(l)\n",
    "l = Model(inputs=eps, outputs=l)\n",
    "# the fifth branch opreates on the second input - volume\n",
    "m = LSTM(3,input_shape = (21,2),return_sequences = True, activation = 'relu')(eps_vec)\n",
    "m = Bidirectional(LSTM(5,return_sequences = False, activation = 'relu'))(m)\n",
    "m = Model(inputs=eps_vec, outputs=m)\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x.output, y.output, z.output, l.output, m.output])\n",
    "#combined = concatenate([x.output, y.output, z.output])\n",
    "combined = Dropout(0.1)(combined)\n",
    "#combined = Dense(40, activation='relu')(combined)\n",
    "\n",
    "out = Dense(2, activation='softmax')(combined) #switched from 3\n",
    "\n",
    "model = Model(inputs=[x.input, y.input, z.input, l.input, m.input], outputs=out)\n",
    "\n",
    "\n",
    "\n",
    "print('completed------4 \\n')\n",
    "\n",
    "\n",
    "model.load_weights('/Users/kukeshayanth/Downloads/vol-100comp-snp-closs.h5')\n",
    "\n",
    "p = model.predict([ test_pattern_lstm_np,test_volume_lstm_np, X_test_np,test_eps_lstm_np, test_eps_vec_np])\n",
    "\n",
    "print('completed------5 \\n')\n",
    "\n",
    "o = []\n",
    "for i in p:\n",
    "    o.append(i.argmax())\n",
    "    \n",
    "print(confusion_matrix(test_label_np.tolist(), o))\n",
    "\n",
    "print(classification_report(test_label_np.tolist(), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed------3 \n",
      "\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_372), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_249/lstm_cell_497/kernel:0' shape=(4, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_249/lstm_cell_497/recurrent_kernel:0' shape=(50, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_249/lstm_cell_497/bias:0' shape=(200,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_373), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_124/forward_lstm_250/lstm_cell_499/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_124/forward_lstm_250/lstm_cell_499/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_124/forward_lstm_250/lstm_cell_499/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_374), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_124/backward_lstm_250/lstm_cell_500/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_124/backward_lstm_250/lstm_cell_500/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_124/backward_lstm_250/lstm_cell_500/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_375), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_251/lstm_cell_501/kernel:0' shape=(2, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_251/lstm_cell_501/recurrent_kernel:0' shape=(50, 200) dtype=float32>\n",
      "  <tf.Variable 'lstm_251/lstm_cell_501/bias:0' shape=(200,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_376), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_125/forward_lstm_252/lstm_cell_503/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_125/forward_lstm_252/lstm_cell_503/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_125/forward_lstm_252/lstm_cell_503/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_377), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_125/backward_lstm_252/lstm_cell_504/kernel:0' shape=(50, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_125/backward_lstm_252/lstm_cell_504/recurrent_kernel:0' shape=(60, 240) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_125/backward_lstm_252/lstm_cell_504/bias:0' shape=(240,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_378), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_253/lstm_cell_505/kernel:0' shape=(2, 24) dtype=float32>\n",
      "  <tf.Variable 'lstm_253/lstm_cell_505/recurrent_kernel:0' shape=(6, 24) dtype=float32>\n",
      "  <tf.Variable 'lstm_253/lstm_cell_505/bias:0' shape=(24,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_379), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_126/forward_lstm_254/lstm_cell_507/kernel:0' shape=(6, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_126/forward_lstm_254/lstm_cell_507/recurrent_kernel:0' shape=(10, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_126/forward_lstm_254/lstm_cell_507/bias:0' shape=(40,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_380), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_126/backward_lstm_254/lstm_cell_508/kernel:0' shape=(6, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_126/backward_lstm_254/lstm_cell_508/recurrent_kernel:0' shape=(10, 40) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_126/backward_lstm_254/lstm_cell_508/bias:0' shape=(40,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_381), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'lstm_255/lstm_cell_509/kernel:0' shape=(2, 12) dtype=float32>\n",
      "  <tf.Variable 'lstm_255/lstm_cell_509/recurrent_kernel:0' shape=(3, 12) dtype=float32>\n",
      "  <tf.Variable 'lstm_255/lstm_cell_509/bias:0' shape=(12,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_382), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_127/forward_lstm_256/lstm_cell_511/kernel:0' shape=(3, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_127/forward_lstm_256/lstm_cell_511/recurrent_kernel:0' shape=(5, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_127/forward_lstm_256/lstm_cell_511/bias:0' shape=(20,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.keras.backend.rnn_383), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'bidirectional_127/backward_lstm_256/lstm_cell_512/kernel:0' shape=(3, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_127/backward_lstm_256/lstm_cell_512/recurrent_kernel:0' shape=(5, 20) dtype=float32>\n",
      "  <tf.Variable 'bidirectional_127/backward_lstm_256/lstm_cell_512/bias:0' shape=(20,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "completed------4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "volume = Input(shape=(lstm_seq_size,2))\n",
    "pattern = Input(shape=(lstm_seq_size,4))\n",
    "technicals = Input(shape=(13,))\n",
    "eps = Input(shape=(5,2))\n",
    "eps_vec = Input(shape=(21,2))\n",
    "\n",
    "print('completed------3 \\n')\n",
    "\n",
    "# the first branch operates on the first input - price\n",
    "x = LSTM(50,input_shape = (lstm_seq_size,4),return_sequences = True, activation = 'relu')(pattern)\n",
    "x = Bidirectional(LSTM(60,return_sequences = False, activation = 'relu'))(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Model(inputs=pattern, outputs=x)\n",
    "# the second branch opreates on the second input - volume\n",
    "y = LSTM(50,input_shape = (lstm_seq_size,2),return_sequences = True, activation = 'relu')(volume)\n",
    "y = Bidirectional(LSTM(60,return_sequences = False, activation = 'relu'))(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Model(inputs=volume, outputs=y)\n",
    "# the third branch opreates on the third input - technicals\n",
    "z = Dense(60, activation='relu', kernel_initializer='he_normal', input_shape=(13,))(technicals)\n",
    "z = Dense(30, activation='relu')(z)\n",
    "z = Dropout(0.1)(z)\n",
    "z = Model(inputs=technicals, outputs=z)\n",
    "# the fourth branch opreates on the second input - volume\n",
    "l = LSTM(6,input_shape = (5,2),return_sequences = True, activation = 'relu')(eps)\n",
    "l = Bidirectional(LSTM(10,return_sequences = False, activation = 'relu'))(l)\n",
    "l = Dropout(0.1)(l)\n",
    "l = Model(inputs=eps, outputs=l)\n",
    "# the fifth branch opreates on the second input - volume\n",
    "m = LSTM(3,input_shape = (21,2),return_sequences = True, activation = 'relu')(eps_vec)\n",
    "m = Bidirectional(LSTM(5,return_sequences = False, activation = 'relu'))(m)\n",
    "m = Model(inputs=eps_vec, outputs=m)\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x.output, y.output, z.output, l.output, m.output])\n",
    "#combined = concatenate([x.output, y.output, z.output])\n",
    "combined = Dropout(0.1)(combined)\n",
    "#combined = Dense(40, activation='relu')(combined)\n",
    "\n",
    "out = Dense(2, activation='softmax')(combined) #switched from 3\n",
    "\n",
    "model = Model(inputs=[x.input, y.input, z.input, l.input, m.input], outputs=out)\n",
    "\n",
    "\n",
    "\n",
    "print('completed------4 \\n')\n",
    "\n",
    "\n",
    "model.load_weights('/Users/kukeshayanth/Downloads/vol-100comp-snp-closs.h5')\n",
    "\n",
    "p = model.predict([ test_pattern_lstm_np,test_volume_lstm_np, X_test_np,test_eps_lstm_np, test_eps_vec_np])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj_YoY</th>\n",
       "      <th>Adj_surprise</th>\n",
       "      <th>Adj_YoYRev</th>\n",
       "      <th>RSI9</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>RSI20</th>\n",
       "      <th>VWMA9_ch_p</th>\n",
       "      <th>VWMA14_ch_p</th>\n",
       "      <th>VWMA20_ch_p</th>\n",
       "      <th>1d_ch_p</th>\n",
       "      <th>2d_ch_p</th>\n",
       "      <th>3d_ch_p</th>\n",
       "      <th>7d_ch_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.076836</td>\n",
       "      <td>0.181240</td>\n",
       "      <td>0.879633</td>\n",
       "      <td>0.764723</td>\n",
       "      <td>0.713634</td>\n",
       "      <td>-0.011236</td>\n",
       "      <td>-0.010638</td>\n",
       "      <td>0.055119</td>\n",
       "      <td>-0.115623</td>\n",
       "      <td>-0.137390</td>\n",
       "      <td>-0.004915</td>\n",
       "      <td>0.036863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.078231</td>\n",
       "      <td>0.181240</td>\n",
       "      <td>1.118191</td>\n",
       "      <td>0.940037</td>\n",
       "      <td>0.841139</td>\n",
       "      <td>-0.155755</td>\n",
       "      <td>-0.177886</td>\n",
       "      <td>-0.129196</td>\n",
       "      <td>-0.178306</td>\n",
       "      <td>-0.291868</td>\n",
       "      <td>-0.313247</td>\n",
       "      <td>-0.097717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.077547</td>\n",
       "      <td>0.181240</td>\n",
       "      <td>1.001552</td>\n",
       "      <td>0.875018</td>\n",
       "      <td>0.800532</td>\n",
       "      <td>-0.064785</td>\n",
       "      <td>-0.085227</td>\n",
       "      <td>-0.051724</td>\n",
       "      <td>0.088260</td>\n",
       "      <td>-0.091620</td>\n",
       "      <td>-0.206183</td>\n",
       "      <td>-0.001535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.076425</td>\n",
       "      <td>0.181240</td>\n",
       "      <td>0.836861</td>\n",
       "      <td>0.777853</td>\n",
       "      <td>0.737667</td>\n",
       "      <td>0.063310</td>\n",
       "      <td>0.052715</td>\n",
       "      <td>0.079083</td>\n",
       "      <td>0.146719</td>\n",
       "      <td>0.236274</td>\n",
       "      <td>0.053755</td>\n",
       "      <td>-0.029657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.074977</td>\n",
       "      <td>0.181240</td>\n",
       "      <td>0.671037</td>\n",
       "      <td>0.670709</td>\n",
       "      <td>0.664312</td>\n",
       "      <td>0.213946</td>\n",
       "      <td>0.229893</td>\n",
       "      <td>0.248178</td>\n",
       "      <td>0.193158</td>\n",
       "      <td>0.342710</td>\n",
       "      <td>0.433996</td>\n",
       "      <td>0.242914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8886</th>\n",
       "      <td>0.066784</td>\n",
       "      <td>0.047283</td>\n",
       "      <td>1.731938</td>\n",
       "      <td>1.283501</td>\n",
       "      <td>1.273347</td>\n",
       "      <td>1.260870</td>\n",
       "      <td>-0.105729</td>\n",
       "      <td>-0.194844</td>\n",
       "      <td>-0.259730</td>\n",
       "      <td>-0.022893</td>\n",
       "      <td>-0.003277</td>\n",
       "      <td>0.053551</td>\n",
       "      <td>-0.245404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8887</th>\n",
       "      <td>0.066784</td>\n",
       "      <td>0.046412</td>\n",
       "      <td>1.731938</td>\n",
       "      <td>1.029313</td>\n",
       "      <td>1.122194</td>\n",
       "      <td>1.160953</td>\n",
       "      <td>0.073731</td>\n",
       "      <td>-0.005183</td>\n",
       "      <td>-0.066509</td>\n",
       "      <td>0.187659</td>\n",
       "      <td>0.164337</td>\n",
       "      <td>0.184320</td>\n",
       "      <td>-0.086124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>0.066784</td>\n",
       "      <td>0.047202</td>\n",
       "      <td>1.731938</td>\n",
       "      <td>1.192686</td>\n",
       "      <td>1.213516</td>\n",
       "      <td>1.220047</td>\n",
       "      <td>-0.062234</td>\n",
       "      <td>-0.117377</td>\n",
       "      <td>-0.218323</td>\n",
       "      <td>-0.167310</td>\n",
       "      <td>0.017209</td>\n",
       "      <td>-0.005723</td>\n",
       "      <td>-0.120424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8889</th>\n",
       "      <td>0.066784</td>\n",
       "      <td>0.046781</td>\n",
       "      <td>1.731938</td>\n",
       "      <td>1.084128</td>\n",
       "      <td>1.145636</td>\n",
       "      <td>1.174018</td>\n",
       "      <td>0.049192</td>\n",
       "      <td>-0.011015</td>\n",
       "      <td>-0.111933</td>\n",
       "      <td>0.089987</td>\n",
       "      <td>-0.078829</td>\n",
       "      <td>0.107351</td>\n",
       "      <td>0.017869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8890</th>\n",
       "      <td>0.066784</td>\n",
       "      <td>0.047419</td>\n",
       "      <td>1.731938</td>\n",
       "      <td>1.206713</td>\n",
       "      <td>1.216831</td>\n",
       "      <td>1.220721</td>\n",
       "      <td>-0.062504</td>\n",
       "      <td>-0.133473</td>\n",
       "      <td>-0.214555</td>\n",
       "      <td>-0.134587</td>\n",
       "      <td>-0.045812</td>\n",
       "      <td>-0.212355</td>\n",
       "      <td>0.024716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8891 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Adj_YoY  Adj_surprise  Adj_YoYRev      RSI9     RSI14     RSI20  \\\n",
       "0     0.000319      0.076836    0.181240  0.879633  0.764723  0.713634   \n",
       "1     0.000319      0.078231    0.181240  1.118191  0.940037  0.841139   \n",
       "2     0.000319      0.077547    0.181240  1.001552  0.875018  0.800532   \n",
       "3     0.000319      0.076425    0.181240  0.836861  0.777853  0.737667   \n",
       "4     0.000319      0.074977    0.181240  0.671037  0.670709  0.664312   \n",
       "...        ...           ...         ...       ...       ...       ...   \n",
       "8886  0.066784      0.047283    1.731938  1.283501  1.273347  1.260870   \n",
       "8887  0.066784      0.046412    1.731938  1.029313  1.122194  1.160953   \n",
       "8888  0.066784      0.047202    1.731938  1.192686  1.213516  1.220047   \n",
       "8889  0.066784      0.046781    1.731938  1.084128  1.145636  1.174018   \n",
       "8890  0.066784      0.047419    1.731938  1.206713  1.216831  1.220721   \n",
       "\n",
       "      VWMA9_ch_p  VWMA14_ch_p  VWMA20_ch_p   1d_ch_p   2d_ch_p   3d_ch_p  \\\n",
       "0      -0.011236    -0.010638     0.055119 -0.115623 -0.137390 -0.004915   \n",
       "1      -0.155755    -0.177886    -0.129196 -0.178306 -0.291868 -0.313247   \n",
       "2      -0.064785    -0.085227    -0.051724  0.088260 -0.091620 -0.206183   \n",
       "3       0.063310     0.052715     0.079083  0.146719  0.236274  0.053755   \n",
       "4       0.213946     0.229893     0.248178  0.193158  0.342710  0.433996   \n",
       "...          ...          ...          ...       ...       ...       ...   \n",
       "8886   -0.105729    -0.194844    -0.259730 -0.022893 -0.003277  0.053551   \n",
       "8887    0.073731    -0.005183    -0.066509  0.187659  0.164337  0.184320   \n",
       "8888   -0.062234    -0.117377    -0.218323 -0.167310  0.017209 -0.005723   \n",
       "8889    0.049192    -0.011015    -0.111933  0.089987 -0.078829  0.107351   \n",
       "8890   -0.062504    -0.133473    -0.214555 -0.134587 -0.045812 -0.212355   \n",
       "\n",
       "       7d_ch_p  \n",
       "0     0.036863  \n",
       "1    -0.097717  \n",
       "2    -0.001535  \n",
       "3    -0.029657  \n",
       "4     0.242914  \n",
       "...        ...  \n",
       "8886 -0.245404  \n",
       "8887 -0.086124  \n",
       "8888 -0.120424  \n",
       "8889  0.017869  \n",
       "8890  0.024716  \n",
       "\n",
       "[8891 rows x 13 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj_YoY</th>\n",
       "      <th>Adj_surprise</th>\n",
       "      <th>Adj_YoYRev</th>\n",
       "      <th>RSI9</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>RSI20</th>\n",
       "      <th>VWMA9_ch_p</th>\n",
       "      <th>VWMA14_ch_p</th>\n",
       "      <th>VWMA20_ch_p</th>\n",
       "      <th>1d_ch_p</th>\n",
       "      <th>2d_ch_p</th>\n",
       "      <th>3d_ch_p</th>\n",
       "      <th>7d_ch_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.076836</td>\n",
       "      <td>0.181240</td>\n",
       "      <td>0.879633</td>\n",
       "      <td>0.764723</td>\n",
       "      <td>0.713634</td>\n",
       "      <td>-0.011236</td>\n",
       "      <td>-0.010638</td>\n",
       "      <td>0.055119</td>\n",
       "      <td>-0.115623</td>\n",
       "      <td>-0.137390</td>\n",
       "      <td>-0.004915</td>\n",
       "      <td>0.036863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.078231</td>\n",
       "      <td>0.181240</td>\n",
       "      <td>1.118191</td>\n",
       "      <td>0.940037</td>\n",
       "      <td>0.841139</td>\n",
       "      <td>-0.155755</td>\n",
       "      <td>-0.177886</td>\n",
       "      <td>-0.129196</td>\n",
       "      <td>-0.178306</td>\n",
       "      <td>-0.291868</td>\n",
       "      <td>-0.313247</td>\n",
       "      <td>-0.097717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.077547</td>\n",
       "      <td>0.181240</td>\n",
       "      <td>1.001552</td>\n",
       "      <td>0.875018</td>\n",
       "      <td>0.800532</td>\n",
       "      <td>-0.064785</td>\n",
       "      <td>-0.085227</td>\n",
       "      <td>-0.051724</td>\n",
       "      <td>0.088260</td>\n",
       "      <td>-0.091620</td>\n",
       "      <td>-0.206183</td>\n",
       "      <td>-0.001535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.076425</td>\n",
       "      <td>0.181240</td>\n",
       "      <td>0.836861</td>\n",
       "      <td>0.777853</td>\n",
       "      <td>0.737667</td>\n",
       "      <td>0.063310</td>\n",
       "      <td>0.052715</td>\n",
       "      <td>0.079083</td>\n",
       "      <td>0.146719</td>\n",
       "      <td>0.236274</td>\n",
       "      <td>0.053755</td>\n",
       "      <td>-0.029657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.074977</td>\n",
       "      <td>0.181240</td>\n",
       "      <td>0.671037</td>\n",
       "      <td>0.670709</td>\n",
       "      <td>0.664312</td>\n",
       "      <td>0.213946</td>\n",
       "      <td>0.229893</td>\n",
       "      <td>0.248178</td>\n",
       "      <td>0.193158</td>\n",
       "      <td>0.342710</td>\n",
       "      <td>0.433996</td>\n",
       "      <td>0.242914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8886</th>\n",
       "      <td>0.066784</td>\n",
       "      <td>0.047283</td>\n",
       "      <td>1.731938</td>\n",
       "      <td>1.283501</td>\n",
       "      <td>1.273347</td>\n",
       "      <td>1.260870</td>\n",
       "      <td>-0.105729</td>\n",
       "      <td>-0.194844</td>\n",
       "      <td>-0.259730</td>\n",
       "      <td>-0.022893</td>\n",
       "      <td>-0.003277</td>\n",
       "      <td>0.053551</td>\n",
       "      <td>-0.245404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8887</th>\n",
       "      <td>0.066784</td>\n",
       "      <td>0.046412</td>\n",
       "      <td>1.731938</td>\n",
       "      <td>1.029313</td>\n",
       "      <td>1.122194</td>\n",
       "      <td>1.160953</td>\n",
       "      <td>0.073731</td>\n",
       "      <td>-0.005183</td>\n",
       "      <td>-0.066509</td>\n",
       "      <td>0.187659</td>\n",
       "      <td>0.164337</td>\n",
       "      <td>0.184320</td>\n",
       "      <td>-0.086124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>0.066784</td>\n",
       "      <td>0.047202</td>\n",
       "      <td>1.731938</td>\n",
       "      <td>1.192686</td>\n",
       "      <td>1.213516</td>\n",
       "      <td>1.220047</td>\n",
       "      <td>-0.062234</td>\n",
       "      <td>-0.117377</td>\n",
       "      <td>-0.218323</td>\n",
       "      <td>-0.167310</td>\n",
       "      <td>0.017209</td>\n",
       "      <td>-0.005723</td>\n",
       "      <td>-0.120424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8889</th>\n",
       "      <td>0.066784</td>\n",
       "      <td>0.046781</td>\n",
       "      <td>1.731938</td>\n",
       "      <td>1.084128</td>\n",
       "      <td>1.145636</td>\n",
       "      <td>1.174018</td>\n",
       "      <td>0.049192</td>\n",
       "      <td>-0.011015</td>\n",
       "      <td>-0.111933</td>\n",
       "      <td>0.089987</td>\n",
       "      <td>-0.078829</td>\n",
       "      <td>0.107351</td>\n",
       "      <td>0.017869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8890</th>\n",
       "      <td>0.066784</td>\n",
       "      <td>0.047419</td>\n",
       "      <td>1.731938</td>\n",
       "      <td>1.206713</td>\n",
       "      <td>1.216831</td>\n",
       "      <td>1.220721</td>\n",
       "      <td>-0.062504</td>\n",
       "      <td>-0.133473</td>\n",
       "      <td>-0.214555</td>\n",
       "      <td>-0.134587</td>\n",
       "      <td>-0.045812</td>\n",
       "      <td>-0.212355</td>\n",
       "      <td>0.024716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8891 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Adj_YoY  Adj_surprise  Adj_YoYRev      RSI9     RSI14     RSI20  \\\n",
       "0     0.000319      0.076836    0.181240  0.879633  0.764723  0.713634   \n",
       "1     0.000319      0.078231    0.181240  1.118191  0.940037  0.841139   \n",
       "2     0.000319      0.077547    0.181240  1.001552  0.875018  0.800532   \n",
       "3     0.000319      0.076425    0.181240  0.836861  0.777853  0.737667   \n",
       "4     0.000319      0.074977    0.181240  0.671037  0.670709  0.664312   \n",
       "...        ...           ...         ...       ...       ...       ...   \n",
       "8886  0.066784      0.047283    1.731938  1.283501  1.273347  1.260870   \n",
       "8887  0.066784      0.046412    1.731938  1.029313  1.122194  1.160953   \n",
       "8888  0.066784      0.047202    1.731938  1.192686  1.213516  1.220047   \n",
       "8889  0.066784      0.046781    1.731938  1.084128  1.145636  1.174018   \n",
       "8890  0.066784      0.047419    1.731938  1.206713  1.216831  1.220721   \n",
       "\n",
       "      VWMA9_ch_p  VWMA14_ch_p  VWMA20_ch_p   1d_ch_p   2d_ch_p   3d_ch_p  \\\n",
       "0      -0.011236    -0.010638     0.055119 -0.115623 -0.137390 -0.004915   \n",
       "1      -0.155755    -0.177886    -0.129196 -0.178306 -0.291868 -0.313247   \n",
       "2      -0.064785    -0.085227    -0.051724  0.088260 -0.091620 -0.206183   \n",
       "3       0.063310     0.052715     0.079083  0.146719  0.236274  0.053755   \n",
       "4       0.213946     0.229893     0.248178  0.193158  0.342710  0.433996   \n",
       "...          ...          ...          ...       ...       ...       ...   \n",
       "8886   -0.105729    -0.194844    -0.259730 -0.022893 -0.003277  0.053551   \n",
       "8887    0.073731    -0.005183    -0.066509  0.187659  0.164337  0.184320   \n",
       "8888   -0.062234    -0.117377    -0.218323 -0.167310  0.017209 -0.005723   \n",
       "8889    0.049192    -0.011015    -0.111933  0.089987 -0.078829  0.107351   \n",
       "8890   -0.062504    -0.133473    -0.214555 -0.134587 -0.045812 -0.212355   \n",
       "\n",
       "       7d_ch_p  \n",
       "0     0.036863  \n",
       "1    -0.097717  \n",
       "2    -0.001535  \n",
       "3    -0.029657  \n",
       "4     0.242914  \n",
       "...        ...  \n",
       "8886 -0.245404  \n",
       "8887 -0.086124  \n",
       "8888 -0.120424  \n",
       "8889  0.017869  \n",
       "8890  0.024716  \n",
       "\n",
       "[8891 rows x 13 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7062    0]\n",
      " [1829    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.89      7062\n",
      "           1       0.00      0.00      0.00      1829\n",
      "\n",
      "    accuracy                           0.79      8891\n",
      "   macro avg       0.40      0.50      0.44      8891\n",
      "weighted avg       0.63      0.79      0.70      8891\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kukeshayanth/Desktop/semantic_corp/technical_analysis/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kukeshayanth/Desktop/semantic_corp/technical_analysis/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kukeshayanth/Desktop/semantic_corp/technical_analysis/env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "o = []\n",
    "for i in p:\n",
    "    o.append(i.argmax())\n",
    "    \n",
    "print(confusion_matrix(test_label_np.tolist(), o))\n",
    "\n",
    "print(classification_report(test_label_np.tolist(), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6652  410]\n",
      " [1612  217]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.94      0.87      7062\n",
      "           1       0.35      0.12      0.18      1829\n",
      "\n",
      "    accuracy                           0.77      8891\n",
      "   macro avg       0.58      0.53      0.52      8891\n",
      "weighted avg       0.71      0.77      0.73      8891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o = []\n",
    "for i in p:\n",
    "    o.append(i.argmax())\n",
    "    \n",
    "print(confusion_matrix(test_label_np.tolist(), o))\n",
    "\n",
    "print(classification_report(test_label_np.tolist(), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7062    0]\n",
      " [1828    1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.89      7062\n",
      "           1       1.00      0.00      0.00      1829\n",
      "\n",
      "    accuracy                           0.79      8891\n",
      "   macro avg       0.90      0.50      0.44      8891\n",
      "weighted avg       0.84      0.79      0.70      8891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o = []\n",
    "for i in p:\n",
    "    o.append(i.argmax())\n",
    "    \n",
    "print(confusion_matrix(test_label_np.tolist(), o))\n",
    "\n",
    "print(classification_report(test_label_np.tolist(), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6940  122]\n",
      " [1731   98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88      7062\n",
      "           1       0.45      0.05      0.10      1829\n",
      "\n",
      "    accuracy                           0.79      8891\n",
      "   macro avg       0.62      0.52      0.49      8891\n",
      "weighted avg       0.73      0.79      0.72      8891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o = []\n",
    "for i in p:\n",
    "    o.append(i.argmax())\n",
    "    \n",
    "print(confusion_matrix(test_label_np.tolist(), o))\n",
    "\n",
    "print(classification_report(test_label_np.tolist(), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6718  344]\n",
      " [1714  115]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.87      7062\n",
      "           1       0.25      0.06      0.10      1829\n",
      "\n",
      "    accuracy                           0.77      8891\n",
      "   macro avg       0.52      0.51      0.48      8891\n",
      "weighted avg       0.68      0.77      0.71      8891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o = []\n",
    "for i in p:\n",
    "    o.append(i.argmax())\n",
    "    \n",
    "print(confusion_matrix(test_label_np.tolist(), o))\n",
    "\n",
    "print(classification_report(test_label_np.tolist(), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed------5 \n",
      "\n",
      "[[7059    3]\n",
      " [1829    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.89      7062\n",
      "           1       0.00      0.00      0.00      1829\n",
      "\n",
      "    accuracy                           0.79      8891\n",
      "   macro avg       0.40      0.50      0.44      8891\n",
      "weighted avg       0.63      0.79      0.70      8891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# p = model.predict([ test_pattern_lstm_np,test_volume_lstm_np, X_test_np,test_eps_lstm_np, test_eps_vec_np])\n",
    "\n",
    "print('completed------5 \\n')\n",
    "\n",
    "o = []\n",
    "for i in p:\n",
    "    o.append(i.argmax())\n",
    "    \n",
    "print(confusion_matrix(test_label_np.tolist(), o))\n",
    "\n",
    "print(classification_report(test_label_np.tolist(), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj_YoY</th>\n",
       "      <th>Adj_surprise</th>\n",
       "      <th>Adj_YoYRev</th>\n",
       "      <th>RSI9</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>RSI20</th>\n",
       "      <th>VWMA9_ch_p</th>\n",
       "      <th>VWMA14_ch_p</th>\n",
       "      <th>VWMA20_ch_p</th>\n",
       "      <th>1d_ch_p</th>\n",
       "      <th>2d_ch_p</th>\n",
       "      <th>3d_ch_p</th>\n",
       "      <th>7d_ch_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005407</td>\n",
       "      <td>0.052908</td>\n",
       "      <td>0.861154</td>\n",
       "      <td>0.531952</td>\n",
       "      <td>0.573029</td>\n",
       "      <td>0.616529</td>\n",
       "      <td>0.070330</td>\n",
       "      <td>0.162753</td>\n",
       "      <td>0.220182</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.193288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>1.419465</td>\n",
       "      <td>0.722054</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.736283</td>\n",
       "      <td>0.048115</td>\n",
       "      <td>0.109316</td>\n",
       "      <td>0.140843</td>\n",
       "      <td>-0.052126</td>\n",
       "      <td>-0.025432</td>\n",
       "      <td>-0.050705</td>\n",
       "      <td>0.089964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.039272</td>\n",
       "      <td>0.121790</td>\n",
       "      <td>1.192702</td>\n",
       "      <td>1.113842</td>\n",
       "      <td>1.053255</td>\n",
       "      <td>-0.103260</td>\n",
       "      <td>-0.102613</td>\n",
       "      <td>-0.116800</td>\n",
       "      <td>0.054980</td>\n",
       "      <td>-0.035961</td>\n",
       "      <td>-0.083522</td>\n",
       "      <td>-0.168799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127379</td>\n",
       "      <td>0.136322</td>\n",
       "      <td>1.733934</td>\n",
       "      <td>0.932742</td>\n",
       "      <td>1.054986</td>\n",
       "      <td>1.110072</td>\n",
       "      <td>0.195228</td>\n",
       "      <td>0.109557</td>\n",
       "      <td>0.056114</td>\n",
       "      <td>0.168044</td>\n",
       "      <td>0.456292</td>\n",
       "      <td>0.452385</td>\n",
       "      <td>-0.048191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004134</td>\n",
       "      <td>0.091524</td>\n",
       "      <td>0.968236</td>\n",
       "      <td>1.128751</td>\n",
       "      <td>1.087713</td>\n",
       "      <td>1.065031</td>\n",
       "      <td>-0.036381</td>\n",
       "      <td>-0.058686</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.046430</td>\n",
       "      <td>-0.053739</td>\n",
       "      <td>-0.004909</td>\n",
       "      <td>-0.046430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234900</th>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.035620</td>\n",
       "      <td>0.048354</td>\n",
       "      <td>1.116727</td>\n",
       "      <td>0.948461</td>\n",
       "      <td>0.859905</td>\n",
       "      <td>-0.085264</td>\n",
       "      <td>-0.125786</td>\n",
       "      <td>-0.111490</td>\n",
       "      <td>-0.044928</td>\n",
       "      <td>-0.027317</td>\n",
       "      <td>-0.087844</td>\n",
       "      <td>-0.189712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234901</th>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.490309</td>\n",
       "      <td>1.300834</td>\n",
       "      <td>1.254572</td>\n",
       "      <td>1.126924</td>\n",
       "      <td>1.064227</td>\n",
       "      <td>-0.186760</td>\n",
       "      <td>-0.241261</td>\n",
       "      <td>-0.200556</td>\n",
       "      <td>-0.077322</td>\n",
       "      <td>-0.275986</td>\n",
       "      <td>-0.244291</td>\n",
       "      <td>-0.354265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234902</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.216629</td>\n",
       "      <td>0.435016</td>\n",
       "      <td>0.614554</td>\n",
       "      <td>0.733274</td>\n",
       "      <td>0.179758</td>\n",
       "      <td>0.285736</td>\n",
       "      <td>0.420829</td>\n",
       "      <td>0.016134</td>\n",
       "      <td>-0.005372</td>\n",
       "      <td>0.055360</td>\n",
       "      <td>0.400782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234903</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.011080</td>\n",
       "      <td>0.318248</td>\n",
       "      <td>1.083885</td>\n",
       "      <td>1.139766</td>\n",
       "      <td>1.153376</td>\n",
       "      <td>0.029223</td>\n",
       "      <td>0.051027</td>\n",
       "      <td>-0.044391</td>\n",
       "      <td>-0.075438</td>\n",
       "      <td>0.149390</td>\n",
       "      <td>0.135144</td>\n",
       "      <td>-0.006116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234904</th>\n",
       "      <td>0.006092</td>\n",
       "      <td>0.070593</td>\n",
       "      <td>0.996386</td>\n",
       "      <td>1.177529</td>\n",
       "      <td>1.155673</td>\n",
       "      <td>1.120716</td>\n",
       "      <td>-0.067506</td>\n",
       "      <td>-0.136865</td>\n",
       "      <td>-0.304312</td>\n",
       "      <td>0.085160</td>\n",
       "      <td>-0.232392</td>\n",
       "      <td>-0.215742</td>\n",
       "      <td>-0.016061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234905 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Adj_YoY  Adj_surprise  Adj_YoYRev      RSI9     RSI14     RSI20  \\\n",
       "0       0.005407      0.052908    0.861154  0.531952  0.573029  0.616529   \n",
       "1       0.003673      0.011859    1.419465  0.722054  0.716000  0.736283   \n",
       "2       0.000291      0.039272    0.121790  1.192702  1.113842  1.053255   \n",
       "3       0.127379      0.136322    1.733934  0.932742  1.054986  1.110072   \n",
       "4       0.004134      0.091524    0.968236  1.128751  1.087713  1.065031   \n",
       "...          ...           ...         ...       ...       ...       ...   \n",
       "234900  0.005665      0.035620    0.048354  1.116727  0.948461  0.859905   \n",
       "234901  0.006366      0.490309    1.300834  1.254572  1.126924  1.064227   \n",
       "234902  0.000018      0.000000    1.216629  0.435016  0.614554  0.733274   \n",
       "234903  0.000021      0.011080    0.318248  1.083885  1.139766  1.153376   \n",
       "234904  0.006092      0.070593    0.996386  1.177529  1.155673  1.120716   \n",
       "\n",
       "        VWMA9_ch_p  VWMA14_ch_p  VWMA20_ch_p   1d_ch_p   2d_ch_p   3d_ch_p  \\\n",
       "0         0.070330     0.162753     0.220182  0.007254  0.009108  0.034722   \n",
       "1         0.048115     0.109316     0.140843 -0.052126 -0.025432 -0.050705   \n",
       "2        -0.103260    -0.102613    -0.116800  0.054980 -0.035961 -0.083522   \n",
       "3         0.195228     0.109557     0.056114  0.168044  0.456292  0.452385   \n",
       "4        -0.036381    -0.058686    -0.000090 -0.046430 -0.053739 -0.004909   \n",
       "...            ...          ...          ...       ...       ...       ...   \n",
       "234900   -0.085264    -0.125786    -0.111490 -0.044928 -0.027317 -0.087844   \n",
       "234901   -0.186760    -0.241261    -0.200556 -0.077322 -0.275986 -0.244291   \n",
       "234902    0.179758     0.285736     0.420829  0.016134 -0.005372  0.055360   \n",
       "234903    0.029223     0.051027    -0.044391 -0.075438  0.149390  0.135144   \n",
       "234904   -0.067506    -0.136865    -0.304312  0.085160 -0.232392 -0.215742   \n",
       "\n",
       "         7d_ch_p  \n",
       "0       0.193288  \n",
       "1       0.089964  \n",
       "2      -0.168799  \n",
       "3      -0.048191  \n",
       "4      -0.046430  \n",
       "...          ...  \n",
       "234900 -0.189712  \n",
       "234901 -0.354265  \n",
       "234902  0.400782  \n",
       "234903 -0.006116  \n",
       "234904 -0.016061  \n",
       "\n",
       "[234905 rows x 13 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj_YoY</th>\n",
       "      <th>Adj_surprise</th>\n",
       "      <th>Adj_YoYRev</th>\n",
       "      <th>RSI9</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>RSI20</th>\n",
       "      <th>VWMA9_ch_p</th>\n",
       "      <th>VWMA14_ch_p</th>\n",
       "      <th>VWMA20_ch_p</th>\n",
       "      <th>1d_ch_p</th>\n",
       "      <th>2d_ch_p</th>\n",
       "      <th>3d_ch_p</th>\n",
       "      <th>7d_ch_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005407</td>\n",
       "      <td>0.052908</td>\n",
       "      <td>0.861154</td>\n",
       "      <td>1.989361</td>\n",
       "      <td>1.988539</td>\n",
       "      <td>1.987669</td>\n",
       "      <td>0.070330</td>\n",
       "      <td>0.162753</td>\n",
       "      <td>0.220182</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.193288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>1.419465</td>\n",
       "      <td>1.985559</td>\n",
       "      <td>1.985680</td>\n",
       "      <td>1.985274</td>\n",
       "      <td>0.048115</td>\n",
       "      <td>0.109316</td>\n",
       "      <td>0.140843</td>\n",
       "      <td>-0.052126</td>\n",
       "      <td>-0.025432</td>\n",
       "      <td>-0.050705</td>\n",
       "      <td>0.089964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.039272</td>\n",
       "      <td>0.121790</td>\n",
       "      <td>1.976146</td>\n",
       "      <td>1.977723</td>\n",
       "      <td>1.978935</td>\n",
       "      <td>-0.103260</td>\n",
       "      <td>-0.102613</td>\n",
       "      <td>-0.116800</td>\n",
       "      <td>0.054980</td>\n",
       "      <td>-0.035961</td>\n",
       "      <td>-0.083522</td>\n",
       "      <td>-0.168799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127379</td>\n",
       "      <td>0.136322</td>\n",
       "      <td>1.733934</td>\n",
       "      <td>1.981345</td>\n",
       "      <td>1.978900</td>\n",
       "      <td>1.977799</td>\n",
       "      <td>0.195228</td>\n",
       "      <td>0.109557</td>\n",
       "      <td>0.056114</td>\n",
       "      <td>0.168044</td>\n",
       "      <td>0.456292</td>\n",
       "      <td>0.452385</td>\n",
       "      <td>-0.048191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004134</td>\n",
       "      <td>0.091524</td>\n",
       "      <td>0.968236</td>\n",
       "      <td>1.977425</td>\n",
       "      <td>1.978246</td>\n",
       "      <td>1.978699</td>\n",
       "      <td>-0.036381</td>\n",
       "      <td>-0.058686</td>\n",
       "      <td>-0.000090</td>\n",
       "      <td>-0.046430</td>\n",
       "      <td>-0.053739</td>\n",
       "      <td>-0.004909</td>\n",
       "      <td>-0.046430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234900</th>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.035620</td>\n",
       "      <td>0.048354</td>\n",
       "      <td>1.977665</td>\n",
       "      <td>1.981031</td>\n",
       "      <td>1.982802</td>\n",
       "      <td>-0.085264</td>\n",
       "      <td>-0.125786</td>\n",
       "      <td>-0.111490</td>\n",
       "      <td>-0.044928</td>\n",
       "      <td>-0.027317</td>\n",
       "      <td>-0.087844</td>\n",
       "      <td>-0.189712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234901</th>\n",
       "      <td>0.006366</td>\n",
       "      <td>0.490309</td>\n",
       "      <td>1.300834</td>\n",
       "      <td>1.974909</td>\n",
       "      <td>1.977462</td>\n",
       "      <td>1.978715</td>\n",
       "      <td>-0.186760</td>\n",
       "      <td>-0.241261</td>\n",
       "      <td>-0.200556</td>\n",
       "      <td>-0.077322</td>\n",
       "      <td>-0.275986</td>\n",
       "      <td>-0.244291</td>\n",
       "      <td>-0.354265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234902</th>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.216629</td>\n",
       "      <td>1.991300</td>\n",
       "      <td>1.987709</td>\n",
       "      <td>1.985335</td>\n",
       "      <td>0.179758</td>\n",
       "      <td>0.285736</td>\n",
       "      <td>0.420829</td>\n",
       "      <td>0.016134</td>\n",
       "      <td>-0.005372</td>\n",
       "      <td>0.055360</td>\n",
       "      <td>0.400782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234903</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.011080</td>\n",
       "      <td>0.318248</td>\n",
       "      <td>1.978322</td>\n",
       "      <td>1.977205</td>\n",
       "      <td>1.976932</td>\n",
       "      <td>0.029223</td>\n",
       "      <td>0.051027</td>\n",
       "      <td>-0.044391</td>\n",
       "      <td>-0.075438</td>\n",
       "      <td>0.149390</td>\n",
       "      <td>0.135144</td>\n",
       "      <td>-0.006116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234904</th>\n",
       "      <td>0.006092</td>\n",
       "      <td>0.070593</td>\n",
       "      <td>0.996386</td>\n",
       "      <td>1.976449</td>\n",
       "      <td>1.976887</td>\n",
       "      <td>1.977586</td>\n",
       "      <td>-0.067506</td>\n",
       "      <td>-0.136865</td>\n",
       "      <td>-0.304312</td>\n",
       "      <td>0.085160</td>\n",
       "      <td>-0.232392</td>\n",
       "      <td>-0.215742</td>\n",
       "      <td>-0.016061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234905 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Adj_YoY  Adj_surprise  Adj_YoYRev      RSI9     RSI14     RSI20  \\\n",
       "0       0.005407      0.052908    0.861154  1.989361  1.988539  1.987669   \n",
       "1       0.003673      0.011859    1.419465  1.985559  1.985680  1.985274   \n",
       "2       0.000291      0.039272    0.121790  1.976146  1.977723  1.978935   \n",
       "3       0.127379      0.136322    1.733934  1.981345  1.978900  1.977799   \n",
       "4       0.004134      0.091524    0.968236  1.977425  1.978246  1.978699   \n",
       "...          ...           ...         ...       ...       ...       ...   \n",
       "234900  0.005665      0.035620    0.048354  1.977665  1.981031  1.982802   \n",
       "234901  0.006366      0.490309    1.300834  1.974909  1.977462  1.978715   \n",
       "234902  0.000018      0.000000    1.216629  1.991300  1.987709  1.985335   \n",
       "234903  0.000021      0.011080    0.318248  1.978322  1.977205  1.976932   \n",
       "234904  0.006092      0.070593    0.996386  1.976449  1.976887  1.977586   \n",
       "\n",
       "        VWMA9_ch_p  VWMA14_ch_p  VWMA20_ch_p   1d_ch_p   2d_ch_p   3d_ch_p  \\\n",
       "0         0.070330     0.162753     0.220182  0.007254  0.009108  0.034722   \n",
       "1         0.048115     0.109316     0.140843 -0.052126 -0.025432 -0.050705   \n",
       "2        -0.103260    -0.102613    -0.116800  0.054980 -0.035961 -0.083522   \n",
       "3         0.195228     0.109557     0.056114  0.168044  0.456292  0.452385   \n",
       "4        -0.036381    -0.058686    -0.000090 -0.046430 -0.053739 -0.004909   \n",
       "...            ...          ...          ...       ...       ...       ...   \n",
       "234900   -0.085264    -0.125786    -0.111490 -0.044928 -0.027317 -0.087844   \n",
       "234901   -0.186760    -0.241261    -0.200556 -0.077322 -0.275986 -0.244291   \n",
       "234902    0.179758     0.285736     0.420829  0.016134 -0.005372  0.055360   \n",
       "234903    0.029223     0.051027    -0.044391 -0.075438  0.149390  0.135144   \n",
       "234904   -0.067506    -0.136865    -0.304312  0.085160 -0.232392 -0.215742   \n",
       "\n",
       "         7d_ch_p  \n",
       "0       0.193288  \n",
       "1       0.089964  \n",
       "2      -0.168799  \n",
       "3      -0.048191  \n",
       "4      -0.046430  \n",
       "...          ...  \n",
       "234900 -0.189712  \n",
       "234901 -0.354265  \n",
       "234902  0.400782  \n",
       "234903 -0.006116  \n",
       "234904 -0.016061  \n",
       "\n",
       "[234905 rows x 13 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vola(x):\n",
    "    if x >= 0.02:\n",
    "        return 1\n",
    "    elif x <= -0.02:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "new_df['vol1'] = new_df[\"fut_snp_5d_ch_p\"].apply(vola)\n",
    "test_df['vol1'] = test_df[\"fut_snp_5d_ch_p\"].apply(vola)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.765173\n",
       "1    0.234827\n",
       "Name: vol, dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['vol'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.624227\n",
       "1    0.375773\n",
       "Name: vol1, dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['vol1'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed------5 \n",
      "\n",
      "[[6857  205]\n",
      " [1697  132]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88      7062\n",
      "           1       0.39      0.07      0.12      1829\n",
      "\n",
      "    accuracy                           0.79      8891\n",
      "   macro avg       0.60      0.52      0.50      8891\n",
      "weighted avg       0.72      0.79      0.72      8891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = model.predict([ test_pattern_lstm_np,test_volume_lstm_np, X_test_np,test_eps_lstm_np, test_eps_vec_np])\n",
    "\n",
    "print('completed------5 \\n')\n",
    "\n",
    "o = []\n",
    "for i in p:\n",
    "    o.append(i.argmax())\n",
    "    \n",
    "print(confusion_matrix(test_label_np.tolist(), o))\n",
    "\n",
    "print(classification_report(test_label_np.tolist(), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9320192 , 0.06798082],\n",
       "       [0.9258238 , 0.07417624],\n",
       "       [0.9309752 , 0.06902475],\n",
       "       ...,\n",
       "       [0.91011477, 0.08988518],\n",
       "       [0.9137494 , 0.08625058],\n",
       "       [0.90693605, 0.09306397]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6972   90]\n",
      " [1754   75]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.99      0.88      7062\n",
      "           1       0.45      0.04      0.08      1829\n",
      "\n",
      "    accuracy                           0.79      8891\n",
      "   macro avg       0.63      0.51      0.48      8891\n",
      "weighted avg       0.73      0.79      0.72      8891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o = []\n",
    "for i in p[:,1].tolist():\n",
    "    if i  > 0.18:\n",
    "        o.append(1)\n",
    "    else:\n",
    "        o.append(0)\n",
    "    \n",
    "print(confusion_matrix(test_df['vol'].tolist(), o))\n",
    "\n",
    "print(classification_report(test_df['vol'].tolist(), o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.95      0.68      4587\n",
      "           1       0.60      0.08      0.14      4304\n",
      "\n",
      "    accuracy                           0.53      8891\n",
      "   macro avg       0.56      0.52      0.41      8891\n",
      "weighted avg       0.56      0.53      0.42      8891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('              precision    recall  f1-score   support\\n\\n           0       0.52      0.95      0.68      4587\\n           1       0.60      0.08      0.14      4304\\n\\n    accuracy                           0.53      8891\\n   macro avg       0.56      0.52      0.41      8891\\nweighted avg       0.56      0.53      0.42      8891\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train6_df = new_df[['VWMA9_ch_p', 'VWMA14_ch_p', 'VWMA20_ch_p','1d_ch_p', '2d_ch_p', '3d_ch_p', '7d_ch_p',\n",
    "                     '1d_adx9_ch_p', '2d_adx9_ch_p', '3d_adx9_ch_p', '7d_adx9_ch_p',\n",
    "                      '1d_adx14_ch_p', '2d_adx14_ch_p', '3d_adx14_ch_p', '7d_adx14_ch_p',\n",
    "                     '1d_RSI14_ch_p', '2d_RSI14_ch_p', '3d_RSI14_ch_p', '7d_RSI14_ch_p',\"VWMA2_ch_p\",\"VWMA5_ch_p\",\"VWMA30_ch_p\",'1d_ch_v','7d_ch_v','12d_ch_v','21d_ch_v',\n",
    "       'vola5','vola9','vola21','macd259','signal259','hist259','macd91121','signal91121','hist91121','1d_DI+14_ushift',\n",
    "                      '2d_DI+14_ushift','3d_DI+14_ushift','7d_DI+14_ushift','1d_DI-14_ushift','2d_DI-14_ushift',\n",
    "                      '3d_DI-14_ushift','7d_DI-14_ushift','1d_DI+20_ushift','2d_DI+20_ushift',\n",
    "                      '3d_DI+20_ushift','7d_DI+20_ushift','1d_DI-20_ushift','2d_DI-20_ushift',\n",
    "                      '3d_DI-20_ushift','7d_DI-20_ushift']]\n",
    "# X_train_df = pd.concat([X_train0_df[:train_len], X_train1_df[:train_len],  X_train2_df[:train_len], X_train6_df[:train_len]], axis=1)\n",
    "# X_test_df = pd.concat([X_train0_df[train_len:], X_train1_df[train_len:],  X_train2_df[train_len:], X_train3_df[train_len:]], axis=1)\n",
    "\n",
    "X_train_np = np.array(X_train6_df[:train_len])\n",
    "X_test_np = np.array(X_train6_df[train_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.1155\n",
      "0:\tlearn: 0.6477733\ttest: 0.6474890\tbest: 0.6474890 (0)\ttotal: 76.8ms\tremaining: 1m 16s\n",
      "1:\tlearn: 0.6133018\ttest: 0.6126958\tbest: 0.6126958 (1)\ttotal: 101ms\tremaining: 50.3s\n",
      "2:\tlearn: 0.5871383\ttest: 0.5863651\tbest: 0.5863651 (2)\ttotal: 124ms\tremaining: 41.2s\n",
      "3:\tlearn: 0.5661681\ttest: 0.5652539\tbest: 0.5652539 (3)\ttotal: 147ms\tremaining: 36.5s\n",
      "4:\tlearn: 0.5509038\ttest: 0.5499100\tbest: 0.5499100 (4)\ttotal: 171ms\tremaining: 34.1s\n",
      "5:\tlearn: 0.5396316\ttest: 0.5385499\tbest: 0.5385499 (5)\ttotal: 194ms\tremaining: 32.2s\n",
      "6:\tlearn: 0.5299428\ttest: 0.5288010\tbest: 0.5288010 (6)\ttotal: 219ms\tremaining: 31.1s\n",
      "7:\tlearn: 0.5230288\ttest: 0.5218823\tbest: 0.5218823 (7)\ttotal: 241ms\tremaining: 29.9s\n",
      "8:\tlearn: 0.5176593\ttest: 0.5165279\tbest: 0.5165279 (8)\ttotal: 266ms\tremaining: 29.2s\n",
      "9:\tlearn: 0.5128567\ttest: 0.5116829\tbest: 0.5116829 (9)\ttotal: 288ms\tremaining: 28.5s\n",
      "10:\tlearn: 0.5093559\ttest: 0.5082249\tbest: 0.5082249 (10)\ttotal: 311ms\tremaining: 28s\n",
      "11:\tlearn: 0.5064351\ttest: 0.5053013\tbest: 0.5053013 (11)\ttotal: 333ms\tremaining: 27.4s\n",
      "12:\tlearn: 0.5040578\ttest: 0.5029492\tbest: 0.5029492 (12)\ttotal: 365ms\tremaining: 27.7s\n",
      "13:\tlearn: 0.5021909\ttest: 0.5011015\tbest: 0.5011015 (13)\ttotal: 393ms\tremaining: 27.6s\n",
      "14:\tlearn: 0.5005611\ttest: 0.4994720\tbest: 0.4994720 (14)\ttotal: 418ms\tremaining: 27.4s\n",
      "15:\tlearn: 0.4994132\ttest: 0.4983409\tbest: 0.4983409 (15)\ttotal: 440ms\tremaining: 27.1s\n",
      "16:\tlearn: 0.4983991\ttest: 0.4973707\tbest: 0.4973707 (16)\ttotal: 464ms\tremaining: 26.8s\n",
      "17:\tlearn: 0.4976645\ttest: 0.4966987\tbest: 0.4966987 (17)\ttotal: 485ms\tremaining: 26.5s\n",
      "18:\tlearn: 0.4969423\ttest: 0.4959949\tbest: 0.4959949 (18)\ttotal: 508ms\tremaining: 26.3s\n",
      "19:\tlearn: 0.4962575\ttest: 0.4953670\tbest: 0.4953670 (19)\ttotal: 535ms\tremaining: 26.2s\n",
      "20:\tlearn: 0.4957539\ttest: 0.4948987\tbest: 0.4948987 (20)\ttotal: 559ms\tremaining: 26s\n",
      "21:\tlearn: 0.4953250\ttest: 0.4944745\tbest: 0.4944745 (21)\ttotal: 583ms\tremaining: 25.9s\n",
      "22:\tlearn: 0.4949775\ttest: 0.4941507\tbest: 0.4941507 (22)\ttotal: 604ms\tremaining: 25.7s\n",
      "23:\tlearn: 0.4947170\ttest: 0.4939236\tbest: 0.4939236 (23)\ttotal: 630ms\tremaining: 25.6s\n",
      "24:\tlearn: 0.4944112\ttest: 0.4936825\tbest: 0.4936825 (24)\ttotal: 654ms\tremaining: 25.5s\n",
      "25:\tlearn: 0.4941286\ttest: 0.4934351\tbest: 0.4934351 (25)\ttotal: 681ms\tremaining: 25.5s\n",
      "26:\tlearn: 0.4938410\ttest: 0.4932472\tbest: 0.4932472 (26)\ttotal: 706ms\tremaining: 25.4s\n",
      "27:\tlearn: 0.4935848\ttest: 0.4930332\tbest: 0.4930332 (27)\ttotal: 728ms\tremaining: 25.3s\n",
      "28:\tlearn: 0.4933952\ttest: 0.4928895\tbest: 0.4928895 (28)\ttotal: 749ms\tremaining: 25.1s\n",
      "29:\tlearn: 0.4932119\ttest: 0.4928079\tbest: 0.4928079 (29)\ttotal: 772ms\tremaining: 25s\n",
      "30:\tlearn: 0.4930123\ttest: 0.4926595\tbest: 0.4926595 (30)\ttotal: 792ms\tremaining: 24.8s\n",
      "31:\tlearn: 0.4928507\ttest: 0.4925218\tbest: 0.4925218 (31)\ttotal: 815ms\tremaining: 24.7s\n",
      "32:\tlearn: 0.4927354\ttest: 0.4924420\tbest: 0.4924420 (32)\ttotal: 841ms\tremaining: 24.6s\n",
      "33:\tlearn: 0.4925911\ttest: 0.4923726\tbest: 0.4923726 (33)\ttotal: 871ms\tremaining: 24.8s\n",
      "34:\tlearn: 0.4924654\ttest: 0.4922986\tbest: 0.4922986 (34)\ttotal: 897ms\tremaining: 24.7s\n",
      "35:\tlearn: 0.4923351\ttest: 0.4922254\tbest: 0.4922254 (35)\ttotal: 926ms\tremaining: 24.8s\n",
      "36:\tlearn: 0.4921940\ttest: 0.4921353\tbest: 0.4921353 (36)\ttotal: 948ms\tremaining: 24.7s\n",
      "37:\tlearn: 0.4920512\ttest: 0.4920403\tbest: 0.4920403 (37)\ttotal: 974ms\tremaining: 24.6s\n",
      "38:\tlearn: 0.4919056\ttest: 0.4919601\tbest: 0.4919601 (38)\ttotal: 997ms\tremaining: 24.6s\n",
      "39:\tlearn: 0.4918033\ttest: 0.4918904\tbest: 0.4918904 (39)\ttotal: 1.02s\tremaining: 24.6s\n",
      "40:\tlearn: 0.4916848\ttest: 0.4917911\tbest: 0.4917911 (40)\ttotal: 1.05s\tremaining: 24.6s\n",
      "41:\tlearn: 0.4916064\ttest: 0.4917415\tbest: 0.4917415 (41)\ttotal: 1.07s\tremaining: 24.5s\n",
      "42:\tlearn: 0.4914849\ttest: 0.4916659\tbest: 0.4916659 (42)\ttotal: 1.1s\tremaining: 24.4s\n",
      "43:\tlearn: 0.4913733\ttest: 0.4916059\tbest: 0.4916059 (43)\ttotal: 1.12s\tremaining: 24.4s\n",
      "44:\tlearn: 0.4912799\ttest: 0.4915537\tbest: 0.4915537 (44)\ttotal: 1.15s\tremaining: 24.4s\n",
      "45:\tlearn: 0.4911711\ttest: 0.4914881\tbest: 0.4914881 (45)\ttotal: 1.17s\tremaining: 24.3s\n",
      "46:\tlearn: 0.4910866\ttest: 0.4914327\tbest: 0.4914327 (46)\ttotal: 1.19s\tremaining: 24.2s\n",
      "47:\tlearn: 0.4909824\ttest: 0.4913755\tbest: 0.4913755 (47)\ttotal: 1.21s\tremaining: 24.1s\n",
      "48:\tlearn: 0.4909182\ttest: 0.4913653\tbest: 0.4913653 (48)\ttotal: 1.24s\tremaining: 24s\n",
      "49:\tlearn: 0.4908350\ttest: 0.4913547\tbest: 0.4913547 (49)\ttotal: 1.26s\tremaining: 23.9s\n",
      "50:\tlearn: 0.4907715\ttest: 0.4913227\tbest: 0.4913227 (50)\ttotal: 1.28s\tremaining: 23.8s\n",
      "51:\tlearn: 0.4906786\ttest: 0.4912940\tbest: 0.4912940 (51)\ttotal: 1.3s\tremaining: 23.8s\n",
      "52:\tlearn: 0.4905463\ttest: 0.4912299\tbest: 0.4912299 (52)\ttotal: 1.33s\tremaining: 23.7s\n",
      "53:\tlearn: 0.4904601\ttest: 0.4911972\tbest: 0.4911972 (53)\ttotal: 1.35s\tremaining: 23.6s\n",
      "54:\tlearn: 0.4903752\ttest: 0.4911404\tbest: 0.4911404 (54)\ttotal: 1.37s\tremaining: 23.5s\n",
      "55:\tlearn: 0.4902862\ttest: 0.4910810\tbest: 0.4910810 (55)\ttotal: 1.39s\tremaining: 23.4s\n",
      "56:\tlearn: 0.4902118\ttest: 0.4910454\tbest: 0.4910454 (56)\ttotal: 1.41s\tremaining: 23.3s\n",
      "57:\tlearn: 0.4900588\ttest: 0.4909440\tbest: 0.4909440 (57)\ttotal: 1.43s\tremaining: 23.2s\n",
      "58:\tlearn: 0.4899841\ttest: 0.4909189\tbest: 0.4909189 (58)\ttotal: 1.45s\tremaining: 23.1s\n",
      "59:\tlearn: 0.4899023\ttest: 0.4908797\tbest: 0.4908797 (59)\ttotal: 1.47s\tremaining: 23.1s\n",
      "60:\tlearn: 0.4898320\ttest: 0.4908492\tbest: 0.4908492 (60)\ttotal: 1.5s\tremaining: 23s\n",
      "61:\tlearn: 0.4897397\ttest: 0.4908095\tbest: 0.4908095 (61)\ttotal: 1.52s\tremaining: 23s\n",
      "62:\tlearn: 0.4896564\ttest: 0.4907906\tbest: 0.4907906 (62)\ttotal: 1.54s\tremaining: 22.9s\n",
      "63:\tlearn: 0.4895616\ttest: 0.4907872\tbest: 0.4907872 (63)\ttotal: 1.56s\tremaining: 22.8s\n",
      "64:\tlearn: 0.4894595\ttest: 0.4907382\tbest: 0.4907382 (64)\ttotal: 1.58s\tremaining: 22.7s\n",
      "65:\tlearn: 0.4893562\ttest: 0.4906701\tbest: 0.4906701 (65)\ttotal: 1.6s\tremaining: 22.6s\n",
      "66:\tlearn: 0.4892734\ttest: 0.4906369\tbest: 0.4906369 (66)\ttotal: 1.62s\tremaining: 22.6s\n",
      "67:\tlearn: 0.4891924\ttest: 0.4906241\tbest: 0.4906241 (67)\ttotal: 1.65s\tremaining: 22.6s\n",
      "68:\tlearn: 0.4891054\ttest: 0.4906125\tbest: 0.4906125 (68)\ttotal: 1.67s\tremaining: 22.5s\n",
      "69:\tlearn: 0.4890251\ttest: 0.4905769\tbest: 0.4905769 (69)\ttotal: 1.69s\tremaining: 22.5s\n",
      "70:\tlearn: 0.4889237\ttest: 0.4905448\tbest: 0.4905448 (70)\ttotal: 1.71s\tremaining: 22.4s\n",
      "71:\tlearn: 0.4888689\ttest: 0.4905022\tbest: 0.4905022 (71)\ttotal: 1.73s\tremaining: 22.3s\n",
      "72:\tlearn: 0.4887949\ttest: 0.4904764\tbest: 0.4904764 (72)\ttotal: 1.75s\tremaining: 22.3s\n",
      "73:\tlearn: 0.4887240\ttest: 0.4904801\tbest: 0.4904764 (72)\ttotal: 1.78s\tremaining: 22.2s\n",
      "74:\tlearn: 0.4885890\ttest: 0.4904310\tbest: 0.4904310 (74)\ttotal: 1.8s\tremaining: 22.2s\n",
      "75:\tlearn: 0.4885066\ttest: 0.4904248\tbest: 0.4904248 (75)\ttotal: 1.82s\tremaining: 22.1s\n",
      "76:\tlearn: 0.4884382\ttest: 0.4904209\tbest: 0.4904209 (76)\ttotal: 1.83s\tremaining: 22s\n",
      "77:\tlearn: 0.4883775\ttest: 0.4904020\tbest: 0.4904020 (77)\ttotal: 1.85s\tremaining: 21.9s\n",
      "78:\tlearn: 0.4882783\ttest: 0.4903388\tbest: 0.4903388 (78)\ttotal: 1.88s\tremaining: 22s\n",
      "79:\tlearn: 0.4882102\ttest: 0.4903122\tbest: 0.4903122 (79)\ttotal: 1.91s\tremaining: 21.9s\n",
      "80:\tlearn: 0.4881015\ttest: 0.4902633\tbest: 0.4902633 (80)\ttotal: 1.93s\tremaining: 21.9s\n",
      "81:\tlearn: 0.4880341\ttest: 0.4902490\tbest: 0.4902490 (81)\ttotal: 1.95s\tremaining: 21.8s\n",
      "82:\tlearn: 0.4879738\ttest: 0.4902509\tbest: 0.4902490 (81)\ttotal: 1.97s\tremaining: 21.8s\n",
      "83:\tlearn: 0.4878970\ttest: 0.4902198\tbest: 0.4902198 (83)\ttotal: 2s\tremaining: 21.8s\n",
      "84:\tlearn: 0.4878291\ttest: 0.4901778\tbest: 0.4901778 (84)\ttotal: 2.02s\tremaining: 21.7s\n",
      "85:\tlearn: 0.4877592\ttest: 0.4901415\tbest: 0.4901415 (85)\ttotal: 2.04s\tremaining: 21.7s\n",
      "86:\tlearn: 0.4876767\ttest: 0.4900903\tbest: 0.4900903 (86)\ttotal: 2.06s\tremaining: 21.7s\n",
      "87:\tlearn: 0.4876218\ttest: 0.4900636\tbest: 0.4900636 (87)\ttotal: 2.09s\tremaining: 21.6s\n",
      "88:\tlearn: 0.4875234\ttest: 0.4900170\tbest: 0.4900170 (88)\ttotal: 2.11s\tremaining: 21.6s\n",
      "89:\tlearn: 0.4874733\ttest: 0.4900036\tbest: 0.4900036 (89)\ttotal: 2.13s\tremaining: 21.5s\n",
      "90:\tlearn: 0.4873639\ttest: 0.4899852\tbest: 0.4899852 (90)\ttotal: 2.15s\tremaining: 21.5s\n",
      "91:\tlearn: 0.4873055\ttest: 0.4899565\tbest: 0.4899565 (91)\ttotal: 2.17s\tremaining: 21.5s\n",
      "92:\tlearn: 0.4872408\ttest: 0.4899458\tbest: 0.4899458 (92)\ttotal: 2.2s\tremaining: 21.4s\n",
      "93:\tlearn: 0.4871586\ttest: 0.4899243\tbest: 0.4899243 (93)\ttotal: 2.21s\tremaining: 21.4s\n",
      "94:\tlearn: 0.4870888\ttest: 0.4898986\tbest: 0.4898986 (94)\ttotal: 2.23s\tremaining: 21.3s\n",
      "95:\tlearn: 0.4870363\ttest: 0.4898941\tbest: 0.4898941 (95)\ttotal: 2.25s\tremaining: 21.2s\n",
      "96:\tlearn: 0.4869536\ttest: 0.4898614\tbest: 0.4898614 (96)\ttotal: 2.28s\tremaining: 21.2s\n",
      "97:\tlearn: 0.4868553\ttest: 0.4897779\tbest: 0.4897779 (97)\ttotal: 2.3s\tremaining: 21.2s\n",
      "98:\tlearn: 0.4867963\ttest: 0.4897326\tbest: 0.4897326 (98)\ttotal: 2.32s\tremaining: 21.1s\n",
      "99:\tlearn: 0.4867174\ttest: 0.4897163\tbest: 0.4897163 (99)\ttotal: 2.35s\tremaining: 21.1s\n",
      "100:\tlearn: 0.4866381\ttest: 0.4896771\tbest: 0.4896771 (100)\ttotal: 2.37s\tremaining: 21.1s\n",
      "101:\tlearn: 0.4865885\ttest: 0.4896717\tbest: 0.4896717 (101)\ttotal: 2.39s\tremaining: 21s\n",
      "102:\tlearn: 0.4865193\ttest: 0.4896608\tbest: 0.4896608 (102)\ttotal: 2.41s\tremaining: 21s\n",
      "103:\tlearn: 0.4864583\ttest: 0.4896401\tbest: 0.4896401 (103)\ttotal: 2.43s\tremaining: 21s\n",
      "104:\tlearn: 0.4863873\ttest: 0.4896180\tbest: 0.4896180 (104)\ttotal: 2.45s\tremaining: 20.9s\n",
      "105:\tlearn: 0.4863104\ttest: 0.4895981\tbest: 0.4895981 (105)\ttotal: 2.47s\tremaining: 20.9s\n",
      "106:\tlearn: 0.4862322\ttest: 0.4895591\tbest: 0.4895591 (106)\ttotal: 2.49s\tremaining: 20.8s\n",
      "107:\tlearn: 0.4861717\ttest: 0.4895277\tbest: 0.4895277 (107)\ttotal: 2.51s\tremaining: 20.8s\n",
      "108:\tlearn: 0.4861009\ttest: 0.4894754\tbest: 0.4894754 (108)\ttotal: 2.53s\tremaining: 20.7s\n",
      "109:\tlearn: 0.4860220\ttest: 0.4894689\tbest: 0.4894689 (109)\ttotal: 2.55s\tremaining: 20.6s\n",
      "110:\tlearn: 0.4859387\ttest: 0.4894720\tbest: 0.4894689 (109)\ttotal: 2.57s\tremaining: 20.6s\n",
      "111:\tlearn: 0.4858464\ttest: 0.4894661\tbest: 0.4894661 (111)\ttotal: 2.59s\tremaining: 20.5s\n",
      "112:\tlearn: 0.4857690\ttest: 0.4894373\tbest: 0.4894373 (112)\ttotal: 2.61s\tremaining: 20.5s\n",
      "113:\tlearn: 0.4856899\ttest: 0.4894039\tbest: 0.4894039 (113)\ttotal: 2.63s\tremaining: 20.4s\n",
      "114:\tlearn: 0.4855834\ttest: 0.4893663\tbest: 0.4893663 (114)\ttotal: 2.65s\tremaining: 20.4s\n",
      "115:\tlearn: 0.4854999\ttest: 0.4893570\tbest: 0.4893570 (115)\ttotal: 2.67s\tremaining: 20.3s\n",
      "116:\tlearn: 0.4853966\ttest: 0.4893199\tbest: 0.4893199 (116)\ttotal: 2.69s\tremaining: 20.3s\n",
      "117:\tlearn: 0.4852952\ttest: 0.4892721\tbest: 0.4892721 (117)\ttotal: 2.71s\tremaining: 20.3s\n",
      "118:\tlearn: 0.4852127\ttest: 0.4892625\tbest: 0.4892625 (118)\ttotal: 2.73s\tremaining: 20.2s\n",
      "119:\tlearn: 0.4851196\ttest: 0.4892458\tbest: 0.4892458 (119)\ttotal: 2.75s\tremaining: 20.2s\n",
      "120:\tlearn: 0.4850387\ttest: 0.4892328\tbest: 0.4892328 (120)\ttotal: 2.77s\tremaining: 20.1s\n",
      "121:\tlearn: 0.4849536\ttest: 0.4891759\tbest: 0.4891759 (121)\ttotal: 2.79s\tremaining: 20.1s\n",
      "122:\tlearn: 0.4848539\ttest: 0.4891708\tbest: 0.4891708 (122)\ttotal: 2.81s\tremaining: 20.1s\n",
      "123:\tlearn: 0.4847938\ttest: 0.4891579\tbest: 0.4891579 (123)\ttotal: 2.83s\tremaining: 20s\n",
      "124:\tlearn: 0.4847182\ttest: 0.4891472\tbest: 0.4891472 (124)\ttotal: 2.85s\tremaining: 20s\n",
      "125:\tlearn: 0.4846482\ttest: 0.4891233\tbest: 0.4891233 (125)\ttotal: 2.87s\tremaining: 19.9s\n",
      "126:\tlearn: 0.4845591\ttest: 0.4891323\tbest: 0.4891233 (125)\ttotal: 2.89s\tremaining: 19.9s\n",
      "127:\tlearn: 0.4844624\ttest: 0.4891293\tbest: 0.4891233 (125)\ttotal: 2.91s\tremaining: 19.8s\n",
      "128:\tlearn: 0.4843945\ttest: 0.4891356\tbest: 0.4891233 (125)\ttotal: 2.93s\tremaining: 19.8s\n",
      "129:\tlearn: 0.4842974\ttest: 0.4891219\tbest: 0.4891219 (129)\ttotal: 2.95s\tremaining: 19.8s\n",
      "130:\tlearn: 0.4842335\ttest: 0.4891133\tbest: 0.4891133 (130)\ttotal: 2.97s\tremaining: 19.7s\n",
      "131:\tlearn: 0.4841673\ttest: 0.4891026\tbest: 0.4891026 (131)\ttotal: 2.99s\tremaining: 19.7s\n",
      "132:\tlearn: 0.4840733\ttest: 0.4890906\tbest: 0.4890906 (132)\ttotal: 3.01s\tremaining: 19.6s\n",
      "133:\tlearn: 0.4840086\ttest: 0.4890946\tbest: 0.4890906 (132)\ttotal: 3.03s\tremaining: 19.6s\n",
      "134:\tlearn: 0.4839011\ttest: 0.4890842\tbest: 0.4890842 (134)\ttotal: 3.05s\tremaining: 19.6s\n",
      "135:\tlearn: 0.4838044\ttest: 0.4890746\tbest: 0.4890746 (135)\ttotal: 3.07s\tremaining: 19.5s\n",
      "136:\tlearn: 0.4837325\ttest: 0.4890843\tbest: 0.4890746 (135)\ttotal: 3.09s\tremaining: 19.5s\n",
      "137:\tlearn: 0.4836432\ttest: 0.4890702\tbest: 0.4890702 (137)\ttotal: 3.11s\tremaining: 19.4s\n",
      "138:\tlearn: 0.4835542\ttest: 0.4890572\tbest: 0.4890572 (138)\ttotal: 3.13s\tremaining: 19.4s\n",
      "139:\tlearn: 0.4834573\ttest: 0.4890441\tbest: 0.4890441 (139)\ttotal: 3.15s\tremaining: 19.4s\n",
      "140:\tlearn: 0.4833701\ttest: 0.4890508\tbest: 0.4890441 (139)\ttotal: 3.17s\tremaining: 19.3s\n",
      "141:\tlearn: 0.4832839\ttest: 0.4890482\tbest: 0.4890441 (139)\ttotal: 3.19s\tremaining: 19.3s\n",
      "142:\tlearn: 0.4832017\ttest: 0.4890493\tbest: 0.4890441 (139)\ttotal: 3.21s\tremaining: 19.3s\n",
      "143:\tlearn: 0.4831106\ttest: 0.4890244\tbest: 0.4890244 (143)\ttotal: 3.23s\tremaining: 19.2s\n",
      "144:\tlearn: 0.4830225\ttest: 0.4889983\tbest: 0.4889983 (144)\ttotal: 3.25s\tremaining: 19.2s\n",
      "145:\tlearn: 0.4829247\ttest: 0.4889526\tbest: 0.4889526 (145)\ttotal: 3.27s\tremaining: 19.1s\n",
      "146:\tlearn: 0.4828427\ttest: 0.4889177\tbest: 0.4889177 (146)\ttotal: 3.29s\tremaining: 19.1s\n",
      "147:\tlearn: 0.4827565\ttest: 0.4888863\tbest: 0.4888863 (147)\ttotal: 3.31s\tremaining: 19s\n",
      "148:\tlearn: 0.4826949\ttest: 0.4888815\tbest: 0.4888815 (148)\ttotal: 3.33s\tremaining: 19s\n",
      "149:\tlearn: 0.4826152\ttest: 0.4888367\tbest: 0.4888367 (149)\ttotal: 3.35s\tremaining: 19s\n",
      "150:\tlearn: 0.4825221\ttest: 0.4888152\tbest: 0.4888152 (150)\ttotal: 3.37s\tremaining: 18.9s\n",
      "151:\tlearn: 0.4824309\ttest: 0.4888117\tbest: 0.4888117 (151)\ttotal: 3.39s\tremaining: 18.9s\n",
      "152:\tlearn: 0.4823492\ttest: 0.4887750\tbest: 0.4887750 (152)\ttotal: 3.41s\tremaining: 18.9s\n",
      "153:\tlearn: 0.4822681\ttest: 0.4887685\tbest: 0.4887685 (153)\ttotal: 3.43s\tremaining: 18.8s\n",
      "154:\tlearn: 0.4821889\ttest: 0.4887587\tbest: 0.4887587 (154)\ttotal: 3.45s\tremaining: 18.8s\n",
      "155:\tlearn: 0.4821043\ttest: 0.4887371\tbest: 0.4887371 (155)\ttotal: 3.47s\tremaining: 18.8s\n",
      "156:\tlearn: 0.4820254\ttest: 0.4887239\tbest: 0.4887239 (156)\ttotal: 3.49s\tremaining: 18.8s\n",
      "157:\tlearn: 0.4819437\ttest: 0.4887250\tbest: 0.4887239 (156)\ttotal: 3.51s\tremaining: 18.7s\n",
      "158:\tlearn: 0.4818515\ttest: 0.4887087\tbest: 0.4887087 (158)\ttotal: 3.53s\tremaining: 18.7s\n",
      "159:\tlearn: 0.4817621\ttest: 0.4886863\tbest: 0.4886863 (159)\ttotal: 3.55s\tremaining: 18.7s\n",
      "160:\tlearn: 0.4816561\ttest: 0.4886337\tbest: 0.4886337 (160)\ttotal: 3.58s\tremaining: 18.6s\n",
      "161:\tlearn: 0.4815592\ttest: 0.4886172\tbest: 0.4886172 (161)\ttotal: 3.6s\tremaining: 18.6s\n",
      "162:\tlearn: 0.4814564\ttest: 0.4885572\tbest: 0.4885572 (162)\ttotal: 3.62s\tremaining: 18.6s\n",
      "163:\tlearn: 0.4813811\ttest: 0.4885309\tbest: 0.4885309 (163)\ttotal: 3.64s\tremaining: 18.6s\n",
      "164:\tlearn: 0.4813172\ttest: 0.4885317\tbest: 0.4885309 (163)\ttotal: 3.66s\tremaining: 18.5s\n",
      "165:\tlearn: 0.4812477\ttest: 0.4885049\tbest: 0.4885049 (165)\ttotal: 3.68s\tremaining: 18.5s\n",
      "166:\tlearn: 0.4811741\ttest: 0.4884838\tbest: 0.4884838 (166)\ttotal: 3.7s\tremaining: 18.4s\n",
      "167:\tlearn: 0.4811009\ttest: 0.4884742\tbest: 0.4884742 (167)\ttotal: 3.72s\tremaining: 18.4s\n",
      "168:\tlearn: 0.4810377\ttest: 0.4884812\tbest: 0.4884742 (167)\ttotal: 3.74s\tremaining: 18.4s\n",
      "169:\tlearn: 0.4809608\ttest: 0.4884647\tbest: 0.4884647 (169)\ttotal: 3.75s\tremaining: 18.3s\n",
      "170:\tlearn: 0.4808621\ttest: 0.4884410\tbest: 0.4884410 (170)\ttotal: 3.78s\tremaining: 18.3s\n",
      "171:\tlearn: 0.4807865\ttest: 0.4884308\tbest: 0.4884308 (171)\ttotal: 3.79s\tremaining: 18.3s\n",
      "172:\tlearn: 0.4807209\ttest: 0.4884291\tbest: 0.4884291 (172)\ttotal: 3.81s\tremaining: 18.2s\n",
      "173:\tlearn: 0.4806285\ttest: 0.4884176\tbest: 0.4884176 (173)\ttotal: 3.84s\tremaining: 18.2s\n",
      "174:\tlearn: 0.4805494\ttest: 0.4883970\tbest: 0.4883970 (174)\ttotal: 3.87s\tremaining: 18.2s\n",
      "175:\tlearn: 0.4804651\ttest: 0.4883825\tbest: 0.4883825 (175)\ttotal: 3.89s\tremaining: 18.2s\n",
      "176:\tlearn: 0.4803944\ttest: 0.4883931\tbest: 0.4883825 (175)\ttotal: 3.9s\tremaining: 18.2s\n",
      "177:\tlearn: 0.4803162\ttest: 0.4884036\tbest: 0.4883825 (175)\ttotal: 3.92s\tremaining: 18.1s\n",
      "178:\tlearn: 0.4802451\ttest: 0.4884260\tbest: 0.4883825 (175)\ttotal: 3.94s\tremaining: 18.1s\n",
      "179:\tlearn: 0.4801794\ttest: 0.4884030\tbest: 0.4883825 (175)\ttotal: 3.96s\tremaining: 18.1s\n",
      "180:\tlearn: 0.4800953\ttest: 0.4883822\tbest: 0.4883822 (180)\ttotal: 3.98s\tremaining: 18s\n",
      "181:\tlearn: 0.4800149\ttest: 0.4883593\tbest: 0.4883593 (181)\ttotal: 4s\tremaining: 18s\n",
      "182:\tlearn: 0.4799353\ttest: 0.4883528\tbest: 0.4883528 (182)\ttotal: 4.02s\tremaining: 18s\n",
      "183:\tlearn: 0.4798604\ttest: 0.4883341\tbest: 0.4883341 (183)\ttotal: 4.04s\tremaining: 17.9s\n",
      "184:\tlearn: 0.4797876\ttest: 0.4883083\tbest: 0.4883083 (184)\ttotal: 4.06s\tremaining: 17.9s\n",
      "185:\tlearn: 0.4796868\ttest: 0.4882912\tbest: 0.4882912 (185)\ttotal: 4.08s\tremaining: 17.9s\n",
      "186:\tlearn: 0.4795942\ttest: 0.4883118\tbest: 0.4882912 (185)\ttotal: 4.11s\tremaining: 17.8s\n",
      "187:\tlearn: 0.4795123\ttest: 0.4883092\tbest: 0.4882912 (185)\ttotal: 4.12s\tremaining: 17.8s\n",
      "188:\tlearn: 0.4794270\ttest: 0.4883013\tbest: 0.4882912 (185)\ttotal: 4.14s\tremaining: 17.8s\n",
      "189:\tlearn: 0.4793402\ttest: 0.4882943\tbest: 0.4882912 (185)\ttotal: 4.16s\tremaining: 17.7s\n",
      "190:\tlearn: 0.4792844\ttest: 0.4882888\tbest: 0.4882888 (190)\ttotal: 4.18s\tremaining: 17.7s\n",
      "191:\tlearn: 0.4792094\ttest: 0.4883110\tbest: 0.4882888 (190)\ttotal: 4.2s\tremaining: 17.7s\n",
      "192:\tlearn: 0.4791350\ttest: 0.4883147\tbest: 0.4882888 (190)\ttotal: 4.22s\tremaining: 17.7s\n",
      "193:\tlearn: 0.4790804\ttest: 0.4883158\tbest: 0.4882888 (190)\ttotal: 4.24s\tremaining: 17.6s\n",
      "194:\tlearn: 0.4790178\ttest: 0.4883131\tbest: 0.4882888 (190)\ttotal: 4.26s\tremaining: 17.6s\n",
      "195:\tlearn: 0.4789035\ttest: 0.4882947\tbest: 0.4882888 (190)\ttotal: 4.28s\tremaining: 17.5s\n",
      "196:\tlearn: 0.4788355\ttest: 0.4882847\tbest: 0.4882847 (196)\ttotal: 4.3s\tremaining: 17.5s\n",
      "197:\tlearn: 0.4787793\ttest: 0.4882925\tbest: 0.4882847 (196)\ttotal: 4.32s\tremaining: 17.5s\n",
      "198:\tlearn: 0.4787037\ttest: 0.4882603\tbest: 0.4882603 (198)\ttotal: 4.34s\tremaining: 17.5s\n",
      "199:\tlearn: 0.4786147\ttest: 0.4882447\tbest: 0.4882447 (199)\ttotal: 4.36s\tremaining: 17.4s\n",
      "200:\tlearn: 0.4785548\ttest: 0.4882485\tbest: 0.4882447 (199)\ttotal: 4.38s\tremaining: 17.4s\n",
      "201:\tlearn: 0.4784901\ttest: 0.4882423\tbest: 0.4882423 (201)\ttotal: 4.4s\tremaining: 17.4s\n",
      "202:\tlearn: 0.4783995\ttest: 0.4882287\tbest: 0.4882287 (202)\ttotal: 4.42s\tremaining: 17.3s\n",
      "203:\tlearn: 0.4783302\ttest: 0.4882085\tbest: 0.4882085 (203)\ttotal: 4.44s\tremaining: 17.3s\n",
      "204:\tlearn: 0.4782518\ttest: 0.4881983\tbest: 0.4881983 (204)\ttotal: 4.46s\tremaining: 17.3s\n",
      "205:\tlearn: 0.4781640\ttest: 0.4881960\tbest: 0.4881960 (205)\ttotal: 4.48s\tremaining: 17.3s\n",
      "206:\tlearn: 0.4781007\ttest: 0.4881695\tbest: 0.4881695 (206)\ttotal: 4.5s\tremaining: 17.3s\n",
      "207:\tlearn: 0.4780129\ttest: 0.4881456\tbest: 0.4881456 (207)\ttotal: 4.52s\tremaining: 17.2s\n",
      "208:\tlearn: 0.4779330\ttest: 0.4881600\tbest: 0.4881456 (207)\ttotal: 4.54s\tremaining: 17.2s\n",
      "209:\tlearn: 0.4778505\ttest: 0.4881617\tbest: 0.4881456 (207)\ttotal: 4.57s\tremaining: 17.2s\n",
      "210:\tlearn: 0.4777693\ttest: 0.4881573\tbest: 0.4881456 (207)\ttotal: 4.59s\tremaining: 17.2s\n",
      "211:\tlearn: 0.4776693\ttest: 0.4881499\tbest: 0.4881456 (207)\ttotal: 4.61s\tremaining: 17.1s\n",
      "212:\tlearn: 0.4775900\ttest: 0.4881573\tbest: 0.4881456 (207)\ttotal: 4.63s\tremaining: 17.1s\n",
      "213:\tlearn: 0.4775208\ttest: 0.4881625\tbest: 0.4881456 (207)\ttotal: 4.65s\tremaining: 17.1s\n",
      "214:\tlearn: 0.4774436\ttest: 0.4881210\tbest: 0.4881210 (214)\ttotal: 4.67s\tremaining: 17.1s\n",
      "215:\tlearn: 0.4773550\ttest: 0.4880676\tbest: 0.4880676 (215)\ttotal: 4.7s\tremaining: 17s\n",
      "216:\tlearn: 0.4772758\ttest: 0.4880646\tbest: 0.4880646 (216)\ttotal: 4.71s\tremaining: 17s\n",
      "217:\tlearn: 0.4771918\ttest: 0.4880349\tbest: 0.4880349 (217)\ttotal: 4.73s\tremaining: 17s\n",
      "218:\tlearn: 0.4771164\ttest: 0.4880008\tbest: 0.4880008 (218)\ttotal: 4.75s\tremaining: 16.9s\n",
      "219:\tlearn: 0.4770799\ttest: 0.4880007\tbest: 0.4880007 (219)\ttotal: 4.77s\tremaining: 16.9s\n",
      "220:\tlearn: 0.4769982\ttest: 0.4879755\tbest: 0.4879755 (220)\ttotal: 4.79s\tremaining: 16.9s\n",
      "221:\tlearn: 0.4769162\ttest: 0.4879716\tbest: 0.4879716 (221)\ttotal: 4.81s\tremaining: 16.9s\n",
      "222:\tlearn: 0.4768501\ttest: 0.4879826\tbest: 0.4879716 (221)\ttotal: 4.83s\tremaining: 16.8s\n",
      "223:\tlearn: 0.4768051\ttest: 0.4879979\tbest: 0.4879716 (221)\ttotal: 4.85s\tremaining: 16.8s\n",
      "224:\tlearn: 0.4767124\ttest: 0.4879941\tbest: 0.4879716 (221)\ttotal: 4.87s\tremaining: 16.8s\n",
      "225:\tlearn: 0.4766608\ttest: 0.4879599\tbest: 0.4879599 (225)\ttotal: 4.89s\tremaining: 16.7s\n",
      "226:\tlearn: 0.4765862\ttest: 0.4879385\tbest: 0.4879385 (226)\ttotal: 4.91s\tremaining: 16.7s\n",
      "227:\tlearn: 0.4765016\ttest: 0.4879214\tbest: 0.4879214 (227)\ttotal: 4.93s\tremaining: 16.7s\n",
      "228:\tlearn: 0.4764336\ttest: 0.4879383\tbest: 0.4879214 (227)\ttotal: 4.95s\tremaining: 16.7s\n",
      "229:\tlearn: 0.4763542\ttest: 0.4879124\tbest: 0.4879124 (229)\ttotal: 4.97s\tremaining: 16.6s\n",
      "230:\tlearn: 0.4762819\ttest: 0.4878942\tbest: 0.4878942 (230)\ttotal: 4.99s\tremaining: 16.6s\n",
      "231:\tlearn: 0.4762028\ttest: 0.4878875\tbest: 0.4878875 (231)\ttotal: 5.01s\tremaining: 16.6s\n",
      "232:\tlearn: 0.4761448\ttest: 0.4878812\tbest: 0.4878812 (232)\ttotal: 5.03s\tremaining: 16.6s\n",
      "233:\tlearn: 0.4760627\ttest: 0.4878343\tbest: 0.4878343 (233)\ttotal: 5.05s\tremaining: 16.5s\n",
      "234:\tlearn: 0.4759865\ttest: 0.4878194\tbest: 0.4878194 (234)\ttotal: 5.07s\tremaining: 16.5s\n",
      "235:\tlearn: 0.4759023\ttest: 0.4878237\tbest: 0.4878194 (234)\ttotal: 5.09s\tremaining: 16.5s\n",
      "236:\tlearn: 0.4758318\ttest: 0.4877772\tbest: 0.4877772 (236)\ttotal: 5.12s\tremaining: 16.5s\n",
      "237:\tlearn: 0.4757443\ttest: 0.4877598\tbest: 0.4877598 (237)\ttotal: 5.14s\tremaining: 16.5s\n",
      "238:\tlearn: 0.4756715\ttest: 0.4877513\tbest: 0.4877513 (238)\ttotal: 5.18s\tremaining: 16.5s\n",
      "239:\tlearn: 0.4755924\ttest: 0.4877558\tbest: 0.4877513 (238)\ttotal: 5.21s\tremaining: 16.5s\n",
      "240:\tlearn: 0.4755246\ttest: 0.4877448\tbest: 0.4877448 (240)\ttotal: 5.23s\tremaining: 16.5s\n",
      "241:\tlearn: 0.4754441\ttest: 0.4877774\tbest: 0.4877448 (240)\ttotal: 5.26s\tremaining: 16.5s\n",
      "242:\tlearn: 0.4753650\ttest: 0.4877629\tbest: 0.4877448 (240)\ttotal: 5.28s\tremaining: 16.5s\n",
      "243:\tlearn: 0.4752858\ttest: 0.4877811\tbest: 0.4877448 (240)\ttotal: 5.3s\tremaining: 16.4s\n",
      "244:\tlearn: 0.4752086\ttest: 0.4877827\tbest: 0.4877448 (240)\ttotal: 5.33s\tremaining: 16.4s\n",
      "245:\tlearn: 0.4751374\ttest: 0.4877701\tbest: 0.4877448 (240)\ttotal: 5.35s\tremaining: 16.4s\n",
      "246:\tlearn: 0.4750722\ttest: 0.4877701\tbest: 0.4877448 (240)\ttotal: 5.37s\tremaining: 16.4s\n",
      "247:\tlearn: 0.4749905\ttest: 0.4877741\tbest: 0.4877448 (240)\ttotal: 5.4s\tremaining: 16.4s\n",
      "248:\tlearn: 0.4749094\ttest: 0.4877652\tbest: 0.4877448 (240)\ttotal: 5.42s\tremaining: 16.3s\n",
      "249:\tlearn: 0.4748168\ttest: 0.4877286\tbest: 0.4877286 (249)\ttotal: 5.44s\tremaining: 16.3s\n",
      "250:\tlearn: 0.4747304\ttest: 0.4876930\tbest: 0.4876930 (250)\ttotal: 5.46s\tremaining: 16.3s\n",
      "251:\tlearn: 0.4746552\ttest: 0.4876875\tbest: 0.4876875 (251)\ttotal: 5.49s\tremaining: 16.3s\n",
      "252:\tlearn: 0.4745696\ttest: 0.4876706\tbest: 0.4876706 (252)\ttotal: 5.51s\tremaining: 16.3s\n",
      "253:\tlearn: 0.4744902\ttest: 0.4876685\tbest: 0.4876685 (253)\ttotal: 5.53s\tremaining: 16.2s\n",
      "254:\tlearn: 0.4744447\ttest: 0.4876620\tbest: 0.4876620 (254)\ttotal: 5.55s\tremaining: 16.2s\n",
      "255:\tlearn: 0.4743767\ttest: 0.4876615\tbest: 0.4876615 (255)\ttotal: 5.57s\tremaining: 16.2s\n",
      "256:\tlearn: 0.4743283\ttest: 0.4876540\tbest: 0.4876540 (256)\ttotal: 5.59s\tremaining: 16.2s\n",
      "257:\tlearn: 0.4742380\ttest: 0.4876307\tbest: 0.4876307 (257)\ttotal: 5.61s\tremaining: 16.1s\n",
      "258:\tlearn: 0.4741770\ttest: 0.4876294\tbest: 0.4876294 (258)\ttotal: 5.63s\tremaining: 16.1s\n",
      "259:\tlearn: 0.4741202\ttest: 0.4876152\tbest: 0.4876152 (259)\ttotal: 5.65s\tremaining: 16.1s\n",
      "260:\tlearn: 0.4740589\ttest: 0.4876366\tbest: 0.4876152 (259)\ttotal: 5.67s\tremaining: 16.1s\n",
      "261:\tlearn: 0.4739812\ttest: 0.4876310\tbest: 0.4876152 (259)\ttotal: 5.69s\tremaining: 16s\n",
      "262:\tlearn: 0.4739106\ttest: 0.4876013\tbest: 0.4876013 (262)\ttotal: 5.71s\tremaining: 16s\n",
      "263:\tlearn: 0.4738510\ttest: 0.4876227\tbest: 0.4876013 (262)\ttotal: 5.73s\tremaining: 16s\n",
      "264:\tlearn: 0.4737866\ttest: 0.4876340\tbest: 0.4876013 (262)\ttotal: 5.75s\tremaining: 15.9s\n",
      "265:\tlearn: 0.4737094\ttest: 0.4876299\tbest: 0.4876013 (262)\ttotal: 5.77s\tremaining: 15.9s\n",
      "266:\tlearn: 0.4736413\ttest: 0.4876493\tbest: 0.4876013 (262)\ttotal: 5.79s\tremaining: 15.9s\n",
      "267:\tlearn: 0.4735503\ttest: 0.4876105\tbest: 0.4876013 (262)\ttotal: 5.82s\tremaining: 15.9s\n",
      "268:\tlearn: 0.4734689\ttest: 0.4875990\tbest: 0.4875990 (268)\ttotal: 5.84s\tremaining: 15.9s\n",
      "269:\tlearn: 0.4733913\ttest: 0.4875859\tbest: 0.4875859 (269)\ttotal: 5.86s\tremaining: 15.8s\n",
      "270:\tlearn: 0.4733305\ttest: 0.4875793\tbest: 0.4875793 (270)\ttotal: 5.88s\tremaining: 15.8s\n",
      "271:\tlearn: 0.4732629\ttest: 0.4875601\tbest: 0.4875601 (271)\ttotal: 5.91s\tremaining: 15.8s\n",
      "272:\tlearn: 0.4731752\ttest: 0.4875303\tbest: 0.4875303 (272)\ttotal: 5.93s\tremaining: 15.8s\n",
      "273:\tlearn: 0.4731024\ttest: 0.4875160\tbest: 0.4875160 (273)\ttotal: 5.95s\tremaining: 15.8s\n",
      "274:\tlearn: 0.4730210\ttest: 0.4874782\tbest: 0.4874782 (274)\ttotal: 5.97s\tremaining: 15.7s\n",
      "275:\tlearn: 0.4729381\ttest: 0.4874349\tbest: 0.4874349 (275)\ttotal: 6s\tremaining: 15.7s\n",
      "276:\tlearn: 0.4728585\ttest: 0.4874190\tbest: 0.4874190 (276)\ttotal: 6.02s\tremaining: 15.7s\n",
      "277:\tlearn: 0.4727932\ttest: 0.4874164\tbest: 0.4874164 (277)\ttotal: 6.04s\tremaining: 15.7s\n",
      "278:\tlearn: 0.4727399\ttest: 0.4874031\tbest: 0.4874031 (278)\ttotal: 6.05s\tremaining: 15.6s\n",
      "279:\tlearn: 0.4726690\ttest: 0.4873955\tbest: 0.4873955 (279)\ttotal: 6.07s\tremaining: 15.6s\n",
      "280:\tlearn: 0.4726036\ttest: 0.4873768\tbest: 0.4873768 (280)\ttotal: 6.1s\tremaining: 15.6s\n",
      "281:\tlearn: 0.4725214\ttest: 0.4873550\tbest: 0.4873550 (281)\ttotal: 6.12s\tremaining: 15.6s\n",
      "282:\tlearn: 0.4724452\ttest: 0.4873444\tbest: 0.4873444 (282)\ttotal: 6.14s\tremaining: 15.6s\n",
      "283:\tlearn: 0.4723742\ttest: 0.4873264\tbest: 0.4873264 (283)\ttotal: 6.16s\tremaining: 15.5s\n",
      "284:\tlearn: 0.4723130\ttest: 0.4873391\tbest: 0.4873264 (283)\ttotal: 6.18s\tremaining: 15.5s\n",
      "285:\tlearn: 0.4722505\ttest: 0.4873320\tbest: 0.4873264 (283)\ttotal: 6.2s\tremaining: 15.5s\n",
      "286:\tlearn: 0.4721796\ttest: 0.4873181\tbest: 0.4873181 (286)\ttotal: 6.23s\tremaining: 15.5s\n",
      "287:\tlearn: 0.4720997\ttest: 0.4873116\tbest: 0.4873116 (287)\ttotal: 6.26s\tremaining: 15.5s\n",
      "288:\tlearn: 0.4720416\ttest: 0.4873338\tbest: 0.4873116 (287)\ttotal: 6.28s\tremaining: 15.4s\n",
      "289:\tlearn: 0.4719711\ttest: 0.4873435\tbest: 0.4873116 (287)\ttotal: 6.3s\tremaining: 15.4s\n",
      "290:\tlearn: 0.4719214\ttest: 0.4873399\tbest: 0.4873116 (287)\ttotal: 6.31s\tremaining: 15.4s\n",
      "291:\tlearn: 0.4718570\ttest: 0.4873334\tbest: 0.4873116 (287)\ttotal: 6.34s\tremaining: 15.4s\n",
      "292:\tlearn: 0.4718020\ttest: 0.4873193\tbest: 0.4873116 (287)\ttotal: 6.36s\tremaining: 15.3s\n",
      "293:\tlearn: 0.4717252\ttest: 0.4873122\tbest: 0.4873116 (287)\ttotal: 6.38s\tremaining: 15.3s\n",
      "294:\tlearn: 0.4716521\ttest: 0.4872957\tbest: 0.4872957 (294)\ttotal: 6.4s\tremaining: 15.3s\n",
      "295:\tlearn: 0.4715986\ttest: 0.4872957\tbest: 0.4872957 (294)\ttotal: 6.42s\tremaining: 15.3s\n",
      "296:\tlearn: 0.4715441\ttest: 0.4872971\tbest: 0.4872957 (294)\ttotal: 6.45s\tremaining: 15.3s\n",
      "297:\tlearn: 0.4714568\ttest: 0.4872927\tbest: 0.4872927 (297)\ttotal: 6.46s\tremaining: 15.2s\n",
      "298:\tlearn: 0.4713722\ttest: 0.4872755\tbest: 0.4872755 (298)\ttotal: 6.49s\tremaining: 15.2s\n",
      "299:\tlearn: 0.4713061\ttest: 0.4872513\tbest: 0.4872513 (299)\ttotal: 6.51s\tremaining: 15.2s\n",
      "300:\tlearn: 0.4712519\ttest: 0.4872515\tbest: 0.4872513 (299)\ttotal: 6.52s\tremaining: 15.2s\n",
      "301:\tlearn: 0.4711936\ttest: 0.4872398\tbest: 0.4872398 (301)\ttotal: 6.54s\tremaining: 15.1s\n",
      "302:\tlearn: 0.4711151\ttest: 0.4872337\tbest: 0.4872337 (302)\ttotal: 6.56s\tremaining: 15.1s\n",
      "303:\tlearn: 0.4710499\ttest: 0.4872399\tbest: 0.4872337 (302)\ttotal: 6.59s\tremaining: 15.1s\n",
      "304:\tlearn: 0.4709807\ttest: 0.4872551\tbest: 0.4872337 (302)\ttotal: 6.61s\tremaining: 15.1s\n",
      "305:\tlearn: 0.4709238\ttest: 0.4872449\tbest: 0.4872337 (302)\ttotal: 6.63s\tremaining: 15s\n",
      "306:\tlearn: 0.4708318\ttest: 0.4872491\tbest: 0.4872337 (302)\ttotal: 6.65s\tremaining: 15s\n",
      "307:\tlearn: 0.4707695\ttest: 0.4872600\tbest: 0.4872337 (302)\ttotal: 6.66s\tremaining: 15s\n",
      "308:\tlearn: 0.4706856\ttest: 0.4872371\tbest: 0.4872337 (302)\ttotal: 6.68s\tremaining: 14.9s\n",
      "309:\tlearn: 0.4706166\ttest: 0.4872368\tbest: 0.4872337 (302)\ttotal: 6.7s\tremaining: 14.9s\n",
      "310:\tlearn: 0.4705421\ttest: 0.4872366\tbest: 0.4872337 (302)\ttotal: 6.72s\tremaining: 14.9s\n",
      "311:\tlearn: 0.4704570\ttest: 0.4872376\tbest: 0.4872337 (302)\ttotal: 6.74s\tremaining: 14.9s\n",
      "312:\tlearn: 0.4703885\ttest: 0.4872442\tbest: 0.4872337 (302)\ttotal: 6.76s\tremaining: 14.8s\n",
      "313:\tlearn: 0.4702964\ttest: 0.4872150\tbest: 0.4872150 (313)\ttotal: 6.78s\tremaining: 14.8s\n",
      "314:\tlearn: 0.4702164\ttest: 0.4872313\tbest: 0.4872150 (313)\ttotal: 6.8s\tremaining: 14.8s\n",
      "315:\tlearn: 0.4701605\ttest: 0.4872360\tbest: 0.4872150 (313)\ttotal: 6.83s\tremaining: 14.8s\n",
      "316:\tlearn: 0.4701002\ttest: 0.4872429\tbest: 0.4872150 (313)\ttotal: 6.85s\tremaining: 14.8s\n",
      "317:\tlearn: 0.4700483\ttest: 0.4872519\tbest: 0.4872150 (313)\ttotal: 6.87s\tremaining: 14.7s\n",
      "318:\tlearn: 0.4699687\ttest: 0.4872574\tbest: 0.4872150 (313)\ttotal: 6.89s\tremaining: 14.7s\n",
      "319:\tlearn: 0.4698967\ttest: 0.4872582\tbest: 0.4872150 (313)\ttotal: 6.92s\tremaining: 14.7s\n",
      "320:\tlearn: 0.4698322\ttest: 0.4872569\tbest: 0.4872150 (313)\ttotal: 6.94s\tremaining: 14.7s\n",
      "321:\tlearn: 0.4697598\ttest: 0.4872295\tbest: 0.4872150 (313)\ttotal: 6.96s\tremaining: 14.7s\n",
      "322:\tlearn: 0.4697014\ttest: 0.4872540\tbest: 0.4872150 (313)\ttotal: 6.99s\tremaining: 14.6s\n",
      "323:\tlearn: 0.4696183\ttest: 0.4872442\tbest: 0.4872150 (313)\ttotal: 7.01s\tremaining: 14.6s\n",
      "324:\tlearn: 0.4695647\ttest: 0.4872305\tbest: 0.4872150 (313)\ttotal: 7.03s\tremaining: 14.6s\n",
      "325:\tlearn: 0.4695084\ttest: 0.4872227\tbest: 0.4872150 (313)\ttotal: 7.05s\tremaining: 14.6s\n",
      "326:\tlearn: 0.4694413\ttest: 0.4872020\tbest: 0.4872020 (326)\ttotal: 7.08s\tremaining: 14.6s\n",
      "327:\tlearn: 0.4693721\ttest: 0.4871766\tbest: 0.4871766 (327)\ttotal: 7.1s\tremaining: 14.5s\n",
      "328:\tlearn: 0.4693224\ttest: 0.4871618\tbest: 0.4871618 (328)\ttotal: 7.12s\tremaining: 14.5s\n",
      "329:\tlearn: 0.4692524\ttest: 0.4871845\tbest: 0.4871618 (328)\ttotal: 7.14s\tremaining: 14.5s\n",
      "330:\tlearn: 0.4692191\ttest: 0.4871739\tbest: 0.4871618 (328)\ttotal: 7.16s\tremaining: 14.5s\n",
      "331:\tlearn: 0.4691609\ttest: 0.4871769\tbest: 0.4871618 (328)\ttotal: 7.19s\tremaining: 14.5s\n",
      "332:\tlearn: 0.4690940\ttest: 0.4871890\tbest: 0.4871618 (328)\ttotal: 7.21s\tremaining: 14.4s\n",
      "333:\tlearn: 0.4690329\ttest: 0.4872090\tbest: 0.4871618 (328)\ttotal: 7.23s\tremaining: 14.4s\n",
      "334:\tlearn: 0.4689514\ttest: 0.4872296\tbest: 0.4871618 (328)\ttotal: 7.25s\tremaining: 14.4s\n",
      "335:\tlearn: 0.4688798\ttest: 0.4872224\tbest: 0.4871618 (328)\ttotal: 7.27s\tremaining: 14.4s\n",
      "336:\tlearn: 0.4688082\ttest: 0.4872330\tbest: 0.4871618 (328)\ttotal: 7.29s\tremaining: 14.3s\n",
      "337:\tlearn: 0.4687443\ttest: 0.4872026\tbest: 0.4871618 (328)\ttotal: 7.31s\tremaining: 14.3s\n",
      "338:\tlearn: 0.4686741\ttest: 0.4872025\tbest: 0.4871618 (328)\ttotal: 7.33s\tremaining: 14.3s\n",
      "339:\tlearn: 0.4686251\ttest: 0.4872097\tbest: 0.4871618 (328)\ttotal: 7.35s\tremaining: 14.3s\n",
      "340:\tlearn: 0.4685704\ttest: 0.4871982\tbest: 0.4871618 (328)\ttotal: 7.37s\tremaining: 14.3s\n",
      "341:\tlearn: 0.4684908\ttest: 0.4871712\tbest: 0.4871618 (328)\ttotal: 7.39s\tremaining: 14.2s\n",
      "342:\tlearn: 0.4684361\ttest: 0.4871723\tbest: 0.4871618 (328)\ttotal: 7.41s\tremaining: 14.2s\n",
      "343:\tlearn: 0.4683651\ttest: 0.4871745\tbest: 0.4871618 (328)\ttotal: 7.43s\tremaining: 14.2s\n",
      "344:\tlearn: 0.4682848\ttest: 0.4871867\tbest: 0.4871618 (328)\ttotal: 7.46s\tremaining: 14.2s\n",
      "345:\tlearn: 0.4682021\ttest: 0.4871638\tbest: 0.4871618 (328)\ttotal: 7.48s\tremaining: 14.1s\n",
      "346:\tlearn: 0.4681121\ttest: 0.4871616\tbest: 0.4871616 (346)\ttotal: 7.5s\tremaining: 14.1s\n",
      "347:\tlearn: 0.4680620\ttest: 0.4871566\tbest: 0.4871566 (347)\ttotal: 7.52s\tremaining: 14.1s\n",
      "348:\tlearn: 0.4679928\ttest: 0.4871454\tbest: 0.4871454 (348)\ttotal: 7.54s\tremaining: 14.1s\n",
      "349:\tlearn: 0.4679233\ttest: 0.4871738\tbest: 0.4871454 (348)\ttotal: 7.56s\tremaining: 14s\n",
      "350:\tlearn: 0.4678346\ttest: 0.4871769\tbest: 0.4871454 (348)\ttotal: 7.59s\tremaining: 14s\n",
      "351:\tlearn: 0.4677546\ttest: 0.4871809\tbest: 0.4871454 (348)\ttotal: 7.61s\tremaining: 14s\n",
      "352:\tlearn: 0.4676933\ttest: 0.4871783\tbest: 0.4871454 (348)\ttotal: 7.63s\tremaining: 14s\n",
      "353:\tlearn: 0.4676502\ttest: 0.4871739\tbest: 0.4871454 (348)\ttotal: 7.65s\tremaining: 14s\n",
      "354:\tlearn: 0.4675834\ttest: 0.4871650\tbest: 0.4871454 (348)\ttotal: 7.67s\tremaining: 13.9s\n",
      "355:\tlearn: 0.4675208\ttest: 0.4871528\tbest: 0.4871454 (348)\ttotal: 7.69s\tremaining: 13.9s\n",
      "356:\tlearn: 0.4674525\ttest: 0.4871523\tbest: 0.4871454 (348)\ttotal: 7.71s\tremaining: 13.9s\n",
      "357:\tlearn: 0.4674218\ttest: 0.4871378\tbest: 0.4871378 (357)\ttotal: 7.73s\tremaining: 13.9s\n",
      "358:\tlearn: 0.4673399\ttest: 0.4871324\tbest: 0.4871324 (358)\ttotal: 7.75s\tremaining: 13.8s\n",
      "359:\tlearn: 0.4672777\ttest: 0.4871236\tbest: 0.4871236 (359)\ttotal: 7.77s\tremaining: 13.8s\n",
      "360:\tlearn: 0.4672360\ttest: 0.4871146\tbest: 0.4871146 (360)\ttotal: 7.79s\tremaining: 13.8s\n",
      "361:\tlearn: 0.4671634\ttest: 0.4871173\tbest: 0.4871146 (360)\ttotal: 7.81s\tremaining: 13.8s\n",
      "362:\tlearn: 0.4670960\ttest: 0.4871071\tbest: 0.4871071 (362)\ttotal: 7.83s\tremaining: 13.7s\n",
      "363:\tlearn: 0.4670317\ttest: 0.4870888\tbest: 0.4870888 (363)\ttotal: 7.85s\tremaining: 13.7s\n",
      "364:\tlearn: 0.4669329\ttest: 0.4870769\tbest: 0.4870769 (364)\ttotal: 7.87s\tremaining: 13.7s\n",
      "365:\tlearn: 0.4668503\ttest: 0.4870942\tbest: 0.4870769 (364)\ttotal: 7.89s\tremaining: 13.7s\n",
      "366:\tlearn: 0.4667745\ttest: 0.4870993\tbest: 0.4870769 (364)\ttotal: 7.92s\tremaining: 13.7s\n",
      "367:\tlearn: 0.4666988\ttest: 0.4871098\tbest: 0.4870769 (364)\ttotal: 7.94s\tremaining: 13.6s\n",
      "368:\tlearn: 0.4666307\ttest: 0.4871068\tbest: 0.4870769 (364)\ttotal: 7.96s\tremaining: 13.6s\n",
      "369:\tlearn: 0.4665828\ttest: 0.4871096\tbest: 0.4870769 (364)\ttotal: 7.98s\tremaining: 13.6s\n",
      "370:\tlearn: 0.4665323\ttest: 0.4871248\tbest: 0.4870769 (364)\ttotal: 8s\tremaining: 13.6s\n",
      "371:\tlearn: 0.4664721\ttest: 0.4871236\tbest: 0.4870769 (364)\ttotal: 8.02s\tremaining: 13.5s\n",
      "372:\tlearn: 0.4663967\ttest: 0.4871118\tbest: 0.4870769 (364)\ttotal: 8.04s\tremaining: 13.5s\n",
      "373:\tlearn: 0.4663275\ttest: 0.4871238\tbest: 0.4870769 (364)\ttotal: 8.06s\tremaining: 13.5s\n",
      "374:\tlearn: 0.4662582\ttest: 0.4871084\tbest: 0.4870769 (364)\ttotal: 8.08s\tremaining: 13.5s\n",
      "375:\tlearn: 0.4661942\ttest: 0.4871051\tbest: 0.4870769 (364)\ttotal: 8.11s\tremaining: 13.5s\n",
      "376:\tlearn: 0.4661398\ttest: 0.4870853\tbest: 0.4870769 (364)\ttotal: 8.12s\tremaining: 13.4s\n",
      "377:\tlearn: 0.4660854\ttest: 0.4870749\tbest: 0.4870749 (377)\ttotal: 8.14s\tremaining: 13.4s\n",
      "378:\tlearn: 0.4660370\ttest: 0.4870683\tbest: 0.4870683 (378)\ttotal: 8.16s\tremaining: 13.4s\n",
      "379:\tlearn: 0.4659586\ttest: 0.4870589\tbest: 0.4870589 (379)\ttotal: 8.18s\tremaining: 13.3s\n",
      "380:\tlearn: 0.4658766\ttest: 0.4870384\tbest: 0.4870384 (380)\ttotal: 8.2s\tremaining: 13.3s\n",
      "381:\tlearn: 0.4658157\ttest: 0.4870543\tbest: 0.4870384 (380)\ttotal: 8.22s\tremaining: 13.3s\n",
      "382:\tlearn: 0.4657526\ttest: 0.4870516\tbest: 0.4870384 (380)\ttotal: 8.24s\tremaining: 13.3s\n",
      "383:\tlearn: 0.4656772\ttest: 0.4870206\tbest: 0.4870206 (383)\ttotal: 8.26s\tremaining: 13.2s\n",
      "384:\tlearn: 0.4656207\ttest: 0.4870339\tbest: 0.4870206 (383)\ttotal: 8.28s\tremaining: 13.2s\n",
      "385:\tlearn: 0.4655548\ttest: 0.4870177\tbest: 0.4870177 (385)\ttotal: 8.3s\tremaining: 13.2s\n",
      "386:\tlearn: 0.4654832\ttest: 0.4870405\tbest: 0.4870177 (385)\ttotal: 8.32s\tremaining: 13.2s\n",
      "387:\tlearn: 0.4654203\ttest: 0.4870346\tbest: 0.4870177 (385)\ttotal: 8.34s\tremaining: 13.2s\n",
      "388:\tlearn: 0.4653699\ttest: 0.4870414\tbest: 0.4870177 (385)\ttotal: 8.36s\tremaining: 13.1s\n",
      "389:\tlearn: 0.4652927\ttest: 0.4870145\tbest: 0.4870145 (389)\ttotal: 8.38s\tremaining: 13.1s\n",
      "390:\tlearn: 0.4652116\ttest: 0.4870035\tbest: 0.4870035 (390)\ttotal: 8.4s\tremaining: 13.1s\n",
      "391:\tlearn: 0.4651719\ttest: 0.4870021\tbest: 0.4870021 (391)\ttotal: 8.42s\tremaining: 13.1s\n",
      "392:\tlearn: 0.4651117\ttest: 0.4869727\tbest: 0.4869727 (392)\ttotal: 8.44s\tremaining: 13s\n",
      "393:\tlearn: 0.4650461\ttest: 0.4869615\tbest: 0.4869615 (393)\ttotal: 8.46s\tremaining: 13s\n",
      "394:\tlearn: 0.4649784\ttest: 0.4869546\tbest: 0.4869546 (394)\ttotal: 8.48s\tremaining: 13s\n",
      "395:\tlearn: 0.4649009\ttest: 0.4869548\tbest: 0.4869546 (394)\ttotal: 8.5s\tremaining: 13s\n",
      "396:\tlearn: 0.4648488\ttest: 0.4869366\tbest: 0.4869366 (396)\ttotal: 8.52s\tremaining: 12.9s\n",
      "397:\tlearn: 0.4647810\ttest: 0.4869531\tbest: 0.4869366 (396)\ttotal: 8.53s\tremaining: 12.9s\n",
      "398:\tlearn: 0.4647417\ttest: 0.4869415\tbest: 0.4869366 (396)\ttotal: 8.55s\tremaining: 12.9s\n",
      "399:\tlearn: 0.4646876\ttest: 0.4869437\tbest: 0.4869366 (396)\ttotal: 8.57s\tremaining: 12.9s\n",
      "400:\tlearn: 0.4646302\ttest: 0.4869470\tbest: 0.4869366 (396)\ttotal: 8.58s\tremaining: 12.8s\n",
      "401:\tlearn: 0.4645637\ttest: 0.4869365\tbest: 0.4869365 (401)\ttotal: 8.6s\tremaining: 12.8s\n",
      "402:\tlearn: 0.4645041\ttest: 0.4869234\tbest: 0.4869234 (402)\ttotal: 8.62s\tremaining: 12.8s\n",
      "403:\tlearn: 0.4644360\ttest: 0.4869155\tbest: 0.4869155 (403)\ttotal: 8.64s\tremaining: 12.7s\n",
      "404:\tlearn: 0.4643802\ttest: 0.4869269\tbest: 0.4869155 (403)\ttotal: 8.66s\tremaining: 12.7s\n",
      "405:\tlearn: 0.4643184\ttest: 0.4869215\tbest: 0.4869155 (403)\ttotal: 8.68s\tremaining: 12.7s\n",
      "406:\tlearn: 0.4642603\ttest: 0.4869244\tbest: 0.4869155 (403)\ttotal: 8.7s\tremaining: 12.7s\n",
      "407:\tlearn: 0.4641905\ttest: 0.4869250\tbest: 0.4869155 (403)\ttotal: 8.72s\tremaining: 12.7s\n",
      "408:\tlearn: 0.4641134\ttest: 0.4869384\tbest: 0.4869155 (403)\ttotal: 8.74s\tremaining: 12.6s\n",
      "409:\tlearn: 0.4640388\ttest: 0.4869236\tbest: 0.4869155 (403)\ttotal: 8.76s\tremaining: 12.6s\n",
      "410:\tlearn: 0.4639856\ttest: 0.4869240\tbest: 0.4869155 (403)\ttotal: 8.78s\tremaining: 12.6s\n",
      "411:\tlearn: 0.4639126\ttest: 0.4868996\tbest: 0.4868996 (411)\ttotal: 8.8s\tremaining: 12.6s\n",
      "412:\tlearn: 0.4638538\ttest: 0.4868858\tbest: 0.4868858 (412)\ttotal: 8.82s\tremaining: 12.5s\n",
      "413:\tlearn: 0.4638100\ttest: 0.4868818\tbest: 0.4868818 (413)\ttotal: 8.84s\tremaining: 12.5s\n",
      "414:\tlearn: 0.4637366\ttest: 0.4868810\tbest: 0.4868810 (414)\ttotal: 8.86s\tremaining: 12.5s\n",
      "415:\tlearn: 0.4636858\ttest: 0.4868809\tbest: 0.4868809 (415)\ttotal: 8.88s\tremaining: 12.5s\n",
      "416:\tlearn: 0.4636160\ttest: 0.4868540\tbest: 0.4868540 (416)\ttotal: 8.91s\tremaining: 12.5s\n",
      "417:\tlearn: 0.4635416\ttest: 0.4868348\tbest: 0.4868348 (417)\ttotal: 8.93s\tremaining: 12.4s\n",
      "418:\tlearn: 0.4634721\ttest: 0.4868288\tbest: 0.4868288 (418)\ttotal: 8.94s\tremaining: 12.4s\n",
      "419:\tlearn: 0.4633966\ttest: 0.4868468\tbest: 0.4868288 (418)\ttotal: 8.96s\tremaining: 12.4s\n",
      "420:\tlearn: 0.4633191\ttest: 0.4868254\tbest: 0.4868254 (420)\ttotal: 8.98s\tremaining: 12.4s\n",
      "421:\tlearn: 0.4632444\ttest: 0.4868292\tbest: 0.4868254 (420)\ttotal: 9s\tremaining: 12.3s\n",
      "422:\tlearn: 0.4631981\ttest: 0.4868098\tbest: 0.4868098 (422)\ttotal: 9.02s\tremaining: 12.3s\n",
      "423:\tlearn: 0.4631554\ttest: 0.4868161\tbest: 0.4868098 (422)\ttotal: 9.04s\tremaining: 12.3s\n",
      "424:\tlearn: 0.4631055\ttest: 0.4868375\tbest: 0.4868098 (422)\ttotal: 9.06s\tremaining: 12.3s\n",
      "425:\tlearn: 0.4630667\ttest: 0.4868459\tbest: 0.4868098 (422)\ttotal: 9.08s\tremaining: 12.2s\n",
      "426:\tlearn: 0.4630033\ttest: 0.4868248\tbest: 0.4868098 (422)\ttotal: 9.11s\tremaining: 12.2s\n",
      "427:\tlearn: 0.4629386\ttest: 0.4868302\tbest: 0.4868098 (422)\ttotal: 9.13s\tremaining: 12.2s\n",
      "428:\tlearn: 0.4628772\ttest: 0.4868045\tbest: 0.4868045 (428)\ttotal: 9.15s\tremaining: 12.2s\n",
      "429:\tlearn: 0.4628098\ttest: 0.4868223\tbest: 0.4868045 (428)\ttotal: 9.17s\tremaining: 12.2s\n",
      "430:\tlearn: 0.4627526\ttest: 0.4868341\tbest: 0.4868045 (428)\ttotal: 9.19s\tremaining: 12.1s\n",
      "431:\tlearn: 0.4626957\ttest: 0.4868445\tbest: 0.4868045 (428)\ttotal: 9.21s\tremaining: 12.1s\n",
      "432:\tlearn: 0.4626485\ttest: 0.4868357\tbest: 0.4868045 (428)\ttotal: 9.23s\tremaining: 12.1s\n",
      "433:\tlearn: 0.4625967\ttest: 0.4868394\tbest: 0.4868045 (428)\ttotal: 9.24s\tremaining: 12.1s\n",
      "434:\tlearn: 0.4625414\ttest: 0.4868319\tbest: 0.4868045 (428)\ttotal: 9.26s\tremaining: 12s\n",
      "435:\tlearn: 0.4624846\ttest: 0.4868091\tbest: 0.4868045 (428)\ttotal: 9.28s\tremaining: 12s\n",
      "436:\tlearn: 0.4624425\ttest: 0.4868071\tbest: 0.4868045 (428)\ttotal: 9.3s\tremaining: 12s\n",
      "437:\tlearn: 0.4623757\ttest: 0.4867923\tbest: 0.4867923 (437)\ttotal: 9.32s\tremaining: 12s\n",
      "438:\tlearn: 0.4623195\ttest: 0.4867688\tbest: 0.4867688 (438)\ttotal: 9.34s\tremaining: 11.9s\n",
      "439:\tlearn: 0.4622584\ttest: 0.4867603\tbest: 0.4867603 (439)\ttotal: 9.37s\tremaining: 11.9s\n",
      "440:\tlearn: 0.4622053\ttest: 0.4867622\tbest: 0.4867603 (439)\ttotal: 9.38s\tremaining: 11.9s\n",
      "441:\tlearn: 0.4621389\ttest: 0.4867343\tbest: 0.4867343 (441)\ttotal: 9.4s\tremaining: 11.9s\n",
      "442:\tlearn: 0.4620841\ttest: 0.4867292\tbest: 0.4867292 (442)\ttotal: 9.42s\tremaining: 11.8s\n",
      "443:\tlearn: 0.4620216\ttest: 0.4867109\tbest: 0.4867109 (443)\ttotal: 9.44s\tremaining: 11.8s\n",
      "444:\tlearn: 0.4619627\ttest: 0.4867194\tbest: 0.4867109 (443)\ttotal: 9.46s\tremaining: 11.8s\n",
      "445:\tlearn: 0.4619160\ttest: 0.4867124\tbest: 0.4867109 (443)\ttotal: 9.48s\tremaining: 11.8s\n",
      "446:\tlearn: 0.4618442\ttest: 0.4866921\tbest: 0.4866921 (446)\ttotal: 9.5s\tremaining: 11.8s\n",
      "447:\tlearn: 0.4617799\ttest: 0.4866618\tbest: 0.4866618 (447)\ttotal: 9.52s\tremaining: 11.7s\n",
      "448:\tlearn: 0.4617352\ttest: 0.4866548\tbest: 0.4866548 (448)\ttotal: 9.54s\tremaining: 11.7s\n",
      "449:\tlearn: 0.4616596\ttest: 0.4866311\tbest: 0.4866311 (449)\ttotal: 9.56s\tremaining: 11.7s\n",
      "450:\tlearn: 0.4615908\ttest: 0.4866434\tbest: 0.4866311 (449)\ttotal: 9.58s\tremaining: 11.7s\n",
      "451:\tlearn: 0.4615469\ttest: 0.4866454\tbest: 0.4866311 (449)\ttotal: 9.6s\tremaining: 11.6s\n",
      "452:\tlearn: 0.4614983\ttest: 0.4866351\tbest: 0.4866311 (449)\ttotal: 9.62s\tremaining: 11.6s\n",
      "453:\tlearn: 0.4614532\ttest: 0.4866339\tbest: 0.4866311 (449)\ttotal: 9.64s\tremaining: 11.6s\n",
      "454:\tlearn: 0.4613923\ttest: 0.4866344\tbest: 0.4866311 (449)\ttotal: 9.66s\tremaining: 11.6s\n",
      "455:\tlearn: 0.4613235\ttest: 0.4866482\tbest: 0.4866311 (449)\ttotal: 9.68s\tremaining: 11.6s\n",
      "456:\tlearn: 0.4612688\ttest: 0.4866287\tbest: 0.4866287 (456)\ttotal: 9.71s\tremaining: 11.5s\n",
      "457:\tlearn: 0.4612147\ttest: 0.4866274\tbest: 0.4866274 (457)\ttotal: 9.72s\tremaining: 11.5s\n",
      "458:\tlearn: 0.4611559\ttest: 0.4866180\tbest: 0.4866180 (458)\ttotal: 9.75s\tremaining: 11.5s\n",
      "459:\tlearn: 0.4610897\ttest: 0.4866266\tbest: 0.4866180 (458)\ttotal: 9.77s\tremaining: 11.5s\n",
      "460:\tlearn: 0.4610278\ttest: 0.4866423\tbest: 0.4866180 (458)\ttotal: 9.79s\tremaining: 11.4s\n",
      "461:\tlearn: 0.4609602\ttest: 0.4866651\tbest: 0.4866180 (458)\ttotal: 9.81s\tremaining: 11.4s\n",
      "462:\tlearn: 0.4608896\ttest: 0.4866709\tbest: 0.4866180 (458)\ttotal: 9.83s\tremaining: 11.4s\n",
      "463:\tlearn: 0.4608311\ttest: 0.4866855\tbest: 0.4866180 (458)\ttotal: 9.86s\tremaining: 11.4s\n",
      "464:\tlearn: 0.4607793\ttest: 0.4866846\tbest: 0.4866180 (458)\ttotal: 9.88s\tremaining: 11.4s\n",
      "465:\tlearn: 0.4606998\ttest: 0.4866980\tbest: 0.4866180 (458)\ttotal: 9.9s\tremaining: 11.3s\n",
      "466:\tlearn: 0.4606187\ttest: 0.4867025\tbest: 0.4866180 (458)\ttotal: 9.93s\tremaining: 11.3s\n",
      "467:\tlearn: 0.4605647\ttest: 0.4867049\tbest: 0.4866180 (458)\ttotal: 9.95s\tremaining: 11.3s\n",
      "468:\tlearn: 0.4605161\ttest: 0.4866881\tbest: 0.4866180 (458)\ttotal: 9.97s\tremaining: 11.3s\n",
      "469:\tlearn: 0.4604683\ttest: 0.4866982\tbest: 0.4866180 (458)\ttotal: 9.99s\tremaining: 11.3s\n",
      "470:\tlearn: 0.4604142\ttest: 0.4866956\tbest: 0.4866180 (458)\ttotal: 10s\tremaining: 11.2s\n",
      "471:\tlearn: 0.4603416\ttest: 0.4866877\tbest: 0.4866180 (458)\ttotal: 10s\tremaining: 11.2s\n",
      "472:\tlearn: 0.4602710\ttest: 0.4866757\tbest: 0.4866180 (458)\ttotal: 10s\tremaining: 11.2s\n",
      "473:\tlearn: 0.4602136\ttest: 0.4866690\tbest: 0.4866180 (458)\ttotal: 10.1s\tremaining: 11.2s\n",
      "474:\tlearn: 0.4601620\ttest: 0.4866489\tbest: 0.4866180 (458)\ttotal: 10.1s\tremaining: 11.1s\n",
      "475:\tlearn: 0.4601176\ttest: 0.4866466\tbest: 0.4866180 (458)\ttotal: 10.1s\tremaining: 11.1s\n",
      "476:\tlearn: 0.4600544\ttest: 0.4866475\tbest: 0.4866180 (458)\ttotal: 10.1s\tremaining: 11.1s\n",
      "477:\tlearn: 0.4600091\ttest: 0.4866359\tbest: 0.4866180 (458)\ttotal: 10.1s\tremaining: 11.1s\n",
      "478:\tlearn: 0.4599569\ttest: 0.4866186\tbest: 0.4866180 (458)\ttotal: 10.2s\tremaining: 11s\n",
      "479:\tlearn: 0.4598947\ttest: 0.4865849\tbest: 0.4865849 (479)\ttotal: 10.2s\tremaining: 11s\n",
      "480:\tlearn: 0.4598418\ttest: 0.4865660\tbest: 0.4865660 (480)\ttotal: 10.2s\tremaining: 11s\n",
      "481:\tlearn: 0.4597844\ttest: 0.4865452\tbest: 0.4865452 (481)\ttotal: 10.2s\tremaining: 11s\n",
      "482:\tlearn: 0.4597173\ttest: 0.4865383\tbest: 0.4865383 (482)\ttotal: 10.2s\tremaining: 11s\n",
      "483:\tlearn: 0.4596524\ttest: 0.4865486\tbest: 0.4865383 (482)\ttotal: 10.3s\tremaining: 10.9s\n",
      "484:\tlearn: 0.4595783\ttest: 0.4865486\tbest: 0.4865383 (482)\ttotal: 10.3s\tremaining: 10.9s\n",
      "485:\tlearn: 0.4595227\ttest: 0.4865495\tbest: 0.4865383 (482)\ttotal: 10.3s\tremaining: 10.9s\n",
      "486:\tlearn: 0.4594671\ttest: 0.4865539\tbest: 0.4865383 (482)\ttotal: 10.3s\tremaining: 10.9s\n",
      "487:\tlearn: 0.4594065\ttest: 0.4865453\tbest: 0.4865383 (482)\ttotal: 10.3s\tremaining: 10.8s\n",
      "488:\tlearn: 0.4593353\ttest: 0.4865480\tbest: 0.4865383 (482)\ttotal: 10.4s\tremaining: 10.8s\n",
      "489:\tlearn: 0.4592872\ttest: 0.4865376\tbest: 0.4865376 (489)\ttotal: 10.4s\tremaining: 10.8s\n",
      "490:\tlearn: 0.4592163\ttest: 0.4865441\tbest: 0.4865376 (489)\ttotal: 10.4s\tremaining: 10.8s\n",
      "491:\tlearn: 0.4591589\ttest: 0.4865361\tbest: 0.4865361 (491)\ttotal: 10.4s\tremaining: 10.8s\n",
      "492:\tlearn: 0.4591050\ttest: 0.4865513\tbest: 0.4865361 (491)\ttotal: 10.4s\tremaining: 10.7s\n",
      "493:\tlearn: 0.4590414\ttest: 0.4865624\tbest: 0.4865361 (491)\ttotal: 10.5s\tremaining: 10.7s\n",
      "494:\tlearn: 0.4589737\ttest: 0.4865734\tbest: 0.4865361 (491)\ttotal: 10.5s\tremaining: 10.7s\n",
      "495:\tlearn: 0.4589188\ttest: 0.4865555\tbest: 0.4865361 (491)\ttotal: 10.5s\tremaining: 10.7s\n",
      "496:\tlearn: 0.4588561\ttest: 0.4865455\tbest: 0.4865361 (491)\ttotal: 10.5s\tremaining: 10.7s\n",
      "497:\tlearn: 0.4587908\ttest: 0.4865186\tbest: 0.4865186 (497)\ttotal: 10.5s\tremaining: 10.6s\n",
      "498:\tlearn: 0.4587300\ttest: 0.4865342\tbest: 0.4865186 (497)\ttotal: 10.6s\tremaining: 10.6s\n",
      "499:\tlearn: 0.4586799\ttest: 0.4865281\tbest: 0.4865186 (497)\ttotal: 10.6s\tremaining: 10.6s\n",
      "500:\tlearn: 0.4586213\ttest: 0.4865442\tbest: 0.4865186 (497)\ttotal: 10.6s\tremaining: 10.6s\n",
      "501:\tlearn: 0.4585573\ttest: 0.4865320\tbest: 0.4865186 (497)\ttotal: 10.6s\tremaining: 10.5s\n",
      "502:\tlearn: 0.4584891\ttest: 0.4865302\tbest: 0.4865186 (497)\ttotal: 10.7s\tremaining: 10.5s\n",
      "503:\tlearn: 0.4584230\ttest: 0.4864986\tbest: 0.4864986 (503)\ttotal: 10.7s\tremaining: 10.5s\n",
      "504:\tlearn: 0.4583562\ttest: 0.4865159\tbest: 0.4864986 (503)\ttotal: 10.7s\tremaining: 10.5s\n",
      "505:\tlearn: 0.4583070\ttest: 0.4865047\tbest: 0.4864986 (503)\ttotal: 10.7s\tremaining: 10.5s\n",
      "506:\tlearn: 0.4582462\ttest: 0.4865251\tbest: 0.4864986 (503)\ttotal: 10.7s\tremaining: 10.4s\n",
      "507:\tlearn: 0.4581803\ttest: 0.4865310\tbest: 0.4864986 (503)\ttotal: 10.8s\tremaining: 10.4s\n",
      "508:\tlearn: 0.4581163\ttest: 0.4865373\tbest: 0.4864986 (503)\ttotal: 10.8s\tremaining: 10.4s\n",
      "509:\tlearn: 0.4580640\ttest: 0.4865144\tbest: 0.4864986 (503)\ttotal: 10.8s\tremaining: 10.4s\n",
      "510:\tlearn: 0.4580008\ttest: 0.4865138\tbest: 0.4864986 (503)\ttotal: 10.8s\tremaining: 10.4s\n",
      "511:\tlearn: 0.4579289\ttest: 0.4864901\tbest: 0.4864901 (511)\ttotal: 10.8s\tremaining: 10.3s\n",
      "512:\tlearn: 0.4578736\ttest: 0.4864971\tbest: 0.4864901 (511)\ttotal: 10.9s\tremaining: 10.3s\n",
      "513:\tlearn: 0.4578383\ttest: 0.4864955\tbest: 0.4864901 (511)\ttotal: 10.9s\tremaining: 10.3s\n",
      "514:\tlearn: 0.4577726\ttest: 0.4864828\tbest: 0.4864828 (514)\ttotal: 10.9s\tremaining: 10.3s\n",
      "515:\tlearn: 0.4577201\ttest: 0.4864758\tbest: 0.4864758 (515)\ttotal: 10.9s\tremaining: 10.2s\n",
      "516:\tlearn: 0.4576632\ttest: 0.4864662\tbest: 0.4864662 (516)\ttotal: 10.9s\tremaining: 10.2s\n",
      "517:\tlearn: 0.4576164\ttest: 0.4864800\tbest: 0.4864662 (516)\ttotal: 11s\tremaining: 10.2s\n",
      "518:\tlearn: 0.4575604\ttest: 0.4864767\tbest: 0.4864662 (516)\ttotal: 11s\tremaining: 10.2s\n",
      "519:\tlearn: 0.4575102\ttest: 0.4864737\tbest: 0.4864662 (516)\ttotal: 11s\tremaining: 10.2s\n",
      "520:\tlearn: 0.4574589\ttest: 0.4864754\tbest: 0.4864662 (516)\ttotal: 11s\tremaining: 10.1s\n",
      "521:\tlearn: 0.4574090\ttest: 0.4864671\tbest: 0.4864662 (516)\ttotal: 11s\tremaining: 10.1s\n",
      "522:\tlearn: 0.4573543\ttest: 0.4864811\tbest: 0.4864662 (516)\ttotal: 11.1s\tremaining: 10.1s\n",
      "523:\tlearn: 0.4573175\ttest: 0.4864701\tbest: 0.4864662 (516)\ttotal: 11.1s\tremaining: 10.1s\n",
      "524:\tlearn: 0.4572624\ttest: 0.4864533\tbest: 0.4864533 (524)\ttotal: 11.1s\tremaining: 10s\n",
      "525:\tlearn: 0.4572046\ttest: 0.4864545\tbest: 0.4864533 (524)\ttotal: 11.1s\tremaining: 10s\n",
      "526:\tlearn: 0.4571361\ttest: 0.4864603\tbest: 0.4864533 (524)\ttotal: 11.2s\tremaining: 10s\n",
      "527:\tlearn: 0.4570886\ttest: 0.4864637\tbest: 0.4864533 (524)\ttotal: 11.2s\tremaining: 9.99s\n",
      "528:\tlearn: 0.4570420\ttest: 0.4864716\tbest: 0.4864533 (524)\ttotal: 11.2s\tremaining: 9.97s\n",
      "529:\tlearn: 0.4569792\ttest: 0.4864547\tbest: 0.4864533 (524)\ttotal: 11.2s\tremaining: 9.95s\n",
      "530:\tlearn: 0.4569108\ttest: 0.4864454\tbest: 0.4864454 (530)\ttotal: 11.2s\tremaining: 9.92s\n",
      "531:\tlearn: 0.4568637\ttest: 0.4864406\tbest: 0.4864406 (531)\ttotal: 11.3s\tremaining: 9.9s\n",
      "532:\tlearn: 0.4568120\ttest: 0.4864198\tbest: 0.4864198 (532)\ttotal: 11.3s\tremaining: 9.88s\n",
      "533:\tlearn: 0.4567462\ttest: 0.4864232\tbest: 0.4864198 (532)\ttotal: 11.3s\tremaining: 9.85s\n",
      "534:\tlearn: 0.4566804\ttest: 0.4863987\tbest: 0.4863987 (534)\ttotal: 11.3s\tremaining: 9.83s\n",
      "535:\tlearn: 0.4566302\ttest: 0.4864079\tbest: 0.4863987 (534)\ttotal: 11.3s\tremaining: 9.8s\n",
      "536:\tlearn: 0.4565748\ttest: 0.4864206\tbest: 0.4863987 (534)\ttotal: 11.3s\tremaining: 9.78s\n",
      "537:\tlearn: 0.4565064\ttest: 0.4863910\tbest: 0.4863910 (537)\ttotal: 11.4s\tremaining: 9.76s\n",
      "538:\tlearn: 0.4564481\ttest: 0.4863669\tbest: 0.4863669 (538)\ttotal: 11.4s\tremaining: 9.74s\n",
      "539:\tlearn: 0.4563814\ttest: 0.4863632\tbest: 0.4863632 (539)\ttotal: 11.4s\tremaining: 9.72s\n",
      "540:\tlearn: 0.4563331\ttest: 0.4863727\tbest: 0.4863632 (539)\ttotal: 11.4s\tremaining: 9.69s\n",
      "541:\tlearn: 0.4562809\ttest: 0.4863564\tbest: 0.4863564 (541)\ttotal: 11.4s\tremaining: 9.67s\n",
      "542:\tlearn: 0.4562116\ttest: 0.4863570\tbest: 0.4863564 (541)\ttotal: 11.5s\tremaining: 9.65s\n",
      "543:\tlearn: 0.4561430\ttest: 0.4863697\tbest: 0.4863564 (541)\ttotal: 11.5s\tremaining: 9.63s\n",
      "544:\tlearn: 0.4560613\ttest: 0.4863872\tbest: 0.4863564 (541)\ttotal: 11.5s\tremaining: 9.6s\n",
      "545:\tlearn: 0.4560006\ttest: 0.4863684\tbest: 0.4863564 (541)\ttotal: 11.5s\tremaining: 9.58s\n",
      "546:\tlearn: 0.4559310\ttest: 0.4863690\tbest: 0.4863564 (541)\ttotal: 11.5s\tremaining: 9.56s\n",
      "547:\tlearn: 0.4558627\ttest: 0.4863552\tbest: 0.4863552 (547)\ttotal: 11.6s\tremaining: 9.54s\n",
      "548:\tlearn: 0.4558018\ttest: 0.4863576\tbest: 0.4863552 (547)\ttotal: 11.6s\tremaining: 9.52s\n",
      "549:\tlearn: 0.4557341\ttest: 0.4863637\tbest: 0.4863552 (547)\ttotal: 11.6s\tremaining: 9.5s\n",
      "550:\tlearn: 0.4556722\ttest: 0.4863803\tbest: 0.4863552 (547)\ttotal: 11.6s\tremaining: 9.48s\n",
      "551:\tlearn: 0.4556116\ttest: 0.4863549\tbest: 0.4863549 (551)\ttotal: 11.7s\tremaining: 9.46s\n",
      "552:\tlearn: 0.4555575\ttest: 0.4863699\tbest: 0.4863549 (551)\ttotal: 11.7s\tremaining: 9.43s\n",
      "553:\tlearn: 0.4555093\ttest: 0.4863590\tbest: 0.4863549 (551)\ttotal: 11.7s\tremaining: 9.41s\n",
      "554:\tlearn: 0.4554525\ttest: 0.4863607\tbest: 0.4863549 (551)\ttotal: 11.7s\tremaining: 9.39s\n",
      "555:\tlearn: 0.4553930\ttest: 0.4863497\tbest: 0.4863497 (555)\ttotal: 11.7s\tremaining: 9.36s\n",
      "556:\tlearn: 0.4553429\ttest: 0.4863530\tbest: 0.4863497 (555)\ttotal: 11.7s\tremaining: 9.34s\n",
      "557:\tlearn: 0.4552812\ttest: 0.4863521\tbest: 0.4863497 (555)\ttotal: 11.8s\tremaining: 9.32s\n",
      "558:\tlearn: 0.4552152\ttest: 0.4863370\tbest: 0.4863370 (558)\ttotal: 11.8s\tremaining: 9.3s\n",
      "559:\tlearn: 0.4551455\ttest: 0.4863237\tbest: 0.4863237 (559)\ttotal: 11.8s\tremaining: 9.28s\n",
      "560:\tlearn: 0.4550843\ttest: 0.4863192\tbest: 0.4863192 (560)\ttotal: 11.8s\tremaining: 9.26s\n",
      "561:\tlearn: 0.4550057\ttest: 0.4863122\tbest: 0.4863122 (561)\ttotal: 11.9s\tremaining: 9.24s\n",
      "562:\tlearn: 0.4549437\ttest: 0.4863132\tbest: 0.4863122 (561)\ttotal: 11.9s\tremaining: 9.22s\n",
      "563:\tlearn: 0.4548759\ttest: 0.4863125\tbest: 0.4863122 (561)\ttotal: 11.9s\tremaining: 9.2s\n",
      "564:\tlearn: 0.4548068\ttest: 0.4863048\tbest: 0.4863048 (564)\ttotal: 11.9s\tremaining: 9.18s\n",
      "565:\tlearn: 0.4547484\ttest: 0.4862873\tbest: 0.4862873 (565)\ttotal: 11.9s\tremaining: 9.15s\n",
      "566:\tlearn: 0.4546983\ttest: 0.4862775\tbest: 0.4862775 (566)\ttotal: 12s\tremaining: 9.13s\n",
      "567:\tlearn: 0.4546479\ttest: 0.4862678\tbest: 0.4862678 (567)\ttotal: 12s\tremaining: 9.11s\n",
      "568:\tlearn: 0.4545875\ttest: 0.4862716\tbest: 0.4862678 (567)\ttotal: 12s\tremaining: 9.09s\n",
      "569:\tlearn: 0.4545297\ttest: 0.4862870\tbest: 0.4862678 (567)\ttotal: 12s\tremaining: 9.07s\n",
      "570:\tlearn: 0.4544588\ttest: 0.4863063\tbest: 0.4862678 (567)\ttotal: 12s\tremaining: 9.05s\n",
      "571:\tlearn: 0.4544030\ttest: 0.4863043\tbest: 0.4862678 (567)\ttotal: 12.1s\tremaining: 9.03s\n",
      "572:\tlearn: 0.4543430\ttest: 0.4862951\tbest: 0.4862678 (567)\ttotal: 12.1s\tremaining: 9s\n",
      "573:\tlearn: 0.4542848\ttest: 0.4863244\tbest: 0.4862678 (567)\ttotal: 12.1s\tremaining: 8.98s\n",
      "574:\tlearn: 0.4542223\ttest: 0.4863218\tbest: 0.4862678 (567)\ttotal: 12.1s\tremaining: 8.96s\n",
      "575:\tlearn: 0.4541588\ttest: 0.4863202\tbest: 0.4862678 (567)\ttotal: 12.1s\tremaining: 8.94s\n",
      "576:\tlearn: 0.4540923\ttest: 0.4863686\tbest: 0.4862678 (567)\ttotal: 12.2s\tremaining: 8.92s\n",
      "577:\tlearn: 0.4540306\ttest: 0.4863565\tbest: 0.4862678 (567)\ttotal: 12.2s\tremaining: 8.89s\n",
      "578:\tlearn: 0.4539643\ttest: 0.4863510\tbest: 0.4862678 (567)\ttotal: 12.2s\tremaining: 8.87s\n",
      "579:\tlearn: 0.4539149\ttest: 0.4863518\tbest: 0.4862678 (567)\ttotal: 12.2s\tremaining: 8.85s\n",
      "580:\tlearn: 0.4538606\ttest: 0.4863352\tbest: 0.4862678 (567)\ttotal: 12.2s\tremaining: 8.83s\n",
      "581:\tlearn: 0.4538142\ttest: 0.4863319\tbest: 0.4862678 (567)\ttotal: 12.3s\tremaining: 8.81s\n",
      "582:\tlearn: 0.4537447\ttest: 0.4863253\tbest: 0.4862678 (567)\ttotal: 12.3s\tremaining: 8.79s\n",
      "583:\tlearn: 0.4537152\ttest: 0.4863251\tbest: 0.4862678 (567)\ttotal: 12.3s\tremaining: 8.77s\n",
      "584:\tlearn: 0.4536608\ttest: 0.4863201\tbest: 0.4862678 (567)\ttotal: 12.3s\tremaining: 8.75s\n",
      "585:\tlearn: 0.4535978\ttest: 0.4863320\tbest: 0.4862678 (567)\ttotal: 12.4s\tremaining: 8.73s\n",
      "586:\tlearn: 0.4535317\ttest: 0.4863195\tbest: 0.4862678 (567)\ttotal: 12.4s\tremaining: 8.71s\n",
      "587:\tlearn: 0.4534849\ttest: 0.4863188\tbest: 0.4862678 (567)\ttotal: 12.4s\tremaining: 8.68s\n",
      "588:\tlearn: 0.4534306\ttest: 0.4863302\tbest: 0.4862678 (567)\ttotal: 12.4s\tremaining: 8.66s\n",
      "589:\tlearn: 0.4533725\ttest: 0.4863444\tbest: 0.4862678 (567)\ttotal: 12.4s\tremaining: 8.64s\n",
      "590:\tlearn: 0.4533117\ttest: 0.4863394\tbest: 0.4862678 (567)\ttotal: 12.5s\tremaining: 8.62s\n",
      "591:\tlearn: 0.4532688\ttest: 0.4863384\tbest: 0.4862678 (567)\ttotal: 12.5s\tremaining: 8.6s\n",
      "592:\tlearn: 0.4531890\ttest: 0.4863546\tbest: 0.4862678 (567)\ttotal: 12.5s\tremaining: 8.58s\n",
      "593:\tlearn: 0.4531252\ttest: 0.4863597\tbest: 0.4862678 (567)\ttotal: 12.5s\tremaining: 8.55s\n",
      "594:\tlearn: 0.4530880\ttest: 0.4863676\tbest: 0.4862678 (567)\ttotal: 12.5s\tremaining: 8.53s\n",
      "595:\tlearn: 0.4530278\ttest: 0.4863661\tbest: 0.4862678 (567)\ttotal: 12.6s\tremaining: 8.51s\n",
      "596:\tlearn: 0.4529756\ttest: 0.4863918\tbest: 0.4862678 (567)\ttotal: 12.6s\tremaining: 8.49s\n",
      "597:\tlearn: 0.4529236\ttest: 0.4863928\tbest: 0.4862678 (567)\ttotal: 12.6s\tremaining: 8.47s\n",
      "598:\tlearn: 0.4528598\ttest: 0.4863907\tbest: 0.4862678 (567)\ttotal: 12.6s\tremaining: 8.44s\n",
      "599:\tlearn: 0.4527894\ttest: 0.4864008\tbest: 0.4862678 (567)\ttotal: 12.6s\tremaining: 8.42s\n",
      "600:\tlearn: 0.4527478\ttest: 0.4864118\tbest: 0.4862678 (567)\ttotal: 12.7s\tremaining: 8.4s\n",
      "601:\tlearn: 0.4526859\ttest: 0.4864113\tbest: 0.4862678 (567)\ttotal: 12.7s\tremaining: 8.38s\n",
      "602:\tlearn: 0.4526103\ttest: 0.4863990\tbest: 0.4862678 (567)\ttotal: 12.7s\tremaining: 8.36s\n",
      "603:\tlearn: 0.4525775\ttest: 0.4863924\tbest: 0.4862678 (567)\ttotal: 12.7s\tremaining: 8.34s\n",
      "604:\tlearn: 0.4525314\ttest: 0.4864041\tbest: 0.4862678 (567)\ttotal: 12.7s\tremaining: 8.32s\n",
      "605:\tlearn: 0.4524844\ttest: 0.4863905\tbest: 0.4862678 (567)\ttotal: 12.8s\tremaining: 8.29s\n",
      "606:\tlearn: 0.4524362\ttest: 0.4863888\tbest: 0.4862678 (567)\ttotal: 12.8s\tremaining: 8.27s\n",
      "607:\tlearn: 0.4523803\ttest: 0.4864013\tbest: 0.4862678 (567)\ttotal: 12.8s\tremaining: 8.25s\n",
      "608:\tlearn: 0.4523203\ttest: 0.4864252\tbest: 0.4862678 (567)\ttotal: 12.8s\tremaining: 8.23s\n",
      "609:\tlearn: 0.4522699\ttest: 0.4864194\tbest: 0.4862678 (567)\ttotal: 12.8s\tremaining: 8.21s\n",
      "610:\tlearn: 0.4522307\ttest: 0.4864274\tbest: 0.4862678 (567)\ttotal: 12.9s\tremaining: 8.19s\n",
      "611:\tlearn: 0.4521757\ttest: 0.4864177\tbest: 0.4862678 (567)\ttotal: 12.9s\tremaining: 8.17s\n",
      "612:\tlearn: 0.4521178\ttest: 0.4864325\tbest: 0.4862678 (567)\ttotal: 12.9s\tremaining: 8.15s\n",
      "613:\tlearn: 0.4520504\ttest: 0.4864140\tbest: 0.4862678 (567)\ttotal: 12.9s\tremaining: 8.13s\n",
      "614:\tlearn: 0.4520011\ttest: 0.4864130\tbest: 0.4862678 (567)\ttotal: 12.9s\tremaining: 8.1s\n",
      "615:\tlearn: 0.4519751\ttest: 0.4864139\tbest: 0.4862678 (567)\ttotal: 13s\tremaining: 8.08s\n",
      "616:\tlearn: 0.4519157\ttest: 0.4864241\tbest: 0.4862678 (567)\ttotal: 13s\tremaining: 8.06s\n",
      "617:\tlearn: 0.4518501\ttest: 0.4864234\tbest: 0.4862678 (567)\ttotal: 13s\tremaining: 8.04s\n",
      "618:\tlearn: 0.4517971\ttest: 0.4864297\tbest: 0.4862678 (567)\ttotal: 13s\tremaining: 8.02s\n",
      "619:\tlearn: 0.4517255\ttest: 0.4864658\tbest: 0.4862678 (567)\ttotal: 13.1s\tremaining: 8s\n",
      "620:\tlearn: 0.4516914\ttest: 0.4864759\tbest: 0.4862678 (567)\ttotal: 13.1s\tremaining: 7.98s\n",
      "621:\tlearn: 0.4516217\ttest: 0.4864781\tbest: 0.4862678 (567)\ttotal: 13.1s\tremaining: 7.96s\n",
      "622:\tlearn: 0.4515669\ttest: 0.4864691\tbest: 0.4862678 (567)\ttotal: 13.1s\tremaining: 7.94s\n",
      "623:\tlearn: 0.4515190\ttest: 0.4864707\tbest: 0.4862678 (567)\ttotal: 13.1s\tremaining: 7.92s\n",
      "624:\tlearn: 0.4514706\ttest: 0.4864552\tbest: 0.4862678 (567)\ttotal: 13.2s\tremaining: 7.9s\n",
      "625:\tlearn: 0.4514220\ttest: 0.4864526\tbest: 0.4862678 (567)\ttotal: 13.2s\tremaining: 7.88s\n",
      "626:\tlearn: 0.4513703\ttest: 0.4864486\tbest: 0.4862678 (567)\ttotal: 13.2s\tremaining: 7.86s\n",
      "627:\tlearn: 0.4513109\ttest: 0.4864679\tbest: 0.4862678 (567)\ttotal: 13.2s\tremaining: 7.84s\n",
      "628:\tlearn: 0.4512543\ttest: 0.4864874\tbest: 0.4862678 (567)\ttotal: 13.3s\tremaining: 7.82s\n",
      "629:\tlearn: 0.4512002\ttest: 0.4865014\tbest: 0.4862678 (567)\ttotal: 13.3s\tremaining: 7.8s\n",
      "630:\tlearn: 0.4511324\ttest: 0.4864913\tbest: 0.4862678 (567)\ttotal: 13.3s\tremaining: 7.78s\n",
      "631:\tlearn: 0.4510628\ttest: 0.4864886\tbest: 0.4862678 (567)\ttotal: 13.3s\tremaining: 7.76s\n",
      "632:\tlearn: 0.4510043\ttest: 0.4865100\tbest: 0.4862678 (567)\ttotal: 13.3s\tremaining: 7.74s\n",
      "633:\tlearn: 0.4509505\ttest: 0.4865138\tbest: 0.4862678 (567)\ttotal: 13.4s\tremaining: 7.72s\n",
      "634:\tlearn: 0.4508882\ttest: 0.4865319\tbest: 0.4862678 (567)\ttotal: 13.4s\tremaining: 7.7s\n",
      "635:\tlearn: 0.4508098\ttest: 0.4865336\tbest: 0.4862678 (567)\ttotal: 13.4s\tremaining: 7.67s\n",
      "636:\tlearn: 0.4507499\ttest: 0.4865247\tbest: 0.4862678 (567)\ttotal: 13.4s\tremaining: 7.66s\n",
      "637:\tlearn: 0.4506900\ttest: 0.4865133\tbest: 0.4862678 (567)\ttotal: 13.5s\tremaining: 7.64s\n",
      "638:\tlearn: 0.4506336\ttest: 0.4865175\tbest: 0.4862678 (567)\ttotal: 13.5s\tremaining: 7.62s\n",
      "639:\tlearn: 0.4505797\ttest: 0.4865267\tbest: 0.4862678 (567)\ttotal: 13.5s\tremaining: 7.6s\n",
      "640:\tlearn: 0.4505372\ttest: 0.4865248\tbest: 0.4862678 (567)\ttotal: 13.5s\tremaining: 7.58s\n",
      "641:\tlearn: 0.4504763\ttest: 0.4865486\tbest: 0.4862678 (567)\ttotal: 13.6s\tremaining: 7.56s\n",
      "642:\tlearn: 0.4504059\ttest: 0.4865698\tbest: 0.4862678 (567)\ttotal: 13.6s\tremaining: 7.54s\n",
      "643:\tlearn: 0.4503422\ttest: 0.4865762\tbest: 0.4862678 (567)\ttotal: 13.6s\tremaining: 7.52s\n",
      "644:\tlearn: 0.4502930\ttest: 0.4865716\tbest: 0.4862678 (567)\ttotal: 13.6s\tremaining: 7.5s\n",
      "645:\tlearn: 0.4502401\ttest: 0.4865593\tbest: 0.4862678 (567)\ttotal: 13.6s\tremaining: 7.47s\n",
      "646:\tlearn: 0.4501918\ttest: 0.4865822\tbest: 0.4862678 (567)\ttotal: 13.7s\tremaining: 7.45s\n",
      "647:\tlearn: 0.4501301\ttest: 0.4866113\tbest: 0.4862678 (567)\ttotal: 13.7s\tremaining: 7.43s\n",
      "648:\tlearn: 0.4500856\ttest: 0.4866281\tbest: 0.4862678 (567)\ttotal: 13.7s\tremaining: 7.41s\n",
      "649:\tlearn: 0.4500457\ttest: 0.4866285\tbest: 0.4862678 (567)\ttotal: 13.7s\tremaining: 7.39s\n",
      "650:\tlearn: 0.4500049\ttest: 0.4866287\tbest: 0.4862678 (567)\ttotal: 13.7s\tremaining: 7.37s\n",
      "651:\tlearn: 0.4499365\ttest: 0.4866195\tbest: 0.4862678 (567)\ttotal: 13.8s\tremaining: 7.35s\n",
      "652:\tlearn: 0.4498691\ttest: 0.4866131\tbest: 0.4862678 (567)\ttotal: 13.8s\tremaining: 7.33s\n",
      "653:\tlearn: 0.4498108\ttest: 0.4866281\tbest: 0.4862678 (567)\ttotal: 13.8s\tremaining: 7.31s\n",
      "654:\tlearn: 0.4497571\ttest: 0.4866288\tbest: 0.4862678 (567)\ttotal: 13.8s\tremaining: 7.29s\n",
      "655:\tlearn: 0.4496932\ttest: 0.4866182\tbest: 0.4862678 (567)\ttotal: 13.9s\tremaining: 7.27s\n",
      "656:\tlearn: 0.4496247\ttest: 0.4866339\tbest: 0.4862678 (567)\ttotal: 13.9s\tremaining: 7.25s\n",
      "657:\tlearn: 0.4495829\ttest: 0.4866228\tbest: 0.4862678 (567)\ttotal: 13.9s\tremaining: 7.23s\n",
      "658:\tlearn: 0.4495265\ttest: 0.4866213\tbest: 0.4862678 (567)\ttotal: 13.9s\tremaining: 7.21s\n",
      "659:\tlearn: 0.4494600\ttest: 0.4866245\tbest: 0.4862678 (567)\ttotal: 13.9s\tremaining: 7.18s\n",
      "660:\tlearn: 0.4494080\ttest: 0.4866472\tbest: 0.4862678 (567)\ttotal: 14s\tremaining: 7.16s\n",
      "661:\tlearn: 0.4493640\ttest: 0.4866641\tbest: 0.4862678 (567)\ttotal: 14s\tremaining: 7.14s\n",
      "662:\tlearn: 0.4493002\ttest: 0.4866740\tbest: 0.4862678 (567)\ttotal: 14s\tremaining: 7.12s\n",
      "663:\tlearn: 0.4492530\ttest: 0.4866654\tbest: 0.4862678 (567)\ttotal: 14s\tremaining: 7.1s\n",
      "664:\tlearn: 0.4491951\ttest: 0.4866709\tbest: 0.4862678 (567)\ttotal: 14s\tremaining: 7.07s\n",
      "665:\tlearn: 0.4491380\ttest: 0.4866864\tbest: 0.4862678 (567)\ttotal: 14.1s\tremaining: 7.05s\n",
      "666:\tlearn: 0.4490796\ttest: 0.4866778\tbest: 0.4862678 (567)\ttotal: 14.1s\tremaining: 7.03s\n",
      "667:\tlearn: 0.4490159\ttest: 0.4866761\tbest: 0.4862678 (567)\ttotal: 14.1s\tremaining: 7.01s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.4862678017\n",
      "bestIteration = 567\n",
      "\n",
      "Shrink model to first 568 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f89d3496eb0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "X_train_np_, X_val_np, label_np_, val_label_np = train_test_split(X_train_np, label_np, test_size=0.2)\n",
    "test_data =  Pool(X_val_np, val_label_np)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "                            )\n",
    "\n",
    "# model = CatBoostClassifier(\n",
    "#                             )\n",
    "# model = CatBoostClassifier(iterations=1000,\n",
    "#                         depth=9,\n",
    "#                         learning_rate=0.1,\n",
    "#                         # loss_function='Logloss',\n",
    "#                         eval_metric='AUC',\n",
    "#                         )\n",
    "# train the model\n",
    "model.fit(X_train_np_, label_np_, eval_set = test_data, use_best_model=True, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6931  131]\n",
      " [1716  113]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88      7062\n",
      "           1       0.46      0.06      0.11      1829\n",
      "\n",
      "    accuracy                           0.79      8891\n",
      "   macro avg       0.63      0.52      0.50      8891\n",
      "weighted avg       0.73      0.79      0.72      8891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_data = X_test_np\n",
    "preds_proba = model.predict_proba(eval_data)\n",
    "# preds_class = model.predict(test_data)\n",
    "pr_d = pd.DataFrame({'val':preds_proba[:,1].tolist()})\n",
    "pr_d['pred'] = pr_d['val'].apply(lambda x: 1 if x >= 0.4 else 0)\n",
    "pr_d['Date'] = test_df['Date']\n",
    "o_d = []\n",
    "for i in pr_d['pred'].tolist():\n",
    "#     if i[1] > 0.7:''\n",
    "#         o.append(1)z\n",
    "#     else:\n",
    "#         o.append(o)\n",
    "    \n",
    "    o_d.append(i)\n",
    "    \n",
    "print(confusion_matrix(test_label_np.tolist(), o_d))\n",
    "\n",
    "print(classification_report(test_label_np.tolist(), o_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = pd.read_csv('historical_data/AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df[\"Adj_open\"] = stock_df[\"Open\"] * stock_df[\"Adjusted_close\"] / stock_df[\"Close\"]\n",
    "stock_df[\"Adj_high\"] = stock_df[\"High\"] * stock_df[\"Adjusted_close\"] / stock_df[\"Close\"]\n",
    "stock_df[\"Adj_low\"] = stock_df[\"Low\"] * stock_df[\"Adjusted_close\"] / stock_df[\"Close\"]\n",
    "stock_df[\"Adj_volume\"] = stock_df[\"Volume\"] * stock_df[\"Close\"] / stock_df[\"Adjusted_close\"]\n",
    "stock_df[\"Tx_amount\"] = stock_df[\"Volume\"] * stock_df[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  Adjusted_close  Adj_open    Adj_high   Adj_low  \\\n",
      "0      1980-12-11          0.0769    0.0769    0.076900    0.0769   \n",
      "1      1980-12-12          0.1006    0.1006    0.101041    0.1006   \n",
      "2      1980-12-15          0.0954    0.0954    0.095841    0.0954   \n",
      "3      1980-12-16          0.0883    0.0883    0.088741    0.0883   \n",
      "4      1980-12-17          0.0905    0.0905    0.090934    0.0905   \n",
      "...           ...             ...       ...         ...       ...   \n",
      "10215  2021-06-21        132.3000  130.3000  132.410000  129.2100   \n",
      "10216  2021-06-22        133.9800  132.1300  134.080000  131.6200   \n",
      "10217  2021-06-23        133.7000  133.7700  134.320000  133.2300   \n",
      "10218  2021-06-24        133.4100  134.4500  134.640000  132.9300   \n",
      "10219  2021-06-25        133.1100  133.4600  133.890000  132.8100   \n",
      "\n",
      "         Adj_volume    Close    Volume     Tx_amount  \n",
      "0      0.000000e+00   22.000         0  0.000000e+00  \n",
      "1      5.984058e+08   28.750   2093900  6.019962e+07  \n",
      "2      2.242841e+08   27.250    785200  2.139670e+07  \n",
      "3      1.349717e+08   25.250    472000  1.191800e+07  \n",
      "4      1.103376e+08   25.876    385900  9.985548e+06  \n",
      "...             ...      ...       ...           ...  \n",
      "10215  7.966330e+07  132.300  79663300  1.053945e+10  \n",
      "10216  7.478360e+07  133.980  74783600  1.001951e+10  \n",
      "10217  6.021420e+07  133.700  60214200  8.050639e+09  \n",
      "10218  6.871100e+07  133.410  68711000  9.166735e+09  \n",
      "10219  7.073070e+07  133.110  70730700  9.414963e+09  \n",
      "\n",
      "[10220 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "stock_sel_df = stock_df[['Date', 'Adjusted_close', 'Adj_open', 'Adj_high', 'Adj_low', 'Adj_volume', 'Close', 'Volume', 'Tx_amount']]\n",
    "print(stock_sel_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def addEpoch(date):\n",
    "    epoch = datetime.strptime(date, \"%Y-%m-%d\").timestamp()\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  Adjusted_close  Adj_open    Adj_high   Adj_low  \\\n",
      "0      1980-12-11          0.0769    0.0769    0.076900    0.0769   \n",
      "1      1980-12-12          0.1006    0.1006    0.101041    0.1006   \n",
      "2      1980-12-15          0.0954    0.0954    0.095841    0.0954   \n",
      "3      1980-12-16          0.0883    0.0883    0.088741    0.0883   \n",
      "4      1980-12-17          0.0905    0.0905    0.090934    0.0905   \n",
      "...           ...             ...       ...         ...       ...   \n",
      "10215  2021-06-21        132.3000  130.3000  132.410000  129.2100   \n",
      "10216  2021-06-22        133.9800  132.1300  134.080000  131.6200   \n",
      "10217  2021-06-23        133.7000  133.7700  134.320000  133.2300   \n",
      "10218  2021-06-24        133.4100  134.4500  134.640000  132.9300   \n",
      "10219  2021-06-25        133.1100  133.4600  133.890000  132.8100   \n",
      "\n",
      "         Adj_volume    Close    Volume     Tx_amount         Epoch  \n",
      "0      0.000000e+00   22.000         0  0.000000e+00  3.453588e+08  \n",
      "1      5.984058e+08   28.750   2093900  6.019962e+07  3.454452e+08  \n",
      "2      2.242841e+08   27.250    785200  2.139670e+07  3.457044e+08  \n",
      "3      1.349717e+08   25.250    472000  1.191800e+07  3.457908e+08  \n",
      "4      1.103376e+08   25.876    385900  9.985548e+06  3.458772e+08  \n",
      "...             ...      ...       ...           ...           ...  \n",
      "10215  7.966330e+07  132.300  79663300  1.053945e+10  1.624248e+09  \n",
      "10216  7.478360e+07  133.980  74783600  1.001951e+10  1.624334e+09  \n",
      "10217  6.021420e+07  133.700  60214200  8.050639e+09  1.624421e+09  \n",
      "10218  6.871100e+07  133.410  68711000  9.166735e+09  1.624507e+09  \n",
      "10219  7.073070e+07  133.110  70730700  9.414963e+09  1.624594e+09  \n",
      "\n",
      "[10220 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# add epoch to data frame\n",
    "stock_sel_df[\"Epoch\"]=stock_sel_df[\"Date\"].apply(addEpoch)\n",
    "print(stock_sel_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start working on subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date = \"2010-11-15\" \n",
    "from_epoch = datetime.strptime(from_date, \"%Y-%m-%d\").timestamp()\n",
    "\n",
    "to_date = \"2020-02-15\" \n",
    "to_epoch = datetime.strptime(to_date, \"%Y-%m-%d\").timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Adjusted_close   Adj_open   Adj_high    Adj_low  \\\n",
      "7549  2010-11-15          9.4294   9.473163   9.537043   9.405906   \n",
      "7550  2010-11-16          9.2622   9.389037   9.446774   9.192486   \n",
      "7551  2010-11-17          9.2288   9.250298   9.335983   9.144651   \n",
      "7552  2010-11-18          9.4723   9.373102   9.510382   9.357440   \n",
      "7553  2010-11-19          9.4201   9.458182   9.471388   9.374340   \n",
      "...          ...             ...        ...        ...        ...   \n",
      "9872  2020-02-10         79.6339  77.808673  79.633900  77.726946   \n",
      "9873  2020-02-11         79.1534  80.141548  80.215845  78.930509   \n",
      "9874  2020-02-12         81.0332  79.614128  81.038153  79.614128   \n",
      "9875  2020-02-13         80.4561  80.287694  80.790436  80.079662   \n",
      "9876  2020-02-14         80.4759  80.423892  80.730986  79.955822   \n",
      "\n",
      "        Adj_volume    Close    Volume     Tx_amount         Epoch  \n",
      "7549  4.693546e+08  307.035  14414422  4.425732e+09  1.289797e+09  \n",
      "7550  7.662964e+08  301.590  23533906  7.097591e+09  1.289884e+09  \n",
      "7551  5.575505e+08  300.500  17123201  5.145522e+09  1.289970e+09  \n",
      "7552  5.750432e+08  308.430  17660349  5.446981e+09  1.290056e+09  \n",
      "7553  4.475345e+08  306.730  13744399  4.215820e+09  1.290143e+09  \n",
      "...            ...      ...       ...           ...           ...  \n",
      "9872  1.103837e+08  321.550  27337215  8.790281e+09  1.581311e+09  \n",
      "9873  9.521578e+07  319.610  23580780  7.536653e+09  1.581397e+09  \n",
      "9874  1.148065e+08  327.200  28432573  9.303138e+09  1.581484e+09  \n",
      "9875  9.564422e+07  324.870  23686892  7.695161e+09  1.581570e+09  \n",
      "9876  8.087196e+07  324.950  20028447  6.508244e+09  1.581656e+09  \n",
      "\n",
      "[2328 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# selecting rows based on condition\n",
    "new_df = stock_sel_df.loc[(stock_sel_df['Epoch'] >= from_epoch) & (stock_sel_df['Epoch'] <= to_epoch)]\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete creating subset from 2014-11-15 to 2020-01-15 and Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addUpIndicator(change_p):\n",
    "    if change_p > 0:\n",
    "        return 'U'\n",
    "    else:\n",
    "        return 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add3LIndicator(change_p):\n",
    "    if change_p >= 0.005:\n",
    "        return 'U'\n",
    "    elif change_p <= -0.005:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate change in pricing n days in the future\n",
    "shift = \"nd_dshift\"\n",
    "change_p = \"future_ch_p\"\n",
    "up = \"up\"\n",
    "l3 = \"3l\"\n",
    "n=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[shift] = new_df[\"Adjusted_close\"]\n",
    "new_df[shift] = new_df[shift].shift(-n)\n",
    "new_df = new_df[:-n]\n",
    "\n",
    "new_df[change_p] = (new_df[shift]-new_df[\"Adjusted_close\"])/new_df[\"Adjusted_close\"]\n",
    "\n",
    "new_df[up]=new_df[change_p].apply(addUpIndicator)\n",
    "new_df[l3]=new_df[change_p].apply(add3LIndicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate change in Pricing & Volume relative to 1 day in the past\n",
    "shift = \"1ushift\"\n",
    "shift_v = \"1ushift_v\"\n",
    "change_p = \"past1d_ch_p\"\n",
    "change_v = \"past1d_ch_v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[shift] = new_df[\"Adjusted_close\"]\n",
    "new_df[shift] = new_df[shift].shift(1)\n",
    "\n",
    "new_df[shift_v] = new_df[\"Volume\"]\n",
    "new_df[shift_v] = new_df[shift_v].shift(1)\n",
    "\n",
    "new_df = new_df[1:]\n",
    "\n",
    "new_df[change_p] = (new_df[\"Adjusted_close\"] - new_df[shift])/new_df[shift]\n",
    "new_df[change_v] = (new_df[\"Volume\"] - new_df[shift_v])/new_df[shift_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Directional Volume (similar to On Balance Volume)\n",
    "new_df[\"Directional_volume\"] = np.sign(new_df[change_p]) * new_df[\"Volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "U    0.527945\n",
       "D    0.472055\n",
       "Name: up, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[\"up\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "U    0.378332\n",
       "N    0.323732\n",
       "D    0.297936\n",
       "Name: 3l, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[\"3l\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def addLabel(value):\n",
    "    if value == 'U':\n",
    "        a = 2\n",
    "    elif value == 'N':\n",
    "        a = 1\n",
    "    elif value == 'D':\n",
    "        a = 0\n",
    "        \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"label\"] = new_df[\"3l\"].apply(addLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completed labeling for predicting n days out\n",
    "#Start normalizing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#sc = MinMaxScaler(feature_range = (0, 1))\n",
    "#training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146477063.0 -189846255.0 189846255.0\n",
      "            Date  Adjusted_close   Adj_open   Adj_high    Adj_low  \\\n",
      "7550  2010-11-16          9.2622   9.389037   9.446774   9.192486   \n",
      "7551  2010-11-17          9.2288   9.250298   9.335983   9.144651   \n",
      "7552  2010-11-18          9.4723   9.373102   9.510382   9.357440   \n",
      "7553  2010-11-19          9.4201   9.458182   9.471388   9.374340   \n",
      "7554  2010-11-22          9.6236   9.418450   9.623600   9.393575   \n",
      "...          ...             ...        ...        ...        ...   \n",
      "9871  2020-02-07         79.2575  79.837016  80.092102  78.754757   \n",
      "9872  2020-02-10         79.6339  77.808673  79.633900  77.726946   \n",
      "9873  2020-02-11         79.1534  80.141548  80.215845  78.930509   \n",
      "9874  2020-02-12         81.0332  79.614128  81.038153  79.614128   \n",
      "9875  2020-02-13         80.4561  80.287694  80.790436  80.079662   \n",
      "\n",
      "        Adj_volume   Close    Volume     Tx_amount         Epoch  ...  \\\n",
      "7550  7.662964e+08  301.59  23533906  7.097591e+09  1.289884e+09  ...   \n",
      "7551  5.575505e+08  300.50  17123201  5.145522e+09  1.289970e+09  ...   \n",
      "7552  5.750432e+08  308.43  17660349  5.446981e+09  1.290056e+09  ...   \n",
      "7553  4.475345e+08  306.73  13744399  4.215820e+09  1.290143e+09  ...   \n",
      "7554  4.571116e+08  313.36  14038356  4.399059e+09  1.290402e+09  ...   \n",
      "...            ...     ...       ...           ...           ...  ...   \n",
      "9871  1.187977e+08  320.03  29421012  9.415606e+09  1.581052e+09  ...   \n",
      "9872  1.103837e+08  321.55  27337215  8.790281e+09  1.581311e+09  ...   \n",
      "9873  9.521578e+07  319.61  23580780  7.536653e+09  1.581397e+09  ...   \n",
      "9874  1.148065e+08  327.20  28432573  9.303138e+09  1.581484e+09  ...   \n",
      "9875  9.564422e+07  324.87  23686892  7.695161e+09  1.581570e+09  ...   \n",
      "\n",
      "      past1d_ch_p  past1d_ch_v Directional_volume label  n_Adjusted_close  \\\n",
      "7550    -0.017732     0.632664        -23533906.0     1          0.114301   \n",
      "7551    -0.003606    -0.272403        -17123201.0     2          0.113889   \n",
      "7552     0.026385     0.031370         17660349.0     0          0.116894   \n",
      "7553    -0.005511    -0.221737        -13744399.0     2          0.116250   \n",
      "7554     0.021603     0.021387         14038356.0     0          0.118761   \n",
      "...           ...          ...                ...   ...               ...   \n",
      "9871    -0.013592     0.116276        -29421012.0     1          0.978087   \n",
      "9872     0.004749    -0.070827         27337215.0     0          0.982732   \n",
      "9873    -0.006034    -0.137411        -23580780.0     2          0.976802   \n",
      "9874     0.023749     0.205752         28432573.0     0          1.000000   \n",
      "9875    -0.007122    -0.166910        -23686892.0     1          0.992878   \n",
      "\n",
      "      n_Adjusted_open  n_Adjusted_high  n_Adjusted_low  n_Adjusted_volume  \\\n",
      "7550         0.115867         0.116579        0.113441           0.350322   \n",
      "7551         0.114154         0.115212        0.112851           0.254891   \n",
      "7552         0.115670         0.117364        0.115477           0.262888   \n",
      "7553         0.116720         0.116883        0.115685           0.204596   \n",
      "7554         0.116230         0.118761        0.115923           0.208974   \n",
      "...               ...              ...             ...                ...   \n",
      "9871         0.985238         0.988386        0.971883           0.054310   \n",
      "9872         0.960207         0.982732        0.959199           0.050463   \n",
      "9873         0.988996         0.989913        0.974051           0.043529   \n",
      "9874         0.982488         1.000061        0.982488           0.052485   \n",
      "9875         0.990800         0.997004        0.988233           0.043725   \n",
      "\n",
      "      n_Directional_volume  \n",
      "7550             -0.123963  \n",
      "7551             -0.090195  \n",
      "7552              0.093024  \n",
      "7553             -0.072398  \n",
      "7554              0.073946  \n",
      "...                    ...  \n",
      "9871             -0.154973  \n",
      "9872              0.143997  \n",
      "9873             -0.124210  \n",
      "9874              0.149766  \n",
      "9875             -0.124769  \n",
      "\n",
      "[2326 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "max_close = new_df[\"Adjusted_close\"].max()\n",
    "new_df[\"n_Adjusted_close\"] = new_df[\"Adjusted_close\"]/max_close\n",
    "#new_df[\"n_Adjusted_close\"] = new_df[\"Adjusted_close\"]\n",
    "new_df[\"n_Adjusted_open\"] = new_df[\"Adj_open\"]/max_close\n",
    "#new_df[\"n_Adjusted_open\"] = new_df[\"Adj_open\"]\n",
    "new_df[\"n_Adjusted_high\"] = new_df[\"Adj_high\"]/max_close\n",
    "#new_df[\"n_Adjusted_high\"] = new_df[\"Adj_high\"]\n",
    "new_df[\"n_Adjusted_low\"] = new_df[\"Adj_low\"]/max_close\n",
    "#new_df[\"n_Adjusted_low\"] = new_df[\"Adj_low\"]\n",
    "new_df[\"n_Adjusted_volume\"] = new_df[\"Adj_volume\"]/new_df[\"Adj_volume\"].max()\n",
    "#new_df[\"n_Adjusted_volume\"] = new_df[\"Adj_volume\"]\n",
    "#new_df[\"n_Tx_amount\"] = new_df[\"Tx_amount\"]/new_df[\"Tx_amount\"].max()\n",
    "max_dir_vol = new_df[\"Directional_volume\"].max()\n",
    "min_dir_vol = new_df[\"Directional_volume\"].min()\n",
    "n_factor = max(max_dir_vol, -min_dir_vol)\n",
    "new_df[\"n_Directional_volume\"] = new_df[\"Directional_volume\"]/n_factor\n",
    "print(max_dir_vol, min_dir_vol, n_factor)\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price_df = new_df[[\"n_Adjusted_close\", \"n_Adjusted_open\", \"n_Adjusted_high\", \"n_Adjusted_low\", \"n_Adjusted_volume\", \"n_Tx_amount\"]]\n",
    "price_df = new_df[[\"n_Adjusted_close\", \"n_Adjusted_open\", \"n_Adjusted_high\", \"n_Adjusted_low\"]]\n",
    "volume_df = new_df[[\"n_Adjusted_volume\", \"n_Directional_volume\"]]\n",
    "#us_price_df = new_df[[\"n_Adjusted_close\", \"n_Adjusted_open\", \"n_Adjusted_high\", \"n_Adjusted_low\", \"n_Adjusted_volume\"]]\n",
    "#price_df = sc.fit_transform(us_price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_seq_size = 40\n",
    "n = lstm_seq_size\n",
    "price_lstm_ = []\n",
    "volume_lstm_ = []\n",
    "\n",
    "for i in range(n-1, price_df.shape[0]):\n",
    "    price_lstm_.append(price_df[i-n+1:i+1])\n",
    "    volume_lstm_.append(volume_df[i-n+1:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_lstm_np = np.array(price_lstm_)\n",
    "volume_lstm_np = np.array(volume_lstm_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.1143013  0.11586655 0.11657906 0.11344098]\n",
      "  [0.11388912 0.11415442 0.11521183 0.11285067]\n",
      "  [0.11689406 0.1156699  0.11736402 0.11547661]\n",
      "  ...\n",
      "  [0.12978878 0.12841492 0.1300825  0.12778579]\n",
      "  [0.12948026 0.13070821 0.13073853 0.12865784]\n",
      "  [0.13053292 0.1300895  0.13053671 0.12961575]]\n",
      "\n",
      " [[0.11388912 0.11415442 0.11521183 0.11285067]\n",
      "  [0.11689406 0.1156699  0.11736402 0.11547661]\n",
      "  [0.11624988 0.11671984 0.11688281 0.11568518]\n",
      "  ...\n",
      "  [0.12948026 0.13070821 0.13073853 0.12865784]\n",
      "  [0.13053292 0.1300895  0.13053671 0.12961575]\n",
      "  [0.1310105  0.13081342 0.13137433 0.13031694]]\n",
      "\n",
      " [[0.11689406 0.1156699  0.11736402 0.11547661]\n",
      "  [0.11624988 0.11671984 0.11688281 0.11568518]\n",
      "  [0.1187612  0.11622953 0.1187612  0.11592254]\n",
      "  ...\n",
      "  [0.13053292 0.1300895  0.13053671 0.12961575]\n",
      "  [0.1310105  0.13081342 0.13137433 0.13031694]\n",
      "  [0.13207179 0.1310902  0.13207179 0.13054066]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.83893145 0.82768065 0.8393888  0.82606468]\n",
      "  [0.85329223 0.8445721  0.8561278  0.84451112]\n",
      "  [0.85496932 0.85240816 0.85911596 0.85006044]\n",
      "  ...\n",
      "  [0.97808676 0.98523835 0.98838627 0.9718826 ]\n",
      "  [0.98273177 0.96020733 0.98273177 0.95919877]\n",
      "  [0.9768021  0.98899646 0.98991333 0.97405149]]\n",
      "\n",
      " [[0.85329223 0.8445721  0.8561278  0.84451112]\n",
      "  [0.85496932 0.85240816 0.85911596 0.85006044]\n",
      "  [0.85292695 0.85310989 0.85951279 0.85103657]\n",
      "  ...\n",
      "  [0.98273177 0.96020733 0.98273177 0.95919877]\n",
      "  [0.9768021  0.98899646 0.98991333 0.97405149]\n",
      "  [1.         0.98248778 1.00006112 0.98248778]]\n",
      "\n",
      " [[0.85496932 0.85240816 0.85911596 0.85006044]\n",
      "  [0.85292695 0.85310989 0.85951279 0.85103657]\n",
      "  [0.85377969 0.85219421 0.85731652 0.85051726]\n",
      "  ...\n",
      "  [0.9768021  0.98899646 0.98991333 0.97405149]\n",
      "  [1.         0.98248778 1.00006112 0.98248778]\n",
      "  [0.99287823 0.99079999 0.99700414 0.98823275]]]\n"
     ]
    }
   ],
   "source": [
    "print(price_lstm_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.35032175 -0.12396297]\n",
      "  [ 0.254891   -0.09019509]\n",
      "  [ 0.262888    0.09302448]\n",
      "  ...\n",
      "  [ 0.23847024  0.08438368]\n",
      "  [ 0.23608892 -0.08354128]\n",
      "  [ 0.16086215  0.05692148]]\n",
      "\n",
      " [[ 0.254891   -0.09019509]\n",
      "  [ 0.262888    0.09302448]\n",
      "  [ 0.20459584 -0.07239753]\n",
      "  ...\n",
      "  [ 0.23608892 -0.08354128]\n",
      "  [ 0.16086215  0.05692148]\n",
      "  [ 0.15850558  0.05608763]]\n",
      "\n",
      " [[ 0.262888    0.09302448]\n",
      "  [ 0.20459584 -0.07239753]\n",
      "  [ 0.20897414  0.07394592]\n",
      "  ...\n",
      "  [ 0.16086215  0.05692148]\n",
      "  [ 0.15850558  0.05608763]\n",
      "  [ 0.16419094  0.05809946]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.06186189  0.17610464]\n",
      "  [ 0.05936079  0.16898466]\n",
      "  [ 0.05287479  0.15052074]\n",
      "  ...\n",
      "  [ 0.0543098  -0.15497283]\n",
      "  [ 0.05046324  0.1439966 ]\n",
      "  [ 0.04352906 -0.12420988]]\n",
      "\n",
      " [[ 0.05936079  0.16898466]\n",
      "  [ 0.05287479  0.15052074]\n",
      "  [ 0.05370536 -0.15288522]\n",
      "  ...\n",
      "  [ 0.05046324  0.1439966 ]\n",
      "  [ 0.04352906 -0.12420988]\n",
      "  [ 0.05248519  0.14976631]]\n",
      "\n",
      " [[ 0.05287479  0.15052074]\n",
      "  [ 0.05370536 -0.15288522]\n",
      "  [ 0.04556812  0.12972048]\n",
      "  ...\n",
      "  [ 0.04352906 -0.12420988]\n",
      "  [ 0.05248519  0.14976631]\n",
      "  [ 0.04372492 -0.12476881]]]\n"
     ]
    }
   ],
   "source": [
    "print(volume_lstm_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2287, 40, 4)\n",
      "(2287, 40, 2)\n"
     ]
    }
   ],
   "source": [
    "print(price_lstm_np.shape)\n",
    "print(volume_lstm_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular MLP input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"1d_ushift\"] = new_df[\"Adjusted_close\"]\n",
    "new_df[\"1d_ushift\"] = new_df[\"1d_ushift\"].shift(1)\n",
    "\n",
    "new_df[\"2d_ushift\"] = new_df[\"Adjusted_close\"]\n",
    "new_df[\"2d_ushift\"] = new_df[\"2d_ushift\"].shift(2)\n",
    "\n",
    "new_df[\"3d_ushift\"] = new_df[\"Adjusted_close\"]\n",
    "new_df[\"3d_ushift\"] = new_df[\"3d_ushift\"].shift(3)\n",
    "\n",
    "new_df[\"7d_ushift\"] = new_df[\"Adjusted_close\"]\n",
    "new_df[\"7d_ushift\"] = new_df[\"7d_ushift\"].shift(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"RSI9\"] = np.nan\n",
    "new_df[\"RSI14\"] = np.nan\n",
    "new_df[\"RSI20\"] = np.nan\n",
    "new_df[\"VWMA9\"] = np.nan\n",
    "new_df[\"VWMA14\"] = np.nan\n",
    "new_df[\"VWMA20\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tulipy as ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_periods = [9, 14, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63.34060342 68.52740394 70.11499043 ... 55.37596415 63.79529547\n",
      " 59.89252231]\n",
      "[ 9.48198674  9.52897072  9.58885274 ... 78.5126666  78.59457488\n",
      " 79.15839775]\n",
      "[67.24778701 69.17677215 67.26193808 ... 58.29990951 63.66434421\n",
      " 61.06706794]\n",
      "[ 9.56651114  9.61355179  9.65879919 ... 78.53888066 78.66434428\n",
      " 78.75370602]\n",
      "[66.87551236 67.42564295 66.59287065 ... 60.67850244 64.47452992\n",
      " 62.52395967]\n",
      "[ 9.63328779  9.67422597  9.70903836 ... 78.35912893 78.54039914\n",
      " 78.68374265]\n"
     ]
    }
   ],
   "source": [
    "for n in ti_periods:\n",
    "    rsi_calc = ti.rsi(new_df[\"Adjusted_close\"].to_numpy(), period=n)\n",
    "    vwma_calc = ti.vwma(new_df[\"Adjusted_close\"].astype(float).to_numpy(), new_df[\"Volume\"].astype(float).to_numpy(), period=n)\n",
    "    print(rsi_calc)\n",
    "    print(vwma_calc)\n",
    "    for i in range(n-1, new_df.shape[0]-1):\n",
    "        col_name = \"RSI\" + str(n)\n",
    "        new_df.loc[new_df.index[i], col_name] = rsi_calc[i-n+1]\n",
    "        col_name = \"VWMA\" + str(n)\n",
    "        new_df.loc[new_df.index[i], col_name] = vwma_calc[i-n+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Adjusted_close   Adj_open   Adj_high    Adj_low  \\\n",
      "7550  2010-11-16          9.2622   9.389037   9.446774   9.192486   \n",
      "7551  2010-11-17          9.2288   9.250298   9.335983   9.144651   \n",
      "7552  2010-11-18          9.4723   9.373102   9.510382   9.357440   \n",
      "7553  2010-11-19          9.4201   9.458182   9.471388   9.374340   \n",
      "7554  2010-11-22          9.6236   9.418450   9.623600   9.393575   \n",
      "...          ...             ...        ...        ...        ...   \n",
      "9871  2020-02-07         79.2575  79.837016  80.092102  78.754757   \n",
      "9872  2020-02-10         79.6339  77.808673  79.633900  77.726946   \n",
      "9873  2020-02-11         79.1534  80.141548  80.215845  78.930509   \n",
      "9874  2020-02-12         81.0332  79.614128  81.038153  79.614128   \n",
      "9875  2020-02-13         80.4561  80.287694  80.790436  80.079662   \n",
      "\n",
      "        Adj_volume   Close    Volume     Tx_amount         Epoch  ...  \\\n",
      "7550  7.662964e+08  301.59  23533906  7.097591e+09  1.289884e+09  ...   \n",
      "7551  5.575505e+08  300.50  17123201  5.145522e+09  1.289970e+09  ...   \n",
      "7552  5.750432e+08  308.43  17660349  5.446981e+09  1.290056e+09  ...   \n",
      "7553  4.475345e+08  306.73  13744399  4.215820e+09  1.290143e+09  ...   \n",
      "7554  4.571116e+08  313.36  14038356  4.399059e+09  1.290402e+09  ...   \n",
      "...            ...     ...       ...           ...           ...  ...   \n",
      "9871  1.187977e+08  320.03  29421012  9.415606e+09  1.581052e+09  ...   \n",
      "9872  1.103837e+08  321.55  27337215  8.790281e+09  1.581311e+09  ...   \n",
      "9873  9.521578e+07  319.61  23580780  7.536653e+09  1.581397e+09  ...   \n",
      "9874  1.148065e+08  327.20  28432573  9.303138e+09  1.581484e+09  ...   \n",
      "9875  9.564422e+07  324.87  23686892  7.695161e+09  1.581570e+09  ...   \n",
      "\n",
      "      1d_ushift  2d_ushift 3d_ushift 7d_ushift       RSI9      RSI14  \\\n",
      "7550        NaN        NaN       NaN       NaN        NaN        NaN   \n",
      "7551     9.2622        NaN       NaN       NaN        NaN        NaN   \n",
      "7552     9.2288     9.2622       NaN       NaN        NaN        NaN   \n",
      "7553     9.4723     9.2288    9.2622       NaN        NaN        NaN   \n",
      "7554     9.4201     9.4723    9.2288       NaN        NaN        NaN   \n",
      "...         ...        ...       ...       ...        ...        ...   \n",
      "9871    80.3496    79.4206   78.7783   80.1347  58.465111  60.417039   \n",
      "9872    79.2575    80.3496   79.4206   80.0186  55.375964  58.299910   \n",
      "9873    79.6339    79.2575   80.3496   76.4706  63.795295  63.664344   \n",
      "9874    79.1534    79.6339   79.2575   76.2606  59.892522  61.067068   \n",
      "9875    81.0332    79.1534   79.6339   78.7783        NaN        NaN   \n",
      "\n",
      "          RSI20      VWMA9     VWMA14     VWMA20  \n",
      "7550        NaN        NaN        NaN        NaN  \n",
      "7551        NaN        NaN        NaN        NaN  \n",
      "7552        NaN        NaN        NaN        NaN  \n",
      "7553        NaN        NaN        NaN        NaN  \n",
      "7554        NaN        NaN        NaN        NaN  \n",
      "...         ...        ...        ...        ...  \n",
      "9871  62.294897  78.634113  78.428877  78.194357  \n",
      "9872  60.678502  78.735606  78.507332  78.329719  \n",
      "9873  64.474530  78.512667  78.538881  78.359129  \n",
      "9874  62.523960  78.594575  78.664344  78.540399  \n",
      "9875        NaN        NaN        NaN        NaN  \n",
      "\n",
      "[2326 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Adjusted_close</th>\n",
       "      <th>Adj_open</th>\n",
       "      <th>Adj_high</th>\n",
       "      <th>Adj_low</th>\n",
       "      <th>Adj_volume</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Tx_amount</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>...</th>\n",
       "      <th>1d_ushift</th>\n",
       "      <th>2d_ushift</th>\n",
       "      <th>3d_ushift</th>\n",
       "      <th>7d_ushift</th>\n",
       "      <th>RSI9</th>\n",
       "      <th>RSI14</th>\n",
       "      <th>RSI20</th>\n",
       "      <th>VWMA9</th>\n",
       "      <th>VWMA14</th>\n",
       "      <th>VWMA20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7550</th>\n",
       "      <td>2010-11-16</td>\n",
       "      <td>9.2622</td>\n",
       "      <td>9.389037</td>\n",
       "      <td>9.446774</td>\n",
       "      <td>9.192486</td>\n",
       "      <td>7.662964e+08</td>\n",
       "      <td>301.5900</td>\n",
       "      <td>23533906</td>\n",
       "      <td>7.097591e+09</td>\n",
       "      <td>1.289884e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7551</th>\n",
       "      <td>2010-11-17</td>\n",
       "      <td>9.2288</td>\n",
       "      <td>9.250298</td>\n",
       "      <td>9.335983</td>\n",
       "      <td>9.144651</td>\n",
       "      <td>5.575505e+08</td>\n",
       "      <td>300.5000</td>\n",
       "      <td>17123201</td>\n",
       "      <td>5.145522e+09</td>\n",
       "      <td>1.289970e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7552</th>\n",
       "      <td>2010-11-18</td>\n",
       "      <td>9.4723</td>\n",
       "      <td>9.373102</td>\n",
       "      <td>9.510382</td>\n",
       "      <td>9.357440</td>\n",
       "      <td>5.750432e+08</td>\n",
       "      <td>308.4300</td>\n",
       "      <td>17660349</td>\n",
       "      <td>5.446981e+09</td>\n",
       "      <td>1.290056e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2288</td>\n",
       "      <td>9.2622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7553</th>\n",
       "      <td>2010-11-19</td>\n",
       "      <td>9.4201</td>\n",
       "      <td>9.458182</td>\n",
       "      <td>9.471388</td>\n",
       "      <td>9.374340</td>\n",
       "      <td>4.475345e+08</td>\n",
       "      <td>306.7300</td>\n",
       "      <td>13744399</td>\n",
       "      <td>4.215820e+09</td>\n",
       "      <td>1.290143e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.4723</td>\n",
       "      <td>9.2288</td>\n",
       "      <td>9.2622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7554</th>\n",
       "      <td>2010-11-22</td>\n",
       "      <td>9.6236</td>\n",
       "      <td>9.418450</td>\n",
       "      <td>9.623600</td>\n",
       "      <td>9.393575</td>\n",
       "      <td>4.571116e+08</td>\n",
       "      <td>313.3600</td>\n",
       "      <td>14038356</td>\n",
       "      <td>4.399059e+09</td>\n",
       "      <td>1.290402e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.4201</td>\n",
       "      <td>9.4723</td>\n",
       "      <td>9.2288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7555</th>\n",
       "      <td>2010-11-23</td>\n",
       "      <td>9.4815</td>\n",
       "      <td>9.534323</td>\n",
       "      <td>9.574248</td>\n",
       "      <td>9.414856</td>\n",
       "      <td>6.040647e+08</td>\n",
       "      <td>308.7300</td>\n",
       "      <td>18551612</td>\n",
       "      <td>5.727439e+09</td>\n",
       "      <td>1.290488e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.6236</td>\n",
       "      <td>9.4201</td>\n",
       "      <td>9.4723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7556</th>\n",
       "      <td>2010-11-24</td>\n",
       "      <td>9.6678</td>\n",
       "      <td>9.581962</td>\n",
       "      <td>9.686380</td>\n",
       "      <td>9.574284</td>\n",
       "      <td>4.817573e+08</td>\n",
       "      <td>314.7950</td>\n",
       "      <td>14795448</td>\n",
       "      <td>4.657533e+09</td>\n",
       "      <td>1.290575e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.4815</td>\n",
       "      <td>9.6236</td>\n",
       "      <td>9.4201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7557</th>\n",
       "      <td>2010-11-26</td>\n",
       "      <td>9.6974</td>\n",
       "      <td>9.635363</td>\n",
       "      <td>9.756977</td>\n",
       "      <td>9.610794</td>\n",
       "      <td>2.764745e+08</td>\n",
       "      <td>315.7600</td>\n",
       "      <td>8490891</td>\n",
       "      <td>2.681084e+09</td>\n",
       "      <td>1.290748e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.6678</td>\n",
       "      <td>9.4815</td>\n",
       "      <td>9.6236</td>\n",
       "      <td>9.2622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7558</th>\n",
       "      <td>2010-11-29</td>\n",
       "      <td>9.7315</td>\n",
       "      <td>9.689425</td>\n",
       "      <td>9.750234</td>\n",
       "      <td>9.562895</td>\n",
       "      <td>5.183749e+08</td>\n",
       "      <td>316.8700</td>\n",
       "      <td>15919985</td>\n",
       "      <td>5.044566e+09</td>\n",
       "      <td>1.291007e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.6974</td>\n",
       "      <td>9.6678</td>\n",
       "      <td>9.4815</td>\n",
       "      <td>9.2288</td>\n",
       "      <td>63.340603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.481987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7559</th>\n",
       "      <td>2010-11-30</td>\n",
       "      <td>9.5558</td>\n",
       "      <td>9.629200</td>\n",
       "      <td>9.654383</td>\n",
       "      <td>9.547201</td>\n",
       "      <td>5.836083e+08</td>\n",
       "      <td>311.1500</td>\n",
       "      <td>17923329</td>\n",
       "      <td>5.576844e+09</td>\n",
       "      <td>1.291093e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7315</td>\n",
       "      <td>9.6974</td>\n",
       "      <td>9.6678</td>\n",
       "      <td>9.4723</td>\n",
       "      <td>68.527404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.528971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7560</th>\n",
       "      <td>2010-12-01</td>\n",
       "      <td>9.7170</td>\n",
       "      <td>9.682296</td>\n",
       "      <td>9.758460</td>\n",
       "      <td>9.674004</td>\n",
       "      <td>5.369747e+08</td>\n",
       "      <td>316.4000</td>\n",
       "      <td>16491096</td>\n",
       "      <td>5.217783e+09</td>\n",
       "      <td>1.291180e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5558</td>\n",
       "      <td>9.7315</td>\n",
       "      <td>9.6974</td>\n",
       "      <td>9.4201</td>\n",
       "      <td>70.114990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.588853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>2010-12-02</td>\n",
       "      <td>9.7708</td>\n",
       "      <td>9.751759</td>\n",
       "      <td>9.796905</td>\n",
       "      <td>9.670681</td>\n",
       "      <td>5.391823e+08</td>\n",
       "      <td>318.1500</td>\n",
       "      <td>16558988</td>\n",
       "      <td>5.268242e+09</td>\n",
       "      <td>1.291266e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7170</td>\n",
       "      <td>9.5558</td>\n",
       "      <td>9.7315</td>\n",
       "      <td>9.6236</td>\n",
       "      <td>68.538944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.626001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7562</th>\n",
       "      <td>2010-12-03</td>\n",
       "      <td>9.7490</td>\n",
       "      <td>9.735794</td>\n",
       "      <td>9.786161</td>\n",
       "      <td>9.715218</td>\n",
       "      <td>3.981421e+08</td>\n",
       "      <td>317.4400</td>\n",
       "      <td>12227469</td>\n",
       "      <td>3.881488e+09</td>\n",
       "      <td>1.291352e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7708</td>\n",
       "      <td>9.7170</td>\n",
       "      <td>9.5558</td>\n",
       "      <td>9.4815</td>\n",
       "      <td>71.308038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.658105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7563</th>\n",
       "      <td>2010-12-06</td>\n",
       "      <td>9.8322</td>\n",
       "      <td>9.785826</td>\n",
       "      <td>9.899150</td>\n",
       "      <td>9.779070</td>\n",
       "      <td>5.215409e+08</td>\n",
       "      <td>320.1500</td>\n",
       "      <td>16017161</td>\n",
       "      <td>5.127894e+09</td>\n",
       "      <td>1.291612e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7490</td>\n",
       "      <td>9.7708</td>\n",
       "      <td>9.7170</td>\n",
       "      <td>9.6678</td>\n",
       "      <td>66.592453</td>\n",
       "      <td>67.247787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.681999</td>\n",
       "      <td>9.566511</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7564</th>\n",
       "      <td>2010-12-07</td>\n",
       "      <td>9.7727</td>\n",
       "      <td>9.944377</td>\n",
       "      <td>9.950212</td>\n",
       "      <td>9.769936</td>\n",
       "      <td>4.552205e+08</td>\n",
       "      <td>318.2100</td>\n",
       "      <td>13980495</td>\n",
       "      <td>4.448733e+09</td>\n",
       "      <td>1.291698e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8322</td>\n",
       "      <td>9.7490</td>\n",
       "      <td>9.7708</td>\n",
       "      <td>9.6974</td>\n",
       "      <td>69.835999</td>\n",
       "      <td>69.176772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.719668</td>\n",
       "      <td>9.613552</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565</th>\n",
       "      <td>2010-12-08</td>\n",
       "      <td>9.8587</td>\n",
       "      <td>9.816318</td>\n",
       "      <td>9.859007</td>\n",
       "      <td>9.738925</td>\n",
       "      <td>3.743758e+08</td>\n",
       "      <td>321.0100</td>\n",
       "      <td>11497643</td>\n",
       "      <td>3.690858e+09</td>\n",
       "      <td>1.291784e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7727</td>\n",
       "      <td>9.8322</td>\n",
       "      <td>9.7490</td>\n",
       "      <td>9.7315</td>\n",
       "      <td>66.572298</td>\n",
       "      <td>67.261938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.737994</td>\n",
       "      <td>9.658799</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7566</th>\n",
       "      <td>2010-12-09</td>\n",
       "      <td>9.8201</td>\n",
       "      <td>9.893014</td>\n",
       "      <td>9.904325</td>\n",
       "      <td>9.797451</td>\n",
       "      <td>3.420660e+08</td>\n",
       "      <td>319.7575</td>\n",
       "      <td>10505218</td>\n",
       "      <td>3.359122e+09</td>\n",
       "      <td>1.291871e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8587</td>\n",
       "      <td>9.7727</td>\n",
       "      <td>9.8322</td>\n",
       "      <td>9.5558</td>\n",
       "      <td>67.660299</td>\n",
       "      <td>67.874729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.747201</td>\n",
       "      <td>9.683648</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>2010-12-10</td>\n",
       "      <td>9.8448</td>\n",
       "      <td>9.816853</td>\n",
       "      <td>9.859849</td>\n",
       "      <td>9.784606</td>\n",
       "      <td>3.053522e+08</td>\n",
       "      <td>320.5600</td>\n",
       "      <td>9377749</td>\n",
       "      <td>3.006131e+09</td>\n",
       "      <td>1.291957e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8201</td>\n",
       "      <td>9.8587</td>\n",
       "      <td>9.7727</td>\n",
       "      <td>9.7170</td>\n",
       "      <td>69.216448</td>\n",
       "      <td>68.744544</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.756554</td>\n",
       "      <td>9.709789</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>2010-12-13</td>\n",
       "      <td>9.8789</td>\n",
       "      <td>9.961820</td>\n",
       "      <td>9.983011</td>\n",
       "      <td>9.858323</td>\n",
       "      <td>5.114608e+08</td>\n",
       "      <td>321.6700</td>\n",
       "      <td>15707619</td>\n",
       "      <td>5.052670e+09</td>\n",
       "      <td>1.292216e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8448</td>\n",
       "      <td>9.8201</td>\n",
       "      <td>9.8587</td>\n",
       "      <td>9.7708</td>\n",
       "      <td>64.851309</td>\n",
       "      <td>66.339363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.801665</td>\n",
       "      <td>9.729311</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7569</th>\n",
       "      <td>2010-12-14</td>\n",
       "      <td>9.8365</td>\n",
       "      <td>9.880724</td>\n",
       "      <td>9.905600</td>\n",
       "      <td>9.796883</td>\n",
       "      <td>4.088396e+08</td>\n",
       "      <td>320.2900</td>\n",
       "      <td>12555966</td>\n",
       "      <td>4.021550e+09</td>\n",
       "      <td>1.292303e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8789</td>\n",
       "      <td>9.8448</td>\n",
       "      <td>9.8201</td>\n",
       "      <td>9.7490</td>\n",
       "      <td>64.974387</td>\n",
       "      <td>66.402062</td>\n",
       "      <td>66.875512</td>\n",
       "      <td>9.817148</td>\n",
       "      <td>9.760258</td>\n",
       "      <td>9.633288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7570</th>\n",
       "      <td>2010-12-15</td>\n",
       "      <td>9.8386</td>\n",
       "      <td>9.827544</td>\n",
       "      <td>9.919677</td>\n",
       "      <td>9.802668</td>\n",
       "      <td>4.852941e+08</td>\n",
       "      <td>320.3600</td>\n",
       "      <td>14903902</td>\n",
       "      <td>4.774614e+09</td>\n",
       "      <td>1.292389e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8365</td>\n",
       "      <td>9.8789</td>\n",
       "      <td>9.8448</td>\n",
       "      <td>9.8322</td>\n",
       "      <td>66.686661</td>\n",
       "      <td>67.258993</td>\n",
       "      <td>67.425643</td>\n",
       "      <td>9.826458</td>\n",
       "      <td>9.773453</td>\n",
       "      <td>9.674226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7571</th>\n",
       "      <td>2010-12-16</td>\n",
       "      <td>9.8660</td>\n",
       "      <td>9.861086</td>\n",
       "      <td>9.907767</td>\n",
       "      <td>9.830682</td>\n",
       "      <td>3.744882e+08</td>\n",
       "      <td>321.2500</td>\n",
       "      <td>11501014</td>\n",
       "      <td>3.694701e+09</td>\n",
       "      <td>1.292476e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8386</td>\n",
       "      <td>9.8365</td>\n",
       "      <td>9.8789</td>\n",
       "      <td>9.7727</td>\n",
       "      <td>64.162445</td>\n",
       "      <td>65.962938</td>\n",
       "      <td>66.592871</td>\n",
       "      <td>9.838538</td>\n",
       "      <td>9.782215</td>\n",
       "      <td>9.709038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7572</th>\n",
       "      <td>2010-12-17</td>\n",
       "      <td>9.8464</td>\n",
       "      <td>9.877726</td>\n",
       "      <td>9.882640</td>\n",
       "      <td>9.834730</td>\n",
       "      <td>4.502139e+08</td>\n",
       "      <td>320.6100</td>\n",
       "      <td>13826724</td>\n",
       "      <td>4.432986e+09</td>\n",
       "      <td>1.292562e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8660</td>\n",
       "      <td>9.8386</td>\n",
       "      <td>9.8365</td>\n",
       "      <td>9.8587</td>\n",
       "      <td>67.616931</td>\n",
       "      <td>67.644936</td>\n",
       "      <td>67.646586</td>\n",
       "      <td>9.840385</td>\n",
       "      <td>9.790993</td>\n",
       "      <td>9.730861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7573</th>\n",
       "      <td>2010-12-20</td>\n",
       "      <td>9.8955</td>\n",
       "      <td>9.876766</td>\n",
       "      <td>9.927440</td>\n",
       "      <td>9.773269</td>\n",
       "      <td>4.484253e+08</td>\n",
       "      <td>322.2100</td>\n",
       "      <td>13771742</td>\n",
       "      <td>4.437393e+09</td>\n",
       "      <td>1.292821e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8464</td>\n",
       "      <td>9.8660</td>\n",
       "      <td>9.8386</td>\n",
       "      <td>9.8201</td>\n",
       "      <td>71.478384</td>\n",
       "      <td>69.660712</td>\n",
       "      <td>68.934303</td>\n",
       "      <td>9.855390</td>\n",
       "      <td>9.820925</td>\n",
       "      <td>9.754327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7574</th>\n",
       "      <td>2010-12-21</td>\n",
       "      <td>9.9568</td>\n",
       "      <td>9.919793</td>\n",
       "      <td>9.962482</td>\n",
       "      <td>9.890617</td>\n",
       "      <td>2.981109e+08</td>\n",
       "      <td>324.2050</td>\n",
       "      <td>9155412</td>\n",
       "      <td>2.968230e+09</td>\n",
       "      <td>1.292908e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.8955</td>\n",
       "      <td>9.8464</td>\n",
       "      <td>9.8660</td>\n",
       "      <td>9.8448</td>\n",
       "      <td>73.196989</td>\n",
       "      <td>70.603445</td>\n",
       "      <td>69.544196</td>\n",
       "      <td>9.863389</td>\n",
       "      <td>9.837213</td>\n",
       "      <td>9.767802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7575</th>\n",
       "      <td>2010-12-22</td>\n",
       "      <td>9.9861</td>\n",
       "      <td>9.961531</td>\n",
       "      <td>10.003298</td>\n",
       "      <td>9.936655</td>\n",
       "      <td>3.094996e+08</td>\n",
       "      <td>325.1600</td>\n",
       "      <td>9505147</td>\n",
       "      <td>3.090694e+09</td>\n",
       "      <td>1.292994e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.9568</td>\n",
       "      <td>9.8955</td>\n",
       "      <td>9.8464</td>\n",
       "      <td>9.8789</td>\n",
       "      <td>65.894492</td>\n",
       "      <td>66.941346</td>\n",
       "      <td>67.271465</td>\n",
       "      <td>9.878086</td>\n",
       "      <td>9.851623</td>\n",
       "      <td>9.795705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7576</th>\n",
       "      <td>2010-12-23</td>\n",
       "      <td>9.9382</td>\n",
       "      <td>9.981196</td>\n",
       "      <td>9.985803</td>\n",
       "      <td>9.924994</td>\n",
       "      <td>2.599092e+08</td>\n",
       "      <td>323.6000</td>\n",
       "      <td>7982168</td>\n",
       "      <td>2.583030e+09</td>\n",
       "      <td>1.293080e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.9861</td>\n",
       "      <td>9.9568</td>\n",
       "      <td>9.8955</td>\n",
       "      <td>9.8365</td>\n",
       "      <td>68.349242</td>\n",
       "      <td>68.169968</td>\n",
       "      <td>68.031406</td>\n",
       "      <td>9.885358</td>\n",
       "      <td>9.863050</td>\n",
       "      <td>9.807453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7577</th>\n",
       "      <td>2010-12-27</td>\n",
       "      <td>9.9713</td>\n",
       "      <td>9.915157</td>\n",
       "      <td>9.994640</td>\n",
       "      <td>9.874253</td>\n",
       "      <td>2.905119e+08</td>\n",
       "      <td>324.6800</td>\n",
       "      <td>8921957</td>\n",
       "      <td>2.896781e+09</td>\n",
       "      <td>1.293426e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.9382</td>\n",
       "      <td>9.9861</td>\n",
       "      <td>9.9568</td>\n",
       "      <td>9.8386</td>\n",
       "      <td>70.125145</td>\n",
       "      <td>69.078534</td>\n",
       "      <td>68.594923</td>\n",
       "      <td>9.893860</td>\n",
       "      <td>9.871996</td>\n",
       "      <td>9.816729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7578</th>\n",
       "      <td>2010-12-28</td>\n",
       "      <td>9.9956</td>\n",
       "      <td>10.009119</td>\n",
       "      <td>10.032146</td>\n",
       "      <td>9.983008</td>\n",
       "      <td>2.045819e+08</td>\n",
       "      <td>325.4700</td>\n",
       "      <td>6282971</td>\n",
       "      <td>2.044919e+09</td>\n",
       "      <td>1.293512e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.9713</td>\n",
       "      <td>9.9382</td>\n",
       "      <td>9.9861</td>\n",
       "      <td>9.8660</td>\n",
       "      <td>69.137373</td>\n",
       "      <td>68.601233</td>\n",
       "      <td>68.308051</td>\n",
       "      <td>9.908043</td>\n",
       "      <td>9.885918</td>\n",
       "      <td>9.826704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>2010-12-29</td>\n",
       "      <td>9.9901</td>\n",
       "      <td>10.018662</td>\n",
       "      <td>10.025725</td>\n",
       "      <td>9.984265</td>\n",
       "      <td>1.898677e+08</td>\n",
       "      <td>325.2900</td>\n",
       "      <td>5831096</td>\n",
       "      <td>1.896797e+09</td>\n",
       "      <td>1.293599e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>9.9956</td>\n",
       "      <td>9.9713</td>\n",
       "      <td>9.9382</td>\n",
       "      <td>9.8464</td>\n",
       "      <td>60.416387</td>\n",
       "      <td>64.246531</td>\n",
       "      <td>65.674481</td>\n",
       "      <td>9.925483</td>\n",
       "      <td>9.892061</td>\n",
       "      <td>9.851253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Adjusted_close   Adj_open   Adj_high   Adj_low  \\\n",
       "7550  2010-11-16          9.2622   9.389037   9.446774  9.192486   \n",
       "7551  2010-11-17          9.2288   9.250298   9.335983  9.144651   \n",
       "7552  2010-11-18          9.4723   9.373102   9.510382  9.357440   \n",
       "7553  2010-11-19          9.4201   9.458182   9.471388  9.374340   \n",
       "7554  2010-11-22          9.6236   9.418450   9.623600  9.393575   \n",
       "7555  2010-11-23          9.4815   9.534323   9.574248  9.414856   \n",
       "7556  2010-11-24          9.6678   9.581962   9.686380  9.574284   \n",
       "7557  2010-11-26          9.6974   9.635363   9.756977  9.610794   \n",
       "7558  2010-11-29          9.7315   9.689425   9.750234  9.562895   \n",
       "7559  2010-11-30          9.5558   9.629200   9.654383  9.547201   \n",
       "7560  2010-12-01          9.7170   9.682296   9.758460  9.674004   \n",
       "7561  2010-12-02          9.7708   9.751759   9.796905  9.670681   \n",
       "7562  2010-12-03          9.7490   9.735794   9.786161  9.715218   \n",
       "7563  2010-12-06          9.8322   9.785826   9.899150  9.779070   \n",
       "7564  2010-12-07          9.7727   9.944377   9.950212  9.769936   \n",
       "7565  2010-12-08          9.8587   9.816318   9.859007  9.738925   \n",
       "7566  2010-12-09          9.8201   9.893014   9.904325  9.797451   \n",
       "7567  2010-12-10          9.8448   9.816853   9.859849  9.784606   \n",
       "7568  2010-12-13          9.8789   9.961820   9.983011  9.858323   \n",
       "7569  2010-12-14          9.8365   9.880724   9.905600  9.796883   \n",
       "7570  2010-12-15          9.8386   9.827544   9.919677  9.802668   \n",
       "7571  2010-12-16          9.8660   9.861086   9.907767  9.830682   \n",
       "7572  2010-12-17          9.8464   9.877726   9.882640  9.834730   \n",
       "7573  2010-12-20          9.8955   9.876766   9.927440  9.773269   \n",
       "7574  2010-12-21          9.9568   9.919793   9.962482  9.890617   \n",
       "7575  2010-12-22          9.9861   9.961531  10.003298  9.936655   \n",
       "7576  2010-12-23          9.9382   9.981196   9.985803  9.924994   \n",
       "7577  2010-12-27          9.9713   9.915157   9.994640  9.874253   \n",
       "7578  2010-12-28          9.9956  10.009119  10.032146  9.983008   \n",
       "7579  2010-12-29          9.9901  10.018662  10.025725  9.984265   \n",
       "\n",
       "        Adj_volume     Close    Volume     Tx_amount         Epoch  ...  \\\n",
       "7550  7.662964e+08  301.5900  23533906  7.097591e+09  1.289884e+09  ...   \n",
       "7551  5.575505e+08  300.5000  17123201  5.145522e+09  1.289970e+09  ...   \n",
       "7552  5.750432e+08  308.4300  17660349  5.446981e+09  1.290056e+09  ...   \n",
       "7553  4.475345e+08  306.7300  13744399  4.215820e+09  1.290143e+09  ...   \n",
       "7554  4.571116e+08  313.3600  14038356  4.399059e+09  1.290402e+09  ...   \n",
       "7555  6.040647e+08  308.7300  18551612  5.727439e+09  1.290488e+09  ...   \n",
       "7556  4.817573e+08  314.7950  14795448  4.657533e+09  1.290575e+09  ...   \n",
       "7557  2.764745e+08  315.7600   8490891  2.681084e+09  1.290748e+09  ...   \n",
       "7558  5.183749e+08  316.8700  15919985  5.044566e+09  1.291007e+09  ...   \n",
       "7559  5.836083e+08  311.1500  17923329  5.576844e+09  1.291093e+09  ...   \n",
       "7560  5.369747e+08  316.4000  16491096  5.217783e+09  1.291180e+09  ...   \n",
       "7561  5.391823e+08  318.1500  16558988  5.268242e+09  1.291266e+09  ...   \n",
       "7562  3.981421e+08  317.4400  12227469  3.881488e+09  1.291352e+09  ...   \n",
       "7563  5.215409e+08  320.1500  16017161  5.127894e+09  1.291612e+09  ...   \n",
       "7564  4.552205e+08  318.2100  13980495  4.448733e+09  1.291698e+09  ...   \n",
       "7565  3.743758e+08  321.0100  11497643  3.690858e+09  1.291784e+09  ...   \n",
       "7566  3.420660e+08  319.7575  10505218  3.359122e+09  1.291871e+09  ...   \n",
       "7567  3.053522e+08  320.5600   9377749  3.006131e+09  1.291957e+09  ...   \n",
       "7568  5.114608e+08  321.6700  15707619  5.052670e+09  1.292216e+09  ...   \n",
       "7569  4.088396e+08  320.2900  12555966  4.021550e+09  1.292303e+09  ...   \n",
       "7570  4.852941e+08  320.3600  14903902  4.774614e+09  1.292389e+09  ...   \n",
       "7571  3.744882e+08  321.2500  11501014  3.694701e+09  1.292476e+09  ...   \n",
       "7572  4.502139e+08  320.6100  13826724  4.432986e+09  1.292562e+09  ...   \n",
       "7573  4.484253e+08  322.2100  13771742  4.437393e+09  1.292821e+09  ...   \n",
       "7574  2.981109e+08  324.2050   9155412  2.968230e+09  1.292908e+09  ...   \n",
       "7575  3.094996e+08  325.1600   9505147  3.090694e+09  1.292994e+09  ...   \n",
       "7576  2.599092e+08  323.6000   7982168  2.583030e+09  1.293080e+09  ...   \n",
       "7577  2.905119e+08  324.6800   8921957  2.896781e+09  1.293426e+09  ...   \n",
       "7578  2.045819e+08  325.4700   6282971  2.044919e+09  1.293512e+09  ...   \n",
       "7579  1.898677e+08  325.2900   5831096  1.896797e+09  1.293599e+09  ...   \n",
       "\n",
       "      1d_ushift  2d_ushift 3d_ushift 7d_ushift       RSI9      RSI14  \\\n",
       "7550        NaN        NaN       NaN       NaN        NaN        NaN   \n",
       "7551     9.2622        NaN       NaN       NaN        NaN        NaN   \n",
       "7552     9.2288     9.2622       NaN       NaN        NaN        NaN   \n",
       "7553     9.4723     9.2288    9.2622       NaN        NaN        NaN   \n",
       "7554     9.4201     9.4723    9.2288       NaN        NaN        NaN   \n",
       "7555     9.6236     9.4201    9.4723       NaN        NaN        NaN   \n",
       "7556     9.4815     9.6236    9.4201       NaN        NaN        NaN   \n",
       "7557     9.6678     9.4815    9.6236    9.2622        NaN        NaN   \n",
       "7558     9.6974     9.6678    9.4815    9.2288  63.340603        NaN   \n",
       "7559     9.7315     9.6974    9.6678    9.4723  68.527404        NaN   \n",
       "7560     9.5558     9.7315    9.6974    9.4201  70.114990        NaN   \n",
       "7561     9.7170     9.5558    9.7315    9.6236  68.538944        NaN   \n",
       "7562     9.7708     9.7170    9.5558    9.4815  71.308038        NaN   \n",
       "7563     9.7490     9.7708    9.7170    9.6678  66.592453  67.247787   \n",
       "7564     9.8322     9.7490    9.7708    9.6974  69.835999  69.176772   \n",
       "7565     9.7727     9.8322    9.7490    9.7315  66.572298  67.261938   \n",
       "7566     9.8587     9.7727    9.8322    9.5558  67.660299  67.874729   \n",
       "7567     9.8201     9.8587    9.7727    9.7170  69.216448  68.744544   \n",
       "7568     9.8448     9.8201    9.8587    9.7708  64.851309  66.339363   \n",
       "7569     9.8789     9.8448    9.8201    9.7490  64.974387  66.402062   \n",
       "7570     9.8365     9.8789    9.8448    9.8322  66.686661  67.258993   \n",
       "7571     9.8386     9.8365    9.8789    9.7727  64.162445  65.962938   \n",
       "7572     9.8660     9.8386    9.8365    9.8587  67.616931  67.644936   \n",
       "7573     9.8464     9.8660    9.8386    9.8201  71.478384  69.660712   \n",
       "7574     9.8955     9.8464    9.8660    9.8448  73.196989  70.603445   \n",
       "7575     9.9568     9.8955    9.8464    9.8789  65.894492  66.941346   \n",
       "7576     9.9861     9.9568    9.8955    9.8365  68.349242  68.169968   \n",
       "7577     9.9382     9.9861    9.9568    9.8386  70.125145  69.078534   \n",
       "7578     9.9713     9.9382    9.9861    9.8660  69.137373  68.601233   \n",
       "7579     9.9956     9.9713    9.9382    9.8464  60.416387  64.246531   \n",
       "\n",
       "          RSI20     VWMA9    VWMA14    VWMA20  \n",
       "7550        NaN       NaN       NaN       NaN  \n",
       "7551        NaN       NaN       NaN       NaN  \n",
       "7552        NaN       NaN       NaN       NaN  \n",
       "7553        NaN       NaN       NaN       NaN  \n",
       "7554        NaN       NaN       NaN       NaN  \n",
       "7555        NaN       NaN       NaN       NaN  \n",
       "7556        NaN       NaN       NaN       NaN  \n",
       "7557        NaN       NaN       NaN       NaN  \n",
       "7558        NaN  9.481987       NaN       NaN  \n",
       "7559        NaN  9.528971       NaN       NaN  \n",
       "7560        NaN  9.588853       NaN       NaN  \n",
       "7561        NaN  9.626001       NaN       NaN  \n",
       "7562        NaN  9.658105       NaN       NaN  \n",
       "7563        NaN  9.681999  9.566511       NaN  \n",
       "7564        NaN  9.719668  9.613552       NaN  \n",
       "7565        NaN  9.737994  9.658799       NaN  \n",
       "7566        NaN  9.747201  9.683648       NaN  \n",
       "7567        NaN  9.756554  9.709789       NaN  \n",
       "7568        NaN  9.801665  9.729311       NaN  \n",
       "7569  66.875512  9.817148  9.760258  9.633288  \n",
       "7570  67.425643  9.826458  9.773453  9.674226  \n",
       "7571  66.592871  9.838538  9.782215  9.709038  \n",
       "7572  67.646586  9.840385  9.790993  9.730861  \n",
       "7573  68.934303  9.855390  9.820925  9.754327  \n",
       "7574  69.544196  9.863389  9.837213  9.767802  \n",
       "7575  67.271465  9.878086  9.851623  9.795705  \n",
       "7576  68.031406  9.885358  9.863050  9.807453  \n",
       "7577  68.594923  9.893860  9.871996  9.816729  \n",
       "7578  68.308051  9.908043  9.885918  9.826704  \n",
       "7579  65.674481  9.925483  9.892061  9.851253  \n",
       "\n",
       "[30 rows x 36 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7550   -0.003606\n",
      "7551    0.026385\n",
      "7552   -0.005511\n",
      "7553    0.021603\n",
      "7554   -0.014766\n",
      "          ...   \n",
      "9871    0.004749\n",
      "9872   -0.006034\n",
      "9873    0.023749\n",
      "9874   -0.007122\n",
      "9875    0.000246\n",
      "Name: future_ch_p, Length: 2326, dtype: float64 7550     9.2288\n",
      "7551     9.4723\n",
      "7552     9.4201\n",
      "7553     9.6236\n",
      "7554     9.4815\n",
      "         ...   \n",
      "9871    79.6339\n",
      "9872    79.1534\n",
      "9873    81.0332\n",
      "9874    80.4561\n",
      "9875    80.4759\n",
      "Name: nd_dshift, Length: 2326, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(new_df[\"future_ch_p\"], new_df[\"nd_dshift\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Adjusted_close   Adj_open   Adj_high    Adj_low  \\\n",
      "7620  2011-02-28         10.8476  10.787098  10.904109  10.783413   \n",
      "7621  2011-03-01         10.7278  10.916982  10.924660  10.677771   \n",
      "7622  2011-03-02         10.8140  10.747664  10.882486  10.699755   \n",
      "7623  2011-03-03         11.0425  10.969715  11.049564  10.930711   \n",
      "7624  2011-03-04         11.0561  11.058250  11.065006  10.986999   \n",
      "...          ...             ...        ...        ...        ...   \n",
      "9870  2020-02-06         80.3496  79.697335  80.352071  79.127790   \n",
      "9871  2020-02-07         79.2575  79.837016  80.092102  78.754757   \n",
      "9872  2020-02-10         79.6339  77.808673  79.633900  77.726946   \n",
      "9873  2020-02-11         79.1534  80.141548  80.215845  78.930509   \n",
      "9874  2020-02-12         81.0332  79.614128  81.038153  79.614128   \n",
      "\n",
      "        Adj_volume   Close    Volume     Tx_amount         Epoch  ...  \\\n",
      "7620  4.687335e+08  353.21  14395497  5.084633e+09  1.298869e+09  ...   \n",
      "7621  5.304319e+08  349.31  16290309  5.690368e+09  1.298956e+09  ...   \n",
      "7622  7.007568e+08  352.12  21521027  7.577984e+09  1.299042e+09  ...   \n",
      "7623  5.823700e+08  359.56  17885252  6.430821e+09  1.299128e+09  ...   \n",
      "7624  5.271031e+08  360.00  16188069  5.827705e+09  1.299215e+09  ...   \n",
      "...            ...     ...       ...           ...           ...  ...   \n",
      "9870  1.066758e+08  325.21  26356385  8.571360e+09  1.580965e+09  ...   \n",
      "9871  1.187977e+08  320.03  29421012  9.415606e+09  1.581052e+09  ...   \n",
      "9872  1.103837e+08  321.55  27337215  8.790281e+09  1.581311e+09  ...   \n",
      "9873  9.521578e+07  319.61  23580780  7.536653e+09  1.581397e+09  ...   \n",
      "9874  1.148065e+08  327.20  28432573  9.303138e+09  1.581484e+09  ...   \n",
      "\n",
      "      1d_ushift  2d_ushift 3d_ushift 7d_ushift       RSI9      RSI14  \\\n",
      "7620    10.6924    10.5303   10.5223   11.1521  50.177524  52.239069   \n",
      "7621    10.8476    10.6924   10.5303   11.0038  53.963635  54.666455   \n",
      "7622    10.7278    10.8476   10.6924   10.7661  62.468948  60.410406   \n",
      "7623    10.8140    10.7278   10.8476   10.3991  62.927560  60.729337   \n",
      "7624    11.0425    10.8140   10.7278   10.5223  54.999821  55.665635   \n",
      "...         ...        ...       ...       ...        ...        ...   \n",
      "9870    79.4206    78.7783   76.2606   78.4917  56.786539  59.343086   \n",
      "9871    80.3496    79.4206   78.7783   80.1347  58.465111  60.417039   \n",
      "9872    79.2575    80.3496   79.4206   80.0186  55.375964  58.299910   \n",
      "9873    79.6339    79.2575   80.3496   76.4706  63.795295  63.664344   \n",
      "9874    79.1534    79.6339   79.2575   76.2606  59.892522  61.067068   \n",
      "\n",
      "          RSI20      VWMA9     VWMA14     VWMA20  \n",
      "7620  54.298998  10.724864  10.797149  10.752794  \n",
      "7621  56.003092  10.706882  10.787249  10.764460  \n",
      "7622  60.149369  10.678279  10.775895  10.774694  \n",
      "7623  60.383276  10.680141  10.779942  10.792979  \n",
      "7624  56.709431  10.700855  10.788100  10.814088  \n",
      "...         ...        ...        ...        ...  \n",
      "9870  61.532312  78.315903  78.402197  78.048107  \n",
      "9871  62.294897  78.634113  78.428877  78.194357  \n",
      "9872  60.678502  78.735606  78.507332  78.329719  \n",
      "9873  64.474530  78.512667  78.538881  78.359129  \n",
      "9874  62.523960  78.594575  78.664344  78.540399  \n",
      "\n",
      "[2255 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "from_date = \"2011-01-01\" \n",
    "from_epoch = datetime.strptime(from_date, \"%Y-%m-%d\").timestamp()\n",
    "\n",
    "# selecting rows based on condition\n",
    "new_df = new_df.loc[new_df['Epoch'] >= from_epoch]\n",
    "new_df = new_df[lstm_seq_size-2:-1]\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"VWMA9_ch_p\"] = (new_df[\"Adjusted_close\"]-new_df[\"VWMA9\"])/new_df[\"VWMA9\"]\n",
    "new_df[\"VWMA14_ch_p\"] = (new_df[\"Adjusted_close\"]-new_df[\"VWMA14\"])/new_df[\"VWMA14\"]\n",
    "new_df[\"VWMA20_ch_p\"] = (new_df[\"Adjusted_close\"]-new_df[\"VWMA20\"])/new_df[\"VWMA20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"1d_ch_p\"] = (new_df[\"Adjusted_close\"]-new_df[\"1d_ushift\"])/new_df[\"1d_ushift\"]\n",
    "new_df[\"2d_ch_p\"] = (new_df[\"Adjusted_close\"]-new_df[\"2d_ushift\"])/new_df[\"2d_ushift\"]\n",
    "new_df[\"3d_ch_p\"] = (new_df[\"Adjusted_close\"]-new_df[\"3d_ushift\"])/new_df[\"3d_ushift\"]\n",
    "new_df[\"7d_ch_p\"] = (new_df[\"Adjusted_close\"]-new_df[\"7d_ushift\"])/new_df[\"7d_ushift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2255, 43)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1_df = (new_df[['RSI9', 'RSI14', 'RSI20']] - 50)/50\n",
    "X_train2_df = new_df[['VWMA9_ch_p', 'VWMA14_ch_p', 'VWMA20_ch_p']] * 10\n",
    "X_train3_df = new_df[['1d_ch_p', '2d_ch_p', '3d_ch_p', '7d_ch_p']] * 10\n",
    "X_train_df = pd.concat([X_train1_df, X_train2_df, X_train3_df], axis=1)\n",
    "print(X_train_df)\n",
    "X_train_np = X_train_df.to_numpy()\n",
    "print(X_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get label np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = new_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_np = label_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_np)\n",
    "print(label_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get change np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_df = new_df[\"future_ch_p\"] * 50\n",
    "change_df = change_df[lstm_seq_size-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_np = change_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change_np)\n",
    "print(change_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model multi input and mixed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model defined with the sequential api\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, concatenate\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two sets of inputs\n",
    "price = Input(shape=(lstm_seq_size,4))\n",
    "volume = Input(shape=(lstm_seq_size,2))\n",
    "technicals = Input(shape=(10,))\n",
    "# the first branch operates on the first input - price\n",
    "x = LSTM(50,input_shape = (lstm_seq_size,4),return_sequences = True, activation = 'relu')(price)\n",
    "x = Bidirectional(LSTM(60,return_sequences = False, activation = 'relu'))(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Model(inputs=price, outputs=x)\n",
    "# the second branch opreates on the second input - volume\n",
    "y = LSTM(50,input_shape = (lstm_seq_size,2),return_sequences = True, activation = 'relu')(volume)\n",
    "y = Bidirectional(LSTM(60,return_sequences = False, activation = 'relu'))(y)\n",
    "y = Dropout(0.1)(y)\n",
    "y = Model(inputs=volume, outputs=y)\n",
    "# the third branch opreates on the third input - technicals\n",
    "z = Dense(60, activation='relu', kernel_initializer='he_normal', input_shape=(10,))(technicals)\n",
    "z = Dense(30, activation='relu')(z)\n",
    "z = Dropout(0.1)(z)\n",
    "z = Model(inputs=techicals, outputs=z)\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x.output, y.output, z.output])\n",
    "combined = Dropout(0.1)(combined)\n",
    "#combined = Multiply()([x.output, y.output])\n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "out = Dense(3)(combined)\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[x.input, y.input, z.input], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer = tf.optimizers.Adam(learning_rate = 0.01), loss = loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit([price_lstm_np, volume_lstm_np, X_train_np], label_np, epochs=150, batch_size=64, verbose=1)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, 5, input_shape = (lstm_seq_size,6), activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer = tf.optimizers.Adam(learning_rate = 0.1), loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(price_lstm_np, label_np, epochs=50, batch_size=64, verbose=1)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import optimizers\n",
    "#define the model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50,input_shape = (lstm_seq_size,6),return_sequences = False, activation = 'relu')))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer = tf.optimizers.Adam(learning_rate = 0.01), loss = loss_fn,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(price_lstm_np, label_np, epochs=100, batch_size=64, verbose=1)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two sets of inputs\n",
    "inputA = Input(shape=(32,))\n",
    "inputB = Input(shape=(128,))\n",
    "# the first branch operates on the first input\n",
    "x = Dense(8, activation=\"relu\")(inputA)\n",
    "x = Dense(4, activation=\"relu\")(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "# the second branch opreates on the second input\n",
    "y = Dense(64, activation=\"relu\")(inputB)\n",
    "y = Dense(32, activation=\"relu\")(y)\n",
    "y = Dense(4, activation=\"relu\")(y)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x.output, y.output])\n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "z = Dense(2, activation=\"relu\")(combined)\n",
    "z = Dense(1, activation=\"linear\")(z)\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e42e85f13f04c1e38671b9d4681ff8e731b1b5418a8a2d67ea46afc7d29f7354"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
